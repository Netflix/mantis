{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mantis Mantis is a platform to build an ecosystem of realtime stream processing applications. Similar to micro-services deployed in a cloud, Mantis applications (jobs) are deployed on the Mantis platform. The Mantis platform provides the APIs to manage the life cycle of jobs (like deploy, update, and terminate), manages the underlying resources by containerizing a common pool of servers, and, similar to a traditional micro-service cloud, allows jobs to discover and communicate with other jobs. By providing stream processing As-a-Service, Mantis allows developers to simply focus on their business logic to build powerful and cost-effective streaming applications. Why We Built Mantis Mantis evolved from the need to get better (faster and in-depth) operational insights in a rapidly growing, complex, micro-service ecosystem at Netflix. As complexity of a system increases, our comprehension of the system rapidly decreases. In order to counter this complexity we need new approaches to operational insights. We need to change the way we generate and collect operational data: We should have access to raw events. Applications should be free to publish every single event. If we reduce the granularity at this stage, such as by pre-aggregating or sampling, then we\u2019re already at a disadvantage when it comes to getting insight since the data in its original form is already lost. We should be able to access this data in realtime. Operational use cases are inherently time-sensitive by nature. This becomes increasingly important with scale as the impact becomes much larger in less time. We should be able to ask new questions of this data without necessarily having to add new instrumentation to our applications. It\u2019s not possible to know ahead of time every single possible failure mode our systems might encounter despite all the rigor built in to make these systems resilient. When these failures do inevitably occur, it\u2019s important that we can derive new insights with this data. We need a new kind of execution environment : Can Process high volume data at low latency Has low Operational burden We need a managed platform where most of the operational tasks are handled automatically on behalf of the user. We don\u2019t need the additional overhead of operating our monitoring system. Is Elastic and Resilient We need a highly reliable system that can automatically recover from node failures and be able to scale the resources dynamically based on the data volume. Ecosystem of Streaming services A lot of use-cases often need the same data. By allowing jobs to discover each other and collaborate together by sharing data and results we can build cost-effective jobs that maximise code and data re-use. We should be able to do all of the above in a cost-effective way . As our business critical systems scale, we need to make sure the systems in support of these business critical systems don\u2019t end up costing more than the business critical systems themselves. Mantis was built to meet all the above needs. It was designed by Netflix . How Can Mantis Be Used? Mantis provides a robust, scalable platform that is ideally suited for high volume, low latency use cases like anomaly detection and alerting. Mantis has been in production at Netflix since 2014. It processes trillions of events and peta-bytes of data every day. As a streaming microservices ecosystem, the Mantis platform provides engineers with capabilities to minimize the costs of observing and operating complex distributed systems without compromising on operational insights. Engineers can build cost-efficient real-time applications on top of Mantis to quickly identify issues, trigger alerts, and apply remediations to minimize or completely avoid downtime to the Netflix service. Next Steps To learn more about Mantis, visit the Concepts page or browse through the list of use cases powered by Mantis. To get involved with community, visit the Community page where you can subscribe to one of our mailing lists.","title":"Welcome"},{"location":"#mantis","text":"Mantis is a platform to build an ecosystem of realtime stream processing applications. Similar to micro-services deployed in a cloud, Mantis applications (jobs) are deployed on the Mantis platform. The Mantis platform provides the APIs to manage the life cycle of jobs (like deploy, update, and terminate), manages the underlying resources by containerizing a common pool of servers, and, similar to a traditional micro-service cloud, allows jobs to discover and communicate with other jobs. By providing stream processing As-a-Service, Mantis allows developers to simply focus on their business logic to build powerful and cost-effective streaming applications.","title":"Mantis"},{"location":"#why-we-built-mantis","text":"Mantis evolved from the need to get better (faster and in-depth) operational insights in a rapidly growing, complex, micro-service ecosystem at Netflix. As complexity of a system increases, our comprehension of the system rapidly decreases. In order to counter this complexity we need new approaches to operational insights. We need to change the way we generate and collect operational data: We should have access to raw events. Applications should be free to publish every single event. If we reduce the granularity at this stage, such as by pre-aggregating or sampling, then we\u2019re already at a disadvantage when it comes to getting insight since the data in its original form is already lost. We should be able to access this data in realtime. Operational use cases are inherently time-sensitive by nature. This becomes increasingly important with scale as the impact becomes much larger in less time. We should be able to ask new questions of this data without necessarily having to add new instrumentation to our applications. It\u2019s not possible to know ahead of time every single possible failure mode our systems might encounter despite all the rigor built in to make these systems resilient. When these failures do inevitably occur, it\u2019s important that we can derive new insights with this data. We need a new kind of execution environment : Can Process high volume data at low latency Has low Operational burden We need a managed platform where most of the operational tasks are handled automatically on behalf of the user. We don\u2019t need the additional overhead of operating our monitoring system. Is Elastic and Resilient We need a highly reliable system that can automatically recover from node failures and be able to scale the resources dynamically based on the data volume. Ecosystem of Streaming services A lot of use-cases often need the same data. By allowing jobs to discover each other and collaborate together by sharing data and results we can build cost-effective jobs that maximise code and data re-use. We should be able to do all of the above in a cost-effective way . As our business critical systems scale, we need to make sure the systems in support of these business critical systems don\u2019t end up costing more than the business critical systems themselves. Mantis was built to meet all the above needs. It was designed by Netflix .","title":"Why We Built Mantis"},{"location":"#how-can-mantis-be-used","text":"Mantis provides a robust, scalable platform that is ideally suited for high volume, low latency use cases like anomaly detection and alerting. Mantis has been in production at Netflix since 2014. It processes trillions of events and peta-bytes of data every day. As a streaming microservices ecosystem, the Mantis platform provides engineers with capabilities to minimize the costs of observing and operating complex distributed systems without compromising on operational insights. Engineers can build cost-efficient real-time applications on top of Mantis to quickly identify issues, trigger alerts, and apply remediations to minimize or completely avoid downtime to the Netflix service.","title":"How Can Mantis Be Used?"},{"location":"#next-steps","text":"To learn more about Mantis, visit the Concepts page or browse through the list of use cases powered by Mantis. To get involved with community, visit the Community page where you can subscribe to one of our mailing lists.","title":"Next Steps"},{"location":"community/","text":"Mantis open source is hosted on Github. Issues are also tracked on Github and we prefer to receive contributions as pull requests. Contributing Mantis components are spread across a few different repos. See the Components section in the README for more information. Before submitting a pull request, make sure to follow the instructions on the repo's CONTRIBUTING.md file. Mailing Lists Developers : mantis-oss-dev@googlegroups.com - used for discussing Mantis development. Users : mantis-oss-users@googlegroups.com - used for community discussions.","title":"Community"},{"location":"community/#contributing","text":"Mantis components are spread across a few different repos. See the Components section in the README for more information. Before submitting a pull request, make sure to follow the instructions on the repo's CONTRIBUTING.md file.","title":"Contributing"},{"location":"community/#mailing-lists","text":"Developers : mantis-oss-dev@googlegroups.com - used for discussing Mantis development. Users : mantis-oss-users@googlegroups.com - used for community discussions.","title":"Mailing Lists"},{"location":"faq/","text":"Is Mantis just another stream processing engine like Flink or Spark ? Mantis can be thought of as a stream processing engine + a container cloud for execution. Mantis jobs do not run in isolation but are part of an ecosystem of jobs. This allows jobs to collaborate with other jobs to form complex streaming microservices system. Allowing jobs to communicate with other jobs promotes code and data reuse thus improving efficiency and productivity as users do not have to re-invent the wheel . Additionally, Mantis jobs can, with the help of the mantis-publish library, source data directly from production systems in an on-demand fashion. This provides a cost effective option to access rich operational data which otherwise would be prohibitive to transport and store (Imagine having to transport and store request and response bodies for every single request) So the answer is a qualified yes but it is much more than a stream processing engine. Does Mantis process events in micro batches or event a time ? By default Mantis Jobs process events one at a time. Users can however create their own micro batches using ReactiveX operators. What kind of message guarantees does Mantis support ? The default delivery semantic is at-most once. This is intentional as the majority of the use cases on Mantis are latency sensitive (think alerting.) where determining an aggregate trend in subsecond latency is preferred to data completeness albeit with potential delay. In cases where the source of data is Kafka, Mantis supports at-least once semantics. The exactly-once or effectively once semantic support is currently not available in Open Source. Do Mantis Jobs operate in event time or clock time ? By default Mantis jobs operate with clock time. How does Mantis deal with backpressure ? Mantis relies on the backpressure support built-in to RxJava. This ensures an upstream operator in the execution DAG will not send more events down until the downstream consumer is ready. Thus backpressure bubbles up along the DAG to the edge where data is being fetched from across the network. At this point, the strategy depends on whether the external source of data is cold source like a queue or database or a hot source like data sent via [mantis-publish]. For [cold] sources like Kafka no more events are read until the previous batch of events have been acknowledged. For [hot] sources events are first buffered and then as a last resort dropped. One effective way to deal with backpressure in Mantis, is to enable [autoscale] on the job with a strategy to scale up on backpressure. Is it possible to use the second [stage] of a three-stage [Mantis Job] as the [source job] for another [MQL] job? Jobs connect to the [sink] stage of other jobs by default, so while it may be technically possible to rig up a solution where you connect to an intermediate stage instead, this is not supported as an out-of-the-box solution. You may instead want to reorganize your original job, perhaps splitting it into two jobs.","title":"FAQ"},{"location":"faq/#is-mantis-just-another-stream-processing-engine-like-flink-or-spark","text":"Mantis can be thought of as a stream processing engine + a container cloud for execution. Mantis jobs do not run in isolation but are part of an ecosystem of jobs. This allows jobs to collaborate with other jobs to form complex streaming microservices system. Allowing jobs to communicate with other jobs promotes code and data reuse thus improving efficiency and productivity as users do not have to re-invent the wheel . Additionally, Mantis jobs can, with the help of the mantis-publish library, source data directly from production systems in an on-demand fashion. This provides a cost effective option to access rich operational data which otherwise would be prohibitive to transport and store (Imagine having to transport and store request and response bodies for every single request) So the answer is a qualified yes but it is much more than a stream processing engine.","title":"Is Mantis just another stream processing engine like Flink or Spark ?"},{"location":"faq/#does-mantis-process-events-in-micro-batches-or-event-a-time","text":"By default Mantis Jobs process events one at a time. Users can however create their own micro batches using ReactiveX operators.","title":"Does Mantis process events in micro batches or event a time ?"},{"location":"faq/#what-kind-of-message-guarantees-does-mantis-support","text":"The default delivery semantic is at-most once. This is intentional as the majority of the use cases on Mantis are latency sensitive (think alerting.) where determining an aggregate trend in subsecond latency is preferred to data completeness albeit with potential delay. In cases where the source of data is Kafka, Mantis supports at-least once semantics. The exactly-once or effectively once semantic support is currently not available in Open Source.","title":"What kind of message guarantees does Mantis support ?"},{"location":"faq/#do-mantis-jobs-operate-in-event-time-or-clock-time","text":"By default Mantis jobs operate with clock time.","title":"Do Mantis Jobs operate in event time or clock time ?"},{"location":"faq/#how-does-mantis-deal-with-backpressure","text":"Mantis relies on the backpressure support built-in to RxJava. This ensures an upstream operator in the execution DAG will not send more events down until the downstream consumer is ready. Thus backpressure bubbles up along the DAG to the edge where data is being fetched from across the network. At this point, the strategy depends on whether the external source of data is cold source like a queue or database or a hot source like data sent via [mantis-publish]. For [cold] sources like Kafka no more events are read until the previous batch of events have been acknowledged. For [hot] sources events are first buffered and then as a last resort dropped. One effective way to deal with backpressure in Mantis, is to enable [autoscale] on the job with a strategy to scale up on backpressure.","title":"How does Mantis deal with backpressure ?"},{"location":"faq/#is-it-possible-to-use-the-second-stage-of-a-three-stage-mantis-job-as-the-source-job-for-another-mql-job","text":"Jobs connect to the [sink] stage of other jobs by default, so while it may be technically possible to rig up a solution where you connect to an intermediate stage instead, this is not supported as an out-of-the-box solution. You may instead want to reorganize your original job, perhaps splitting it into two jobs.","title":"Is it possible to use the second [stage] of a three-stage [Mantis Job] as the [source job] for another [MQL] job?"},{"location":"glossary/","text":"artifact file Each [Mantis Job] has an associated artifact file that contains its source code and JSON configuration. autoscaling You can establish an autoscaling policy for each [component] of your [Mantis Job] that governs how Mantis adjusts the number of [workers] assigned to that component as its workload changes. \u21d2 See Mantis Job Autoscaling backpressure Backpressure refers to a set of possible strategies for coping with [ReactiveX] [Observables] that produce items more rapidly than their observers consume them. \u21d2 See ReactiveX.io: backpressure operators Binary compression has a particular meaning in the Mantis context... see (Connecting to a Source Job)[/writingjobs/source#connecting-to-a-source-job] Broadcast mode In broadcast mode, each [worker] of your [Mantis Job] gets all the data from all workers of the [Source Job] rather than having that data distributed equally among the workers of your Job. \u21d2 See Source Job Sources: Broadcast Mode Cassandra Apache Cassandra is an open source, distributed database management system. \u21d2 See Apache Cassandra Documentation . Cluster A Job Cluster is a containing entity for [Mantis Jobs]. It defines metadata and certain [service-level agreements]. Job Clusters ease job lifecycle management and job revisioning. A Mantis Cluster is a group of cloud container instances that hold your Mantis resources. cold Observables A cold [ReactiveX] [Observable] waits until an observer subscribes to it before it begins to emit items. This means the observer is guaranteed to see the whole Observable sequence from the beginning. This is in contrast to a hot Observable, which may begin emitting items as soon as it is created, even before observers have subscribed to it. component A Mantis [Job] is composed of three types of component: a [Source], one or more [Processing Stages], and a [Sink]. Custom source In contrast to a [Source Job], which is a built-in variety of [Source] [component] designed to pull data from a common sort of data source, a custom source typically accesses data from less-common sources or has unusual delivery guarantee semantics. Executor The stage executor is responsible for loading the bytecode for a [Mantis Job] and then executing its [stages] and [workers] in a coordinated fashion. In the [Mesos] UI, workers are also referred to as executors. Fenzo Fenzo is a Java library that implements a generic task scheduler for [Mesos] frameworks. \u21d2 See the Fenzo documentation . grouped data Grouped (or keyed) data is distinguished from [scalar] data in that each datum is accompanied by a key that indicates what group it belongs to. Grouped data can be processed by a [RxJava] GroupedObservable or by a MantisGroup . GRPC GRPC is an open-source RPC framework using Protocol Buffers. \u21d2 See GRPC.io hot Observables A hot [ReactiveX] [Observable] may begin emitting items as soon as it is created, even before observers have subscribed to it. This means the observer may miss items that were emitted before the observer subscribed. This is in contrast to a cold Observable, which waits until an observer subscribes to it before it begins to emit items. JMC Java Mission Control is a tool from Oracle with which developers can monitor and manage Java applications. \u21d2 See Java Components Job A Mantis Job takes in a stream of data, transforms it by using [RxJava] operators, and then outputs the results as another stream. It is composed of a [Source], one or more [Processing Stages], and a [Sink]. \u21d2 See Writing Mantis Jobs Job Cluster see [Cluster] Job Master If a job is configured with [autoscaling], Mantis will add a Job Master component to it as its initial component. This component will send metrics back to Mantis to help it govern the autoscaling process. Kafka Apache Kafka is a large-scale, distributed streaming platform. \u21d2 See Apache Kafka . keyed data see [grouped data] Keystone Keystone is Netflix\u2019s data backbone, a stream processing platform that focuses on data analytics. \u21d2 See Keystone Real-time Stream Processing Platform label A label is a text key/value pair that you can add to a [Job Cluster] or to an individual [Job] to make it easier to search for or group. Log4j Log4j is a Java-based logging framework. \u21d2 See Apache Log4j Mantis Master The Mantis Master coordinates the execution of [Mantis Jobs] and starts the services on each [Worker]. Mesos Apache Mesos is an open-source technique for balancing resources across frameworks in clusters. \u21d2 See Apache Mesos Metadata Mantis inserts metadata into its [Job] payload. This may include information about where the data came from, for instance. You can define additional metadata to include in the payload when you establish the [Job Cluster]. meta message A [Source Job] may occasionally inject meta messages into its data stream that indicate things like data drops. migration strategy define Mantis Publish Mantis Publish (internally at Netflix known as Mantis Realtime Events or MRE) is a library that your application can use to stream events into Mantis while respecting [MQL] filters. \u21d2 See MQL . MQL You use Mantis Query Language to define filters and other data processing that Mantis applies to a [Source] data stream at its point of origin, so as to reduce the amount of data going over the wire. Observable In [ReactiveX] an Observable is the method of processing a stream of data in a way that facilitates its [transformation] and consumption by observers. Observables come in [hot] and [cold] varieties. There is also a GroupedObservable that is specialized to [grouped] data. \u21d2 See ReactiveX.io: Observable . Parameter A [Mantis Job] may accept parameters that modify its behavior. You can define these in your [Job Cluster] definition, and set their values on a per-Job basis. Processing Stage A Processing Stage component of a Mantis [Job] transforms the [RxJava] [Observables] it obtains from the [Source] component. A Job with only one Processing Stage is called a single-stage Job. \u21d2 See The Processing Stage Component property A property is a particular named data value found within events in an event stream. Reactive Streams Reactive Streams is the latest advance of the [ReactiveX] project. It is an API for manipulating streams of asynchronous data in a non-blocking fashion, with [backpressure]. \u21d2 See Reactive Streams . ReactiveX ReactiveX is a software technique for transforming, combining, reacting to, and managing streams of data. [RxJava] is an example of a library that implements this technique. \u21d2 See reactivex.io . RxJava RxJava is the Java implementation of [ReactiveX], a software technique for transforming, combining, reacting to, and managing streams of data. \u21d2 See reactivex.io . sampling Sampling is an MQL strategy for mitigating data volume issues. There are two sampling strategies: Random and Sticky. Random sampling uniformly downsamples the source stream to a percentage of its original volume. Sticky sampling selectively samples data from the source stream based on key values. \u21d2 See MQL: Sampling scalar data Scalar data is distinguished from keyed or [grouped] data in that it is not categorized into groups by key. Scalar data can be processed by an ordinary [ReactiveX] [Observable]. Sink The Sink is the final component of a Mantis [Job]. It takes the [Observables] that has been transformed by the [Processing Stage] and outputs it in the form of a new data stream. \u21d2 See The Sink Component SLA A service-level agreement , in the Mantis context, is defined on a per-[Cluster] basis. You use it to configure how many [Jobs] in the cluster will be in operation at any time, among other things. Source The Source component of a Mantis [Job] fetches data from a source outside of Mantis and makes it available to the [Processing Stage] component in the form of an [RxJava] [Observable]. There are two varieties of Source: a [Source Job] and a [custom source]. \u21d2 See The Source Component Source Job A Source Job is a Mantis [Job] that you can use as a [Source], which wraps a data source external to Mantis and makes it easier for you to create a job that observes its data. \u21d2 See Mantis Source Jobs server-sent events Server-sent events (SSE) are a way for a browser to receive automatic updates from a server through an HTTP connection. Mantis includes an SSE [Sink]. \u21d2 See Wikipedia: Server-sent events Transformation A transformation acts on each datum from a stream or [Observables] of data, changing it in some manner before passing it along as a new stream or Observable. Transformations may change data between [scalar] and [grouped] forms. transient jobs A transient (or ephemeral) [Mantis Job] is automatically killed by Mantis after a certain amount of time has passed since the last subscriber to the job disconnects. WebSocket WebSocket is a two-way, interactive communication channel that works over HTTP. In the Mantis context, it is an alternative to [SSE]. \u21d2 See WebSocket.org . Worker A worker is the smallest unit of work that is scheduled within a Mantis [component]. You can configure how many resources Mantis allocates to each worker, and Mantis will adjust the number of workers your Mantis component needs based on its [autoscaling] policy. Zookeeper Apache Zookeeper is an open-source server that maintains configuration information and other services required by distributed applications. \u21d2 See Apache ZooKeeper","title":"Glossary"},{"location":"develop/connectors/iceberg/","text":"Sink Add this package to your dependencies: implementation io.mantisrx:mantis-connector-iceberg:1.2.+ The Iceberg Sink has two components: Writers and a Committer . Writers Writers write Iceberg Record s to files in a specific file format. Writers periodically stage their data by flushing their underlying files to produce metadata in the form of DataFile s which are sent to Committers. Add an Iceberg Writer using one of the following approaches: Separate Processing Stage Use this approach to decouple your application logic from Iceberg writing logic to may make it easier to debug your Mantis Job. This approach incurs extra encode/decode and network cost to move data between workers. public class ExampleIcebergSinkJob extends MantisJobProvider Map String , Object { @Override public Job Map String , Object getJobInstance () { return MantisJob . source ( source that produces MantisServerSentEvents ) . stage (...) // (0) . stage ( new IcebergWriterStage (), IcebergWriterStage . config ()) // (1) . stage ( new IcebergCommitterStage (), IcebergCommitterStage . config ()) // (2) . sink (...) . lifecycle ( NetflixLifecycles . governator ( job file properties name , new IcebergModule ())) // (3) . create (); } } (0) A series of Processing Stages that you define for your application logic. Produces an Iceberg Record. To emit Iceberg Records out of your separate Processing Stage, add the Iceberg Record Codec to your stage config. (1) A Processing Stage of n parallelizable IcebergWriter s. The Stage Config automatically adds an Iceberg DataFile Codec to encode/decode DataFiles between these workers and the Committer workers from the next stage downstream. (2) A Processing Stage for 1 IcebergCommitter . (3) A module for injecting dependencies and configs required to authenticate/connect/interact with backing Iceberg infrastructure. With an existing Processing Stage Use this approach to avoid incurring encode/decode and network costs. This approach may make your Mantis Job more difficult to debug. public class ExampleIcebergSinkJob extends MantisJobProvider Map String , Object { @Override public Job Map String , Object getJobInstance () { return MantisJob . source ( source that produces MantisServerSentEvents ) . stage ( new ProcessingAndWriterStage (), config ) // (0) . stage ( new IcebergCommitterStage (), IcebergCommitterStage . config ()) // (1) . sink (...) . lifecycle ( NetflixLifecycles . governator ( job file properties name , new IcebergModule ())) // (2) . create (); } } /** * Example class which takes in MantisServerSentEvents, performs some logic, * produces Iceberg Records, writes the Records to files, * and produces DataFiles for a downstream Iceberg Committer. */ public class ProcessingAndWriterStage implements ScalarComputation MantisServerSentEvent , DataFile { private Transformer transformer ; public static ScalarToScalar . Config Record , DataFile config () { return new ScalarToScalar . Config Record , DataFile () . description ( ) . codec ( IcebergCodecs . dataFile ()) // (3) . withParameters (...); } @Override public void init ( Context context ) { transformer = IcebergWriterStage . newTransformer ( context ); // (4) } @Override public Observable DataFile call ( Context context , Observable Record recordObservable ) { return recordObservable . map ( some application logic ) . map ( some more application logic ) . compose ( transformer ); // (5) } } (0) A series of Processing Stages for your application logic and Iceberg writing logic. You may further reduce network cost by combining your Processing Stage(s) logic into your Source. (1) A Processing Stage for 1 IcebergCommitter . (2) A module for injecting dependencies and configs required to authenticate/connect/interact with backing Iceberg infrastructure. (3) Remember to add an Iceberg DataFile Codec to emit DataFile s to the Committer. (4) Create a new Iceberg Writer Transformer from the static method provided by IcebergWriterStage . (5) Compose the transformer with your application logic Observable. Note Writers are stateless and may be parallelized/autoscaled. Committer The Committer commits DataFile s to Iceberg. Records are queryable from the Iceberg table only after a Committer commits DataFile s. A Committer commits on a configured interval (default: 5 min). If a Committer fails, it will retry until a retry threshold is met, after which it will continue onto the next window of Record s. This avoids backpressure issues originating from downstream consumers which makes it more suitable for operational use cases. public class ExampleIcebergSinkJob extends MantisJobProvider Map String , Object { @Override public Job Map String , Object getJobInstance () { return MantisJob . source ( source that produces Iceberg Records ) . stage (...) . stage ( new IcebergWriterStage (), IcebergWriterStage . config ()) . stage ( new IcebergCommitterStage (), IcebergCommitterStage . config ()) // (1) . sink (...) . lifecycle ( NetflixLifecycles . governator ( job file properties name , new IcebergModule ())) . create (); } } (1) A Processing Stage for 1 IcebergCommitter . The Committer outputs a Map which you can subscribe to with a Sink for optional debugging or connecting to another Mantis Job. Otherwise, a Sink not required because the Committer will write directly to Iceberg. Important You should try to have only one Committer per Iceberg Table and try to avoid a high frequency commit intervals (default: 5 min ). This avoids commit pressure on Iceberg. Configuration Options Name Type Default Description writerRowGroupSize int 100 Number of rows to chunk before checking for file size writerFlushFrequencyBytes String \"134217728\" (128 MiB) Flush frequency by size in Bytes writerFlushFrequencyMsec String \"60000\" (1 min) Flush frequency by time in milliseconds writerFileFormat String parquet File format for writing data files to backing Iceberg store writerMaximumPoolSize int 5 Maximum number of writers that should exist per worker commitFrequencyMs String \"300000\" (5 min) Iceberg Committer frequency by time in milliseconds Metrics Prefix: io.mantisrx.connector.iceberg.sink.writer.metrics.WriterMetrics Name Type Description openSuccessCount Counter Number of times a file was successfully opened openFailureCount Counter Number of times a file failed to open writeSuccessCount Counter Number of times an Iceberg Record was successfully written to a file writeFailureCount Counter Number of times an Iceberg Record failed to be written to a file batchSuccessCount Counter Number of times a file was successfully flushed to produce a DataFile batchFailureCount Counter Number of times a file failed to flush batchSize Gauge Number of Iceberg Records per writer flush as described within a DataFile batchSizeBytes Gauge Cumulative size of Iceberg Records from a writer flush Prefix: io.mantisrx.connector.iceberg.sink.committer.metrics.CommitterMetrics Name Type Description invocationCount Counter Number of times a commit was invoked commitSuccessCount Counter Number of times a commit was successful commitFailureCount Counter Number of times a commit failed commitLatencyMsec Gauge Time it took to perform the most recent commit commitBatchSize Gauge Cumulative size of DataFile s from a commit","title":"Iceberg (Beta)"},{"location":"develop/connectors/iceberg/#sink","text":"Add this package to your dependencies: implementation io.mantisrx:mantis-connector-iceberg:1.2.+ The Iceberg Sink has two components: Writers and a Committer .","title":"Sink"},{"location":"develop/connectors/iceberg/#writers","text":"Writers write Iceberg Record s to files in a specific file format. Writers periodically stage their data by flushing their underlying files to produce metadata in the form of DataFile s which are sent to Committers. Add an Iceberg Writer using one of the following approaches:","title":"Writers"},{"location":"develop/connectors/iceberg/#separate-processing-stage","text":"Use this approach to decouple your application logic from Iceberg writing logic to may make it easier to debug your Mantis Job. This approach incurs extra encode/decode and network cost to move data between workers. public class ExampleIcebergSinkJob extends MantisJobProvider Map String , Object { @Override public Job Map String , Object getJobInstance () { return MantisJob . source ( source that produces MantisServerSentEvents ) . stage (...) // (0) . stage ( new IcebergWriterStage (), IcebergWriterStage . config ()) // (1) . stage ( new IcebergCommitterStage (), IcebergCommitterStage . config ()) // (2) . sink (...) . lifecycle ( NetflixLifecycles . governator ( job file properties name , new IcebergModule ())) // (3) . create (); } } (0) A series of Processing Stages that you define for your application logic. Produces an Iceberg Record. To emit Iceberg Records out of your separate Processing Stage, add the Iceberg Record Codec to your stage config. (1) A Processing Stage of n parallelizable IcebergWriter s. The Stage Config automatically adds an Iceberg DataFile Codec to encode/decode DataFiles between these workers and the Committer workers from the next stage downstream. (2) A Processing Stage for 1 IcebergCommitter . (3) A module for injecting dependencies and configs required to authenticate/connect/interact with backing Iceberg infrastructure.","title":"Separate Processing Stage"},{"location":"develop/connectors/iceberg/#with-an-existing-processing-stage","text":"Use this approach to avoid incurring encode/decode and network costs. This approach may make your Mantis Job more difficult to debug. public class ExampleIcebergSinkJob extends MantisJobProvider Map String , Object { @Override public Job Map String , Object getJobInstance () { return MantisJob . source ( source that produces MantisServerSentEvents ) . stage ( new ProcessingAndWriterStage (), config ) // (0) . stage ( new IcebergCommitterStage (), IcebergCommitterStage . config ()) // (1) . sink (...) . lifecycle ( NetflixLifecycles . governator ( job file properties name , new IcebergModule ())) // (2) . create (); } } /** * Example class which takes in MantisServerSentEvents, performs some logic, * produces Iceberg Records, writes the Records to files, * and produces DataFiles for a downstream Iceberg Committer. */ public class ProcessingAndWriterStage implements ScalarComputation MantisServerSentEvent , DataFile { private Transformer transformer ; public static ScalarToScalar . Config Record , DataFile config () { return new ScalarToScalar . Config Record , DataFile () . description ( ) . codec ( IcebergCodecs . dataFile ()) // (3) . withParameters (...); } @Override public void init ( Context context ) { transformer = IcebergWriterStage . newTransformer ( context ); // (4) } @Override public Observable DataFile call ( Context context , Observable Record recordObservable ) { return recordObservable . map ( some application logic ) . map ( some more application logic ) . compose ( transformer ); // (5) } } (0) A series of Processing Stages for your application logic and Iceberg writing logic. You may further reduce network cost by combining your Processing Stage(s) logic into your Source. (1) A Processing Stage for 1 IcebergCommitter . (2) A module for injecting dependencies and configs required to authenticate/connect/interact with backing Iceberg infrastructure. (3) Remember to add an Iceberg DataFile Codec to emit DataFile s to the Committer. (4) Create a new Iceberg Writer Transformer from the static method provided by IcebergWriterStage . (5) Compose the transformer with your application logic Observable. Note Writers are stateless and may be parallelized/autoscaled.","title":"With an existing Processing Stage"},{"location":"develop/connectors/iceberg/#committer","text":"The Committer commits DataFile s to Iceberg. Records are queryable from the Iceberg table only after a Committer commits DataFile s. A Committer commits on a configured interval (default: 5 min). If a Committer fails, it will retry until a retry threshold is met, after which it will continue onto the next window of Record s. This avoids backpressure issues originating from downstream consumers which makes it more suitable for operational use cases. public class ExampleIcebergSinkJob extends MantisJobProvider Map String , Object { @Override public Job Map String , Object getJobInstance () { return MantisJob . source ( source that produces Iceberg Records ) . stage (...) . stage ( new IcebergWriterStage (), IcebergWriterStage . config ()) . stage ( new IcebergCommitterStage (), IcebergCommitterStage . config ()) // (1) . sink (...) . lifecycle ( NetflixLifecycles . governator ( job file properties name , new IcebergModule ())) . create (); } } (1) A Processing Stage for 1 IcebergCommitter . The Committer outputs a Map which you can subscribe to with a Sink for optional debugging or connecting to another Mantis Job. Otherwise, a Sink not required because the Committer will write directly to Iceberg. Important You should try to have only one Committer per Iceberg Table and try to avoid a high frequency commit intervals (default: 5 min ). This avoids commit pressure on Iceberg.","title":"Committer"},{"location":"develop/connectors/iceberg/#configuration-options","text":"Name Type Default Description writerRowGroupSize int 100 Number of rows to chunk before checking for file size writerFlushFrequencyBytes String \"134217728\" (128 MiB) Flush frequency by size in Bytes writerFlushFrequencyMsec String \"60000\" (1 min) Flush frequency by time in milliseconds writerFileFormat String parquet File format for writing data files to backing Iceberg store writerMaximumPoolSize int 5 Maximum number of writers that should exist per worker commitFrequencyMs String \"300000\" (5 min) Iceberg Committer frequency by time in milliseconds","title":"Configuration Options"},{"location":"develop/connectors/iceberg/#metrics","text":"Prefix: io.mantisrx.connector.iceberg.sink.writer.metrics.WriterMetrics Name Type Description openSuccessCount Counter Number of times a file was successfully opened openFailureCount Counter Number of times a file failed to open writeSuccessCount Counter Number of times an Iceberg Record was successfully written to a file writeFailureCount Counter Number of times an Iceberg Record failed to be written to a file batchSuccessCount Counter Number of times a file was successfully flushed to produce a DataFile batchFailureCount Counter Number of times a file failed to flush batchSize Gauge Number of Iceberg Records per writer flush as described within a DataFile batchSizeBytes Gauge Cumulative size of Iceberg Records from a writer flush Prefix: io.mantisrx.connector.iceberg.sink.committer.metrics.CommitterMetrics Name Type Description invocationCount Counter Number of times a commit was invoked commitSuccessCount Counter Number of times a commit was successful commitFailureCount Counter Number of times a commit failed commitLatencyMsec Gauge Time it took to perform the most recent commit commitBatchSize Gauge Cumulative size of DataFile s from a commit","title":"Metrics"},{"location":"develop/ingestion/netty/","text":"You can stream events on-demand from your application to be ingested directly into Mantis using the Mantis Publish library (sometimes casually referred to as \"MRE\"). Mantis Publish takes care of filtering the events you send into Mantis, and it will only transmit them over the network if a downstream consumer that is interested in such events is currently subscribed. Mantis Publish contains a subscription registry where each client subscription is represented by its corresponding Mantis Query Language query. Mantis Publish evaluates all MQL queries from its subscription registry against each event as the event is generated by your application. It then tags events with all of the matching MQL queries and enriches the event with a superset of fields from the matched queries. That is, rather than emitting n events for n matching MQL queries, Mantis Publish will instead emit a single event containing all of the fields requested by the matched queries. This happens directly within your application before any events are sent over the network into Mantis. Further, Mantis Publish only dispatches events if a client is subscribed with a matching query. This means that you can freely produce events without incurring the cost until an active subscription exists. Choose a Package MRE works with both Guice-enabled and standalone injectors. Guice-based Injector implementation io.mantisrx:mantis-publish-netty-guice:1.2.+ Enable the Publisher Client Inject the MantisRealtimeEventsPublishModule into your application. In addition to injecting MantisRealtimeEventsPublishModule you will also need to add the ArchaiusModule and the SpectatorModule if not already injected. Injector injector = Guice . createInjector ( new MyBasicModule (), new ArchaiusModule (), new MantisRealtimeEventsPublishModule (), new SpectatorModule ()); Standalone Injector implementation io.mantisrx:mantis-publish-netty:1.2.+ Enable the Publisher Client Follow the example standalone initializer to manually inject dependencies. Once you have constructed a MrePublishClientInitializer , call the MantisPublishClientInitializer#start method to initialize underlying components. Configure Where to Send Svents You will need to configure the location of the Mantis API server for the mantis-publish library to bootstrap. Add the following properties to your application.properties : mantis.publish.discovery.api.hostname = IP of Mantis API # mantis api port mantis.publish.discovery.api.port = port for Mantis API # This application s name mantis.publish.app.name = JavaApp Send Events into Mantis For each event your application wishes to send to Mantis, create a Event object with your desired event fields, and pass that Event to the MantisEventPublisher#publish method. For example: // Create an `Event` for Mantis Publish using your application event. final Event event = new Event (); event . set ( testKey , testValue ); // Send your `Event` into Mantis. // Note: This event will only be dispatched over the network // if a subscription with a matching MQL query exists. eventPublisher . publish ( event ); Consuming a Mantis Stream Visit the Querying page for details on how you can consume your application's event stream. Configuration Options There are a number of configuration options available to control the behavior of the publishing client. Prefix: mantis.publish Name Default Description enabled true Determine if event processing is enabled. tee.enabled false Allows events to simultaneously be sent to an external system outside of Mantis tee.stream default_stream Specifies which external stream name tee will write to blacklist param.password Comma separated list of field names where the value will be obfuscated max.num.streams 5 Maximum number of streams this application can create stream.inactive.duration.threshold.sec 86400 (24 hours) Maximum duration in seconds for the stream to be considered inactive if there are no events stream name .stream.queue.size 1000 Size of the blocking queue to hold events to be pushed for the specific stream max.subscriptions.per.stream.default 20 Default maximum number of subscriptions per stream. After the limit is reached, further subscriptions on that stream are rejected max.subscriptions.stream. stream name 20 Overrides the default maximum number of subscriptions for the specific stream subs.refresh.interval.sec 1 Interval in seconds when subscriptions are fetched. In the default implementation, subscriptions are fetched over http from the workers returned by Discovery API drainer.interval.msec 100 Interval in milliseconds when events are drained from the stream queue and delegated to underlying transmitter for sending subs.expiry.interval.sec 300 Duration in seconds between a subscription is last fetched and when it is removed jobdiscovery.refresh.interval.sec 10 Duration in seconds between workers are refreshed for a job cluster jobcluster.mapping.refresh.interval.sec 60 Duration in seconds between job cluster mapping is refreshed for the current application deepcopy.eventmap.enabled true Determine if event processing should operate on a deep copy of the event. Otherwise the event object is processed directly subs.refresh.max.num.workers 3 Maximum number of mantis workers to fetch subscription from. Workers are randomly chosen from the list returned by Discovery API subs.fetch.query.params.string \"\" Additional query params to pass to the api call to fetch subscription. It should be of the form \"param1=value1 param2=value2\" channel.gzip.enabled true Netty channel configuration for pushing events. Determine if events should be gzip encoded when send over the channel channel.idleTimeout.sec 300 Netty channel configuration for pushing events. Write idle timeout in seconds for the channel channel.writeTimeout.sec 1 Netty channel configuration for pushing events. Write timeout in seconds for the channel channel.httpChunkSize.bytes 32768 (32 KiB) Netty channel configuration for pushing events. Chunked size in bytes of the channel content. It is used by HttpObjectAggregator channel.flushInterval.msec 50 Netty channel configuration for pushing events. Maximum duration in milliseconds between content flushes channel.flushInterval.bytes 524288 Netty channel configuration for pushing events. Content is flushed when aggregated event size is above this threshold channel.lowWriteBufferWatermark.bytes 1572864 Netty channel configuration for pushing events. Used for setting write buffer watermark channel.highWriteBufferWatermark.bytes 2097152 Netty channel configuration for pushing events. Used for setting write buffer watermark channel.ioThreads 1 Netty channel configuration for pushing events. Number of threads in the eventLoopGroup channel.compressionThreads 1 Netty channel configuration for pushing events. Number of threads in the encoderEventLoopGroup when gzip is enabled workerpool.capacity 1000 Size of the pool of Mantis workers to push events to workerpool.refresh.internal.sec 10 Duration in seconds between Mantis workers are refreshed in the pool workerpool.worker.error.quota 60 Number of errors to receive from a Mantis worker before it is blacklisted in the pool workerpool.worker.error.timeout.sec 300 Duration in seconds after which a blacklisted Mantis worker may be reconsidered for selection","title":"Netty"},{"location":"develop/ingestion/netty/#choose-a-package","text":"MRE works with both Guice-enabled and standalone injectors.","title":"Choose a Package"},{"location":"develop/ingestion/netty/#guice-based-injector","text":"implementation io.mantisrx:mantis-publish-netty-guice:1.2.+","title":"Guice-based Injector"},{"location":"develop/ingestion/netty/#enable-the-publisher-client","text":"Inject the MantisRealtimeEventsPublishModule into your application. In addition to injecting MantisRealtimeEventsPublishModule you will also need to add the ArchaiusModule and the SpectatorModule if not already injected. Injector injector = Guice . createInjector ( new MyBasicModule (), new ArchaiusModule (), new MantisRealtimeEventsPublishModule (), new SpectatorModule ());","title":"Enable the Publisher Client"},{"location":"develop/ingestion/netty/#standalone-injector","text":"implementation io.mantisrx:mantis-publish-netty:1.2.+","title":"Standalone Injector"},{"location":"develop/ingestion/netty/#enable-the-publisher-client_1","text":"Follow the example standalone initializer to manually inject dependencies. Once you have constructed a MrePublishClientInitializer , call the MantisPublishClientInitializer#start method to initialize underlying components.","title":"Enable the Publisher Client"},{"location":"develop/ingestion/netty/#configure-where-to-send-svents","text":"You will need to configure the location of the Mantis API server for the mantis-publish library to bootstrap. Add the following properties to your application.properties : mantis.publish.discovery.api.hostname = IP of Mantis API # mantis api port mantis.publish.discovery.api.port = port for Mantis API # This application s name mantis.publish.app.name = JavaApp","title":"Configure Where to Send Svents"},{"location":"develop/ingestion/netty/#send-events-into-mantis","text":"For each event your application wishes to send to Mantis, create a Event object with your desired event fields, and pass that Event to the MantisEventPublisher#publish method. For example: // Create an `Event` for Mantis Publish using your application event. final Event event = new Event (); event . set ( testKey , testValue ); // Send your `Event` into Mantis. // Note: This event will only be dispatched over the network // if a subscription with a matching MQL query exists. eventPublisher . publish ( event );","title":"Send Events into Mantis"},{"location":"develop/ingestion/netty/#consuming-a-mantis-stream","text":"Visit the Querying page for details on how you can consume your application's event stream.","title":"Consuming a Mantis Stream"},{"location":"develop/ingestion/netty/#configuration-options","text":"There are a number of configuration options available to control the behavior of the publishing client. Prefix: mantis.publish Name Default Description enabled true Determine if event processing is enabled. tee.enabled false Allows events to simultaneously be sent to an external system outside of Mantis tee.stream default_stream Specifies which external stream name tee will write to blacklist param.password Comma separated list of field names where the value will be obfuscated max.num.streams 5 Maximum number of streams this application can create stream.inactive.duration.threshold.sec 86400 (24 hours) Maximum duration in seconds for the stream to be considered inactive if there are no events stream name .stream.queue.size 1000 Size of the blocking queue to hold events to be pushed for the specific stream max.subscriptions.per.stream.default 20 Default maximum number of subscriptions per stream. After the limit is reached, further subscriptions on that stream are rejected max.subscriptions.stream. stream name 20 Overrides the default maximum number of subscriptions for the specific stream subs.refresh.interval.sec 1 Interval in seconds when subscriptions are fetched. In the default implementation, subscriptions are fetched over http from the workers returned by Discovery API drainer.interval.msec 100 Interval in milliseconds when events are drained from the stream queue and delegated to underlying transmitter for sending subs.expiry.interval.sec 300 Duration in seconds between a subscription is last fetched and when it is removed jobdiscovery.refresh.interval.sec 10 Duration in seconds between workers are refreshed for a job cluster jobcluster.mapping.refresh.interval.sec 60 Duration in seconds between job cluster mapping is refreshed for the current application deepcopy.eventmap.enabled true Determine if event processing should operate on a deep copy of the event. Otherwise the event object is processed directly subs.refresh.max.num.workers 3 Maximum number of mantis workers to fetch subscription from. Workers are randomly chosen from the list returned by Discovery API subs.fetch.query.params.string \"\" Additional query params to pass to the api call to fetch subscription. It should be of the form \"param1=value1 param2=value2\" channel.gzip.enabled true Netty channel configuration for pushing events. Determine if events should be gzip encoded when send over the channel channel.idleTimeout.sec 300 Netty channel configuration for pushing events. Write idle timeout in seconds for the channel channel.writeTimeout.sec 1 Netty channel configuration for pushing events. Write timeout in seconds for the channel channel.httpChunkSize.bytes 32768 (32 KiB) Netty channel configuration for pushing events. Chunked size in bytes of the channel content. It is used by HttpObjectAggregator channel.flushInterval.msec 50 Netty channel configuration for pushing events. Maximum duration in milliseconds between content flushes channel.flushInterval.bytes 524288 Netty channel configuration for pushing events. Content is flushed when aggregated event size is above this threshold channel.lowWriteBufferWatermark.bytes 1572864 Netty channel configuration for pushing events. Used for setting write buffer watermark channel.highWriteBufferWatermark.bytes 2097152 Netty channel configuration for pushing events. Used for setting write buffer watermark channel.ioThreads 1 Netty channel configuration for pushing events. Number of threads in the eventLoopGroup channel.compressionThreads 1 Netty channel configuration for pushing events. Number of threads in the encoderEventLoopGroup when gzip is enabled workerpool.capacity 1000 Size of the pool of Mantis workers to push events to workerpool.refresh.internal.sec 10 Duration in seconds between Mantis workers are refreshed in the pool workerpool.worker.error.quota 60 Number of errors to receive from a Mantis worker before it is blacklisted in the pool workerpool.worker.error.timeout.sec 300 Duration in seconds after which a blacklisted Mantis worker may be reconsidered for selection","title":"Configuration Options"},{"location":"develop/querying/mql/","text":"Mantis Query Language (MQL) is a dialect of SQL implemented as an abstraction over RxJava Observables . The purpose of MQL is to make it easy for Mantis users to query, transform, and analyze data flowing through Mantis. MQL maintains high fidelity to SQL syntax while adding capabilities for dealing with JSON structure, as well as a rich set of aggregates for answering analytical queries about the data in question. Consuming a Mantis Stream There are a few ways to consume a Mantis Stream. Using the Low-Level Library To use MQL, include the mql-jvm library in your dependencies. Refer to the example below for a minimalist getting started guide. Imagine we have a source of Map String, Object representing your data (recall that Mantis events can be easily parsed as such). Executing a query against that Observable is only a matter of putting that Observable into a Map String, Observable to represent the context and calling evalMql against said context. The result will be an Observable that represents the results of the query: package my.package ; import java.util.HashMap ; class MqlExample { public static void main ( String [] args ) { // Create a test observable source of x, y coordinates. Observable HashMap String , Object source = Observable . interval ( 100 , TimeUnit . MILLISECONDS ). map ( x - { HashMap String , Object d = new HashMap (); d . put ( x , x ); d . put ( y , x ); }); HashMap String , Observable HashMap String , Object context = new HashMap (); context . put ( observations , source ); // You don t have to block, and shouldn t. It is just to keep the example running. io . mantisrx . mql . Core . evalMql ( select y from observations where x 50 OR y == 10 , context ). toBlocking (). forEach ( System . out :: println ); }","title":"Mantis Query Language"},{"location":"develop/querying/mql/#consuming-a-mantis-stream","text":"There are a few ways to consume a Mantis Stream.","title":"Consuming a Mantis Stream"},{"location":"develop/querying/mql/#using-the-low-level-library","text":"To use MQL, include the mql-jvm library in your dependencies. Refer to the example below for a minimalist getting started guide. Imagine we have a source of Map String, Object representing your data (recall that Mantis events can be easily parsed as such). Executing a query against that Observable is only a matter of putting that Observable into a Map String, Observable to represent the context and calling evalMql against said context. The result will be an Observable that represents the results of the query: package my.package ; import java.util.HashMap ; class MqlExample { public static void main ( String [] args ) { // Create a test observable source of x, y coordinates. Observable HashMap String , Object source = Observable . interval ( 100 , TimeUnit . MILLISECONDS ). map ( x - { HashMap String , Object d = new HashMap (); d . put ( x , x ); d . put ( y , x ); }); HashMap String , Observable HashMap String , Object context = new HashMap (); context . put ( observations , source ); // You don t have to block, and shouldn t. It is just to keep the example running. io . mantisrx . mql . Core . evalMql ( select y from observations where x 50 OR y == 10 , context ). toBlocking (). forEach ( System . out :: println ); }","title":"Using the Low-Level Library"},{"location":"develop/querying/sampling/","text":"Sampling in MQL mitigates data volume issues. There are two sampling strategies \u2014 Random and Sticky: Random Sampling Random sampling uniformly downsamples the stream to a percentage of its original volume. You establish random sampling through a sampling clause like the following: select * from stream SAMPLE { strategy : RANDOM , threshold : 200, factor : 10000} For each item in the stream, a numeric hash is generated. That hash is modded by factor to produce a result between 0 (inclusive) and factor (exclusive). If that result is less than threshold the item will be sampled, otherwise it will be ignored. You can determine the sampling percentage by remembering that \\frac{threshold}{factor} \\frac{threshold}{factor} values will be sampled \u2014 2% in the example above with a threshold of 200 and factor of 10000. You can also set a salt value, which will change the calculation of the hash. This is helpful if you want to sample over the same set of values but retrieve a different sample of those values. For example: select * from stream SAMPLE { strategy : RANDOM , threshold : 200, factor : 10000, salt : 123} Sticky Sampling Sticky sampling \u201csticks\u201d to certain values for the provided keys. That is if you are sampling on \u201c zipcode \u201d (as in the following example) and you observe a specific zipcode in the stream, you will observe all events which contain that specific zipcode . Sticky sampling can be achieved with a query like such: select * from stream SAMPLE { strategy : STICKY , keys :[ zipcode ], threshold :200, factor :10000, salt : 1} The query above should retrieve 2% of the total stream (see Random Sampling above for why), predicated on the events being uniformly distributed over the zipcode s.","title":"Sampling"},{"location":"develop/querying/sampling/#random-sampling","text":"Random sampling uniformly downsamples the stream to a percentage of its original volume. You establish random sampling through a sampling clause like the following: select * from stream SAMPLE { strategy : RANDOM , threshold : 200, factor : 10000} For each item in the stream, a numeric hash is generated. That hash is modded by factor to produce a result between 0 (inclusive) and factor (exclusive). If that result is less than threshold the item will be sampled, otherwise it will be ignored. You can determine the sampling percentage by remembering that \\frac{threshold}{factor} \\frac{threshold}{factor} values will be sampled \u2014 2% in the example above with a threshold of 200 and factor of 10000. You can also set a salt value, which will change the calculation of the hash. This is helpful if you want to sample over the same set of values but retrieve a different sample of those values. For example: select * from stream SAMPLE { strategy : RANDOM , threshold : 200, factor : 10000, salt : 123}","title":"Random Sampling"},{"location":"develop/querying/sampling/#sticky-sampling","text":"Sticky sampling \u201csticks\u201d to certain values for the provided keys. That is if you are sampling on \u201c zipcode \u201d (as in the following example) and you observe a specific zipcode in the stream, you will observe all events which contain that specific zipcode . Sticky sampling can be achieved with a query like such: select * from stream SAMPLE { strategy : STICKY , keys :[ zipcode ], threshold :200, factor :10000, salt : 1} The query above should retrieve 2% of the total stream (see Random Sampling above for why), predicated on the events being uniformly distributed over the zipcode s.","title":"Sticky Sampling"},{"location":"develop/writing-jobs/group-by/","text":"Writing Your Third Mantis Job: Group By / Aggregate NOTE: This tutorial is a work in progress. Until now we've run single stage Mantis jobs which a can run in a single process / container. Much of the power provided by Mantis is that we can design and implement a distributed job. Let's take a look at the groupby-sample job definition and then break it down stage by stage. @Override public Job String getJobInstance () { return MantisJob // Stream Request Events from our random data generator source . source ( new RandomRequestSource ()) // Groups requests by path . stage ( new GroupByStage (), GroupByStage . config ()) // Computes count per path over a window . stage ( new AggregationStage (), AggregationStage . config ()) // Collects the data and makes it availabe over SSE . stage ( new CollectStage (), CollectStage . config ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ))) . metadata ( new Metadata . Builder () . name ( GroupByPath ) . description ( Connects to a random data generator source + and counts the number of requests for each uri within a window ) . build ()) . create (); } The job definition above should look relatively familiar with the exception of the fact that our job has three stages. These stages and their configurations are all relatively simple but take advantage of the scalability of Mantis when used in conjunction with each other; The GroupByStage will group events according to some user specified criteria. The AggregationStage will perform an aggregation on a per-group basis. The CollectStage will collect all aggregations and create a report. Let's explore each of these stages in sequence. Stage 1: Group By Stage The GroupByStage implements ToGroupComputation RequestEvent, String, RequestEvent which tells Mantis that the call method will be returning MantisGroup String, RequestEvent which represents a group key and the data. You may recall in the previous tutorials we used the reactive groupBy operator to group data. The reactive operator performs an in-memory group by which has some scaling limitations. Grouping data using a Mantis stage does not have the same limitations and allows us to scale the group by operation across multiple containers. The config() method also changes to specify this different stage type. It also allows us to specify concurrentInput() on the config allowing the call method to be run concurrently in this container. Also note the result of getParameters() is added to the config via the withParameters method, it isn't an interface method for stages but we're specifying parameters this way for convenience. public class GroupByStage implements ToGroupComputation RequestEvent , String , RequestEvent { private static final String GROUPBY_FIELD_PARAM = groupByField ; private boolean groupByPath = true ; @Override public Observable MantisGroup String , RequestEvent call ( Context context , Observable RequestEvent requestEventO ) { return requestEventO . map (( requestEvent ) - { if ( groupByPath ) { return new MantisGroup ( requestEvent . getRequestPath (), requestEvent ); } else { return new MantisGroup ( requestEvent . getIpAddress (), requestEvent ); } }); } @Override public void init ( Context context ) { String groupByField = ( String ) context . getParameters (). get ( GROUPBY_FIELD_PARAM , path ); groupByPath = groupByField . equalsIgnoreCase ( path ) ? true : false ; } /** * Here we declare stage specific parameters. * @return */ public static List ParameterDefinition ? getParameters () { List ParameterDefinition ? params = new ArrayList (); // Group by field params . add ( new StringParameter () . name ( GROUPBY_FIELD_PARAM ) . description ( The key to group events by ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( path ) . build ()) ; return params ; } public static ScalarToGroup . Config RequestEvent , String , RequestEvent config (){ return new ScalarToGroup . Config RequestEvent , String , RequestEvent () . description ( Group event data by path/ip ) . concurrentInput () // signifies events can be processed in parallel . withParameters ( getParameters ()) . codec ( RequestEvent . requestEventCodec ()); } } We should note that this stage is horizontally scalable. We can run as many of these as necessary to handle the inflow of data which is how Mantis allows us to run a scalable group by operation. Stage 2: Aggregation Stage The previous stage produces MantisGroup String, RequestEvent by implementing ToGroupComputation . If we think of ToGroupComputation producing groups of data we can think of GroupToScalarComputation K, T, R as the inverse computing a scalar value from a group. The init() , config() and getParameters() methods should be familiar by now but let's take a closer look at the call() method. We're performing the same operation the stream as we did in the first two tutorials. Notice how we still group by MantisGroup::getKeyValue . This is because while Mantis guarantees that all data for an individual group from the previous stage will land on the same container in this stage, it does not invoke the call() method for each individual group and thus we need to handle the groups ourselves. @Slf4j public class AggregationStage implements GroupToScalarComputation String , RequestEvent , RequestAggregation { public static final String AGGREGATION_DURATION_MSEC_PARAM = AggregationDurationMsec ; int aggregationDurationMsec ; /** * The call method is invoked by the Mantis runtime while executing the job. * @param context Provides metadata information related to the current job. * @param mantisGroupO This is an Observable of {@link MantisGroup} events. Each event is a pair of the Key - uri Path and * the {@link RequestEvent} event itself. * @return */ @Override public Observable RequestAggregation call ( Context context , Observable MantisGroup String , RequestEvent mantisGroupO ) { return mantisGroupO . window ( aggregationDurationMsec , TimeUnit . MILLISECONDS ) . flatMap (( omg ) - omg . groupBy ( MantisGroup :: getKeyValue ) . flatMap (( go ) - go . reduce ( 0 , ( accumulator , value ) - accumulator = accumulator + 1 ) . map (( count ) - RequestAggregation . builder (). count ( count ). path ( go . getKey ()). build ()) . doOnNext (( aggregate ) - { log . debug ( Generated aggregate {} , aggregate ); }) )); } /** * Invoked only once during job startup. A good place to add one time initialization actions. * @param context */ @Override public void init ( Context context ) { aggregationDurationMsec = ( int ) context . getParameters (). get ( AGGREGATION_DURATION_MSEC_PARAM , 1000 ); } /** * Provides the Mantis runtime configuration information about the type of computation done by this stage. * E.g in this case it specifies this is a GroupToScalar computation and also provides a {@link Codec} on how to * serialize the {@link RequestAggregation} events before sending it to the {@link CollectStage} * @return */ public static GroupToScalar . Config String , RequestEvent , RequestAggregation config (){ return new GroupToScalar . Config String , RequestEvent , RequestAggregation () . description ( sum events for a path ) . codec ( RequestAggregation . requestAggregationCodec ()) . withParameters ( getParameters ()); } /** * Here we declare stage specific parameters. * @return */ public static List ParameterDefinition ? getParameters () { List ParameterDefinition ? params = new ArrayList (); // Aggregation duration params . add ( new IntParameter () . name ( AGGREGATION_DURATION_MSEC_PARAM ) . description ( window size for aggregation ) . validator ( Validators . range ( 100 , 10000 )) . defaultValue ( 5000 ) . build ()) ; return params ; } } Much like the previous stage this stage is also horizontally scalable up to the cardinality of our group by. Allowing us to individually (or automatically via autoscaling) scale these two stages ensures this job can be correctly sized for the workload. Stage 3: Collect Stage The third stage collects data from all of the upstream workers and generates a report. We can see in the code below the stream is windowed for five seconds, then we flatmap a reduce over the window and invoke the RequestAggregationAccumulator#generateReport() method on the reduced value. @Slf4j public class CollectStage implements ScalarComputation RequestAggregation , String { private static final ObjectMapper mapper = new ObjectMapper (); @Override public Observable String call ( Context context , Observable RequestAggregation requestAggregationO ) { return requestAggregationO . window ( 5 , TimeUnit . SECONDS ) . flatMap (( requestAggO ) - requestAggO . reduce ( new RequestAggregationAccumulator (),( acc , requestAgg ) - acc . addAggregation ( requestAgg )) . map ( RequestAggregationAccumulator :: generateReport ) . doOnNext (( report ) - { log . debug ( Generated Collection report {} , report ); }) ) . map (( report ) - { try { return mapper . writeValueAsString ( report ); } catch ( JsonProcessingException e ) { log . error ( e . getMessage ()); return null ; } }). filter ( Objects :: nonNull ); } @Override public void init ( Context context ) { } public static ScalarToScalar . Config RequestAggregation , String config (){ return new ScalarToScalar . Config RequestAggregation , String () . codec ( Codecs . string ()); } /** * The accumulator class as the name suggests accumulates all aggregates seen during a window and * generates a consolidated report at the end. */ static class RequestAggregationAccumulator { private final Map String , Integer pathToCountMap = new HashMap (); public RequestAggregationAccumulator () {} public RequestAggregationAccumulator addAggregation ( RequestAggregation agg ) { pathToCountMap . put ( agg . getPath (), agg . getCount ()); return this ; } public AggregationReport generateReport () { log . info ( Generated report from= {} , pathToCountMap ); return new AggregationReport ( pathToCountMap ); } } } Unlike the previous stages we only run a single collect stage which gathers all the data in five second batches and generates a report to be output on the sink. Conclusion We've now explored the concept of a multi-stage Mantis job which allows us to horizontally scale individual stages and express group by and aggregate semantics as a Mantis topology.","title":"Writing Your Third Job"},{"location":"develop/writing-jobs/group-by/#writing-your-third-mantis-job-group-by-aggregate","text":"NOTE: This tutorial is a work in progress. Until now we've run single stage Mantis jobs which a can run in a single process / container. Much of the power provided by Mantis is that we can design and implement a distributed job. Let's take a look at the groupby-sample job definition and then break it down stage by stage. @Override public Job String getJobInstance () { return MantisJob // Stream Request Events from our random data generator source . source ( new RandomRequestSource ()) // Groups requests by path . stage ( new GroupByStage (), GroupByStage . config ()) // Computes count per path over a window . stage ( new AggregationStage (), AggregationStage . config ()) // Collects the data and makes it availabe over SSE . stage ( new CollectStage (), CollectStage . config ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ))) . metadata ( new Metadata . Builder () . name ( GroupByPath ) . description ( Connects to a random data generator source + and counts the number of requests for each uri within a window ) . build ()) . create (); } The job definition above should look relatively familiar with the exception of the fact that our job has three stages. These stages and their configurations are all relatively simple but take advantage of the scalability of Mantis when used in conjunction with each other; The GroupByStage will group events according to some user specified criteria. The AggregationStage will perform an aggregation on a per-group basis. The CollectStage will collect all aggregations and create a report. Let's explore each of these stages in sequence.","title":"Writing Your Third Mantis Job: Group By / Aggregate"},{"location":"develop/writing-jobs/group-by/#stage-1-group-by-stage","text":"The GroupByStage implements ToGroupComputation RequestEvent, String, RequestEvent which tells Mantis that the call method will be returning MantisGroup String, RequestEvent which represents a group key and the data. You may recall in the previous tutorials we used the reactive groupBy operator to group data. The reactive operator performs an in-memory group by which has some scaling limitations. Grouping data using a Mantis stage does not have the same limitations and allows us to scale the group by operation across multiple containers. The config() method also changes to specify this different stage type. It also allows us to specify concurrentInput() on the config allowing the call method to be run concurrently in this container. Also note the result of getParameters() is added to the config via the withParameters method, it isn't an interface method for stages but we're specifying parameters this way for convenience. public class GroupByStage implements ToGroupComputation RequestEvent , String , RequestEvent { private static final String GROUPBY_FIELD_PARAM = groupByField ; private boolean groupByPath = true ; @Override public Observable MantisGroup String , RequestEvent call ( Context context , Observable RequestEvent requestEventO ) { return requestEventO . map (( requestEvent ) - { if ( groupByPath ) { return new MantisGroup ( requestEvent . getRequestPath (), requestEvent ); } else { return new MantisGroup ( requestEvent . getIpAddress (), requestEvent ); } }); } @Override public void init ( Context context ) { String groupByField = ( String ) context . getParameters (). get ( GROUPBY_FIELD_PARAM , path ); groupByPath = groupByField . equalsIgnoreCase ( path ) ? true : false ; } /** * Here we declare stage specific parameters. * @return */ public static List ParameterDefinition ? getParameters () { List ParameterDefinition ? params = new ArrayList (); // Group by field params . add ( new StringParameter () . name ( GROUPBY_FIELD_PARAM ) . description ( The key to group events by ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( path ) . build ()) ; return params ; } public static ScalarToGroup . Config RequestEvent , String , RequestEvent config (){ return new ScalarToGroup . Config RequestEvent , String , RequestEvent () . description ( Group event data by path/ip ) . concurrentInput () // signifies events can be processed in parallel . withParameters ( getParameters ()) . codec ( RequestEvent . requestEventCodec ()); } } We should note that this stage is horizontally scalable. We can run as many of these as necessary to handle the inflow of data which is how Mantis allows us to run a scalable group by operation.","title":"Stage 1: Group By Stage"},{"location":"develop/writing-jobs/group-by/#stage-2-aggregation-stage","text":"The previous stage produces MantisGroup String, RequestEvent by implementing ToGroupComputation . If we think of ToGroupComputation producing groups of data we can think of GroupToScalarComputation K, T, R as the inverse computing a scalar value from a group. The init() , config() and getParameters() methods should be familiar by now but let's take a closer look at the call() method. We're performing the same operation the stream as we did in the first two tutorials. Notice how we still group by MantisGroup::getKeyValue . This is because while Mantis guarantees that all data for an individual group from the previous stage will land on the same container in this stage, it does not invoke the call() method for each individual group and thus we need to handle the groups ourselves. @Slf4j public class AggregationStage implements GroupToScalarComputation String , RequestEvent , RequestAggregation { public static final String AGGREGATION_DURATION_MSEC_PARAM = AggregationDurationMsec ; int aggregationDurationMsec ; /** * The call method is invoked by the Mantis runtime while executing the job. * @param context Provides metadata information related to the current job. * @param mantisGroupO This is an Observable of {@link MantisGroup} events. Each event is a pair of the Key - uri Path and * the {@link RequestEvent} event itself. * @return */ @Override public Observable RequestAggregation call ( Context context , Observable MantisGroup String , RequestEvent mantisGroupO ) { return mantisGroupO . window ( aggregationDurationMsec , TimeUnit . MILLISECONDS ) . flatMap (( omg ) - omg . groupBy ( MantisGroup :: getKeyValue ) . flatMap (( go ) - go . reduce ( 0 , ( accumulator , value ) - accumulator = accumulator + 1 ) . map (( count ) - RequestAggregation . builder (). count ( count ). path ( go . getKey ()). build ()) . doOnNext (( aggregate ) - { log . debug ( Generated aggregate {} , aggregate ); }) )); } /** * Invoked only once during job startup. A good place to add one time initialization actions. * @param context */ @Override public void init ( Context context ) { aggregationDurationMsec = ( int ) context . getParameters (). get ( AGGREGATION_DURATION_MSEC_PARAM , 1000 ); } /** * Provides the Mantis runtime configuration information about the type of computation done by this stage. * E.g in this case it specifies this is a GroupToScalar computation and also provides a {@link Codec} on how to * serialize the {@link RequestAggregation} events before sending it to the {@link CollectStage} * @return */ public static GroupToScalar . Config String , RequestEvent , RequestAggregation config (){ return new GroupToScalar . Config String , RequestEvent , RequestAggregation () . description ( sum events for a path ) . codec ( RequestAggregation . requestAggregationCodec ()) . withParameters ( getParameters ()); } /** * Here we declare stage specific parameters. * @return */ public static List ParameterDefinition ? getParameters () { List ParameterDefinition ? params = new ArrayList (); // Aggregation duration params . add ( new IntParameter () . name ( AGGREGATION_DURATION_MSEC_PARAM ) . description ( window size for aggregation ) . validator ( Validators . range ( 100 , 10000 )) . defaultValue ( 5000 ) . build ()) ; return params ; } } Much like the previous stage this stage is also horizontally scalable up to the cardinality of our group by. Allowing us to individually (or automatically via autoscaling) scale these two stages ensures this job can be correctly sized for the workload.","title":"Stage 2: Aggregation Stage"},{"location":"develop/writing-jobs/group-by/#stage-3-collect-stage","text":"The third stage collects data from all of the upstream workers and generates a report. We can see in the code below the stream is windowed for five seconds, then we flatmap a reduce over the window and invoke the RequestAggregationAccumulator#generateReport() method on the reduced value. @Slf4j public class CollectStage implements ScalarComputation RequestAggregation , String { private static final ObjectMapper mapper = new ObjectMapper (); @Override public Observable String call ( Context context , Observable RequestAggregation requestAggregationO ) { return requestAggregationO . window ( 5 , TimeUnit . SECONDS ) . flatMap (( requestAggO ) - requestAggO . reduce ( new RequestAggregationAccumulator (),( acc , requestAgg ) - acc . addAggregation ( requestAgg )) . map ( RequestAggregationAccumulator :: generateReport ) . doOnNext (( report ) - { log . debug ( Generated Collection report {} , report ); }) ) . map (( report ) - { try { return mapper . writeValueAsString ( report ); } catch ( JsonProcessingException e ) { log . error ( e . getMessage ()); return null ; } }). filter ( Objects :: nonNull ); } @Override public void init ( Context context ) { } public static ScalarToScalar . Config RequestAggregation , String config (){ return new ScalarToScalar . Config RequestAggregation , String () . codec ( Codecs . string ()); } /** * The accumulator class as the name suggests accumulates all aggregates seen during a window and * generates a consolidated report at the end. */ static class RequestAggregationAccumulator { private final Map String , Integer pathToCountMap = new HashMap (); public RequestAggregationAccumulator () {} public RequestAggregationAccumulator addAggregation ( RequestAggregation agg ) { pathToCountMap . put ( agg . getPath (), agg . getCount ()); return this ; } public AggregationReport generateReport () { log . info ( Generated report from= {} , pathToCountMap ); return new AggregationReport ( pathToCountMap ); } } } Unlike the previous stages we only run a single collect stage which gathers all the data in five second batches and generates a report to be output on the sink.","title":"Stage 3: Collect Stage"},{"location":"develop/writing-jobs/group-by/#conclusion","text":"We've now explored the concept of a multi-stage Mantis job which allows us to horizontally scale individual stages and express group by and aggregate semantics as a Mantis topology.","title":"Conclusion"},{"location":"develop/writing-jobs/twitter/","text":"Writing Your Second Mantis Job Our first tutorial primed us for writing and executing a job end-to-end but it wasn't particularly interesting from a data perspective because it just repeatedly looped over the contents of a book. In this example we'll explore writing a more involved source which reads an infinite stream of data from Twitter and performs the same word count in real-time. Mantis jobs can easily subscribe to one another using some built in sources but the technique in this tutorial can be used to pull external data into the Mantis ecosystem. To proceed you'll need to head over to Twitter and grab yourself a pair of API keys. The Source The source is responsible for ingesting data to be processed within the job. Many Mantis jobs will subscribe to other jobs and can simply use a templatized source such as io.mantisrx.connectors.job.source.JobSource which handles all the minutiae of connecting to other jobs for us. If however your job exists on the edge of Mantis it will need to pull data in via a custom source. Since we're reading from the Twitter API we'll need to do this ourselves. Our TwitterSource must implement io.mantisrx.runtime.source.Source which requires us to implement call and optionally init . Mantis provides some guarantees here in that init will be invoked exactly once and before call which will be invoked at least once. This makes init the ideal location to perform one time setup and configuration for the source and call the ideal location for performing work on the incoming stream. The objective of this entire class is to have call return an Observable Observable T which will be passed as a parameter to the first stage of our job. Let's deconstruct the init method first. Here we will extract our parameters from the Context -- this allows us to write more generic sources which can be templatized and reused across many jobs. This is a very common pattern for writing Mantis jobs and allows you to iterate quickly testing various configurations as jobs can be resubmitted easily with new parameters. /** * Init method is called only once during initialization. It is the ideal place to perform one time * configuration actions. * * @param context Provides access to Mantis system information like JobId, Job parameters etc * @param index This provides access to the unique workerIndex assigned to this container. It also provides * the total number of workers of this job. */ @Override public void init ( Context context , Index index ) { String consumerKey = ( String ) context . getParameters (). get ( CONSUMER_KEY_PARAM ); String consumerSecret = ( String ) context . getParameters (). get ( CONSUMER_SECRET_PARAM ); String token = ( String ) context . getParameters (). get ( TOKEN_PARAM ); String tokenSecret = ( String ) context . getParameters (). get ( TOKEN_SECRET_PARAM ); String terms = ( String ) context . getParameters (). get ( TERMS_PARAM ); Authentication auth = new OAuth1 ( consumerKey , consumerSecret , token , tokenSecret ); StatusesFilterEndpoint endpoint = new StatusesFilterEndpoint (); String [] termArray = terms . split ( , ); List String termsList = Arrays . asList ( termArray ); endpoint . trackTerms ( termsList ); client = new ClientBuilder () . name ( twitter-source ) . hosts ( Constants . STREAM_HOST ) . endpoint ( endpoint ) . authentication ( auth ) . processor ( new StringDelimitedProcessor ( twitterObservable )) . build (); client . connect (); } Our call method is very simple thanks to the fact that our twitter client writes to a custom BlockingQueue adapter that we've written. We simply need to return an Observable Observable T . @Override public Observable Observable String call ( Context context , Index index ) { return Observable . just ( twitterObservable . observe ()); } You may have noticed that our init method is pulling a bunch of parameters out of the Context . These are specified in Source#getParameters() and allow us to parameterize this source so that different instances of this job may work with different parameters. This is a very useful concept for designing reusable components for constructing jobs as well as completely reusable jobs. /** * Define parameters required by this source. * * @return */ @Override public List ParameterDefinition ? getParameters () { List ParameterDefinition ? params = Lists . newArrayList (); // Consumer key params . add ( new StringParameter () . name ( CONSUMER_KEY_PARAM ) . description ( twitter consumer key ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( CONSUMER_SECRET_PARAM ) . description ( twitter consumer secret ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( TOKEN_PARAM ) . description ( twitter token ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( TOKEN_SECRET_PARAM ) . description ( twitter token secret ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( TERMS_PARAM ) . description ( terms to follow ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( Netflix,Dark ) . build ()); return params ; } Now our primary class TwitterJob which implements MantisJobProvider needs to specify our new source so change the source line to match the following. . source ( new TwitterSource ()) The Stage The stage is nearly equivalent to the previous tutorial. We need to add a few lines to the beginning of the chain of operations to deserialize the string, filter for English Tweets and pluck out the text. . stage (( context , dataO ) - dataO // Deserialize data . map ( JsonUtility :: jsonToMap ) // Filter for English Tweets . filter (( eventMap ) - { if ( eventMap . containsKey ( lang ) eventMap . containsKey ( text )) { String lang = ( String ) eventMap . get ( lang ); return en . equalsIgnoreCase ( lang ); } return false ; }) // Extract Tweet body . map (( eventMap ) - ( String ) eventMap . get ( text )) // Same from here... Conclusion We've learned how to create a parameterized source which reads from Twitter and pulls data into the ecosystem. With some slight modifications our previous example's stage deserializes the messages and extracts the data to perform the same word count. If you've checked out the mantis-examples repository then running ./gradlew :mantis-examples-twitter-sample:execute --args='consumerKey consumerSecret token tokensecret' at the root of the repository should begin running the job and expose a local port for SSE streaming. As an exercise consider how you might begin to scale this work out over multiple machines if the workload were too large to perform on a single host. This will be the topic of the next tutorial.","title":"Writing Your Second Job"},{"location":"develop/writing-jobs/twitter/#writing-your-second-mantis-job","text":"Our first tutorial primed us for writing and executing a job end-to-end but it wasn't particularly interesting from a data perspective because it just repeatedly looped over the contents of a book. In this example we'll explore writing a more involved source which reads an infinite stream of data from Twitter and performs the same word count in real-time. Mantis jobs can easily subscribe to one another using some built in sources but the technique in this tutorial can be used to pull external data into the Mantis ecosystem. To proceed you'll need to head over to Twitter and grab yourself a pair of API keys.","title":"Writing Your Second Mantis Job"},{"location":"develop/writing-jobs/twitter/#the-source","text":"The source is responsible for ingesting data to be processed within the job. Many Mantis jobs will subscribe to other jobs and can simply use a templatized source such as io.mantisrx.connectors.job.source.JobSource which handles all the minutiae of connecting to other jobs for us. If however your job exists on the edge of Mantis it will need to pull data in via a custom source. Since we're reading from the Twitter API we'll need to do this ourselves. Our TwitterSource must implement io.mantisrx.runtime.source.Source which requires us to implement call and optionally init . Mantis provides some guarantees here in that init will be invoked exactly once and before call which will be invoked at least once. This makes init the ideal location to perform one time setup and configuration for the source and call the ideal location for performing work on the incoming stream. The objective of this entire class is to have call return an Observable Observable T which will be passed as a parameter to the first stage of our job. Let's deconstruct the init method first. Here we will extract our parameters from the Context -- this allows us to write more generic sources which can be templatized and reused across many jobs. This is a very common pattern for writing Mantis jobs and allows you to iterate quickly testing various configurations as jobs can be resubmitted easily with new parameters. /** * Init method is called only once during initialization. It is the ideal place to perform one time * configuration actions. * * @param context Provides access to Mantis system information like JobId, Job parameters etc * @param index This provides access to the unique workerIndex assigned to this container. It also provides * the total number of workers of this job. */ @Override public void init ( Context context , Index index ) { String consumerKey = ( String ) context . getParameters (). get ( CONSUMER_KEY_PARAM ); String consumerSecret = ( String ) context . getParameters (). get ( CONSUMER_SECRET_PARAM ); String token = ( String ) context . getParameters (). get ( TOKEN_PARAM ); String tokenSecret = ( String ) context . getParameters (). get ( TOKEN_SECRET_PARAM ); String terms = ( String ) context . getParameters (). get ( TERMS_PARAM ); Authentication auth = new OAuth1 ( consumerKey , consumerSecret , token , tokenSecret ); StatusesFilterEndpoint endpoint = new StatusesFilterEndpoint (); String [] termArray = terms . split ( , ); List String termsList = Arrays . asList ( termArray ); endpoint . trackTerms ( termsList ); client = new ClientBuilder () . name ( twitter-source ) . hosts ( Constants . STREAM_HOST ) . endpoint ( endpoint ) . authentication ( auth ) . processor ( new StringDelimitedProcessor ( twitterObservable )) . build (); client . connect (); } Our call method is very simple thanks to the fact that our twitter client writes to a custom BlockingQueue adapter that we've written. We simply need to return an Observable Observable T . @Override public Observable Observable String call ( Context context , Index index ) { return Observable . just ( twitterObservable . observe ()); } You may have noticed that our init method is pulling a bunch of parameters out of the Context . These are specified in Source#getParameters() and allow us to parameterize this source so that different instances of this job may work with different parameters. This is a very useful concept for designing reusable components for constructing jobs as well as completely reusable jobs. /** * Define parameters required by this source. * * @return */ @Override public List ParameterDefinition ? getParameters () { List ParameterDefinition ? params = Lists . newArrayList (); // Consumer key params . add ( new StringParameter () . name ( CONSUMER_KEY_PARAM ) . description ( twitter consumer key ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( CONSUMER_SECRET_PARAM ) . description ( twitter consumer secret ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( TOKEN_PARAM ) . description ( twitter token ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( TOKEN_SECRET_PARAM ) . description ( twitter token secret ) . validator ( Validators . notNullOrEmpty ()) . required () . build ()); params . add ( new StringParameter () . name ( TERMS_PARAM ) . description ( terms to follow ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( Netflix,Dark ) . build ()); return params ; } Now our primary class TwitterJob which implements MantisJobProvider needs to specify our new source so change the source line to match the following. . source ( new TwitterSource ())","title":"The Source"},{"location":"develop/writing-jobs/twitter/#the-stage","text":"The stage is nearly equivalent to the previous tutorial. We need to add a few lines to the beginning of the chain of operations to deserialize the string, filter for English Tweets and pluck out the text. . stage (( context , dataO ) - dataO // Deserialize data . map ( JsonUtility :: jsonToMap ) // Filter for English Tweets . filter (( eventMap ) - { if ( eventMap . containsKey ( lang ) eventMap . containsKey ( text )) { String lang = ( String ) eventMap . get ( lang ); return en . equalsIgnoreCase ( lang ); } return false ; }) // Extract Tweet body . map (( eventMap ) - ( String ) eventMap . get ( text )) // Same from here...","title":"The Stage"},{"location":"develop/writing-jobs/twitter/#conclusion","text":"We've learned how to create a parameterized source which reads from Twitter and pulls data into the ecosystem. With some slight modifications our previous example's stage deserializes the messages and extracts the data to perform the same word count. If you've checked out the mantis-examples repository then running ./gradlew :mantis-examples-twitter-sample:execute --args='consumerKey consumerSecret token tokensecret' at the root of the repository should begin running the job and expose a local port for SSE streaming. As an exercise consider how you might begin to scale this work out over multiple machines if the workload were too large to perform on a single host. This will be the topic of the next tutorial.","title":"Conclusion"},{"location":"develop/writing-jobs/word-count/","text":"Writing Your First Mantis Job We'll be doing the classic word count example for streaming data for the tutorial section. For this example we'll be keeping it simple and focusing on the processing logic and job provider. The tutorials are structured progressively to allow us to incrementally build some experience writing jobs without getting overwhelmed with details. We'll stream text from a Project Gutenberg book, perform some application logic on the stream, and then write the data to a sink for consumption by other Mantis jobs. If you want to follow along check out the Word Count project in the mantis-examples repository. There are a few things to keep in mind when implementing a Mantis Job; We're just writing Java and there are a few interfaces necessary for Mantis Mantis jobs are composed of a source, n stages, and a sink. Mantis makes heavy use of Reactive Streams as a DSL for implementing processing logic. WordCountJob The full source of the WordCountJob class is included below with imports elided. This class implements the io.mantisrx.runtime.MantisJobProvider interface which the Mantis runtime loads. MantisJobProvider#getJobInstance() provides the runtime with an entry point to your job's code. /** * This sample demonstrates ingesting data from a text file and counting the number of occurrences of words within a 10 * sec hopping window. * Run the main method of this class and then look for a the SSE port in the output * E.g * code Serving modern HTTP SSE server sink on port: 8650 /code * You can curl this port code curl localhost:8650 /code to view the output of the job. * * To run via gradle * /gradlew :mantis-examples-wordcount:execute */ @Slf4j public class WordCountJob extends MantisJobProvider String { @Override public Job String getJobInstance () { return MantisJob . source ( new IlliadSource ()) // Ignore for now, we ll implement one in the next tutorial. . stage (( context , dataO ) - dataO // Tokenize . flatMap (( text ) - Observable . from ( tokenize ( text ))) // On a hopping window of 10 seconds . window ( 10 , TimeUnit . SECONDS ) . flatMap (( wordCountPairObservable ) - wordCountPairObservable // count how many times a word appears . groupBy ( WordCountPair :: getWord ) . flatMap (( groupO ) - groupO . reduce ( 0 , ( cnt , wordCntPair ) - cnt + 1 ) . map (( cnt ) - new WordCountPair ( groupO . getKey (), cnt )))) . map ( WordCountPair :: toString ) , StageConfigs . scalarToScalarConfig ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ))) . metadata ( new Metadata . Builder () . name ( WordCount ) . description ( Reads Homer s The Illiad faster than we can. ) . build ()) . create (); } private List WordCountPair tokenize ( String text ) { StringTokenizer tokenizer = new StringTokenizer ( text ); List WordCountPair wordCountPairs = new ArrayList (); while ( tokenizer . hasMoreTokens ()) { String word = tokenizer . nextToken (). replaceAll ( \\\\s* , ). toLowerCase (); wordCountPairs . add ( new WordCountPair ( word , 1 )); } return wordCountPairs ; } public static void main ( String [] args ) { LocalJobExecutorNetworked . execute ( new WordCountJob (). getJobInstance ()); } } There are several things going on here, let's examine them one at a time... The Source We specify our source in the line .source(new IlliadSource()) . The source handles data ingestion and it is very common to use a pre-existing parameterized source when writing jobs. Mantis provides several sources which handle managing connections and queries to other jobs. In the next tutorial we'll learn how to implement our own source which ingests data from Twitter. The Stage Our stage implements the bulk of the processing logic for the streaming job. Recall that a Mantis job has 1..n stages which can be used to create a topology for data processing. This stage is a ScalarComputation but we'll learn about other stage types in the third tutorial when we make word counting a distributed job. We'll take advantage of Java's lambda syntax to implement this stage inline. The call method receives a Context object and an Observable String provided by our source. The stage's responsibility is to produce an Observable R for consumption by down stream stages or the sink if this is the last stage. . stage (( context , dataO ) - dataO // Tokenize the string . flatMap (( text ) - Observable . from ( tokenize ( text ))) // Hopping / Tumbling window of 10 seconds . window ( 10 , TimeUnit . SECONDS ) // Reduce each window . flatMap (( wordCountPairObservable ) - wordCountPairObservable // count how many times a word appears . groupBy ( WordCountPair :: getWord ) . flatMap (( groupO ) - groupO . reduce ( 0 , ( cnt , wordCntPair ) - cnt + 1 ) . map (( cnt ) - new WordCountPair ( groupO . getKey (), cnt )))) // Convert the result to a string . map ( WordCountPair :: toString ) If you're familiar with reactive stream processing the above should be fairly easy to comprehend. Unfortunately if you aren't then an introduction to this is outside of the scope of this tutorial. Head over to reactivex.io to learn more about the concept. The stage configuration below specifies a few things; First that this stage is a scalar to scalar stage in that it ingests single events, and produces single events. The type of the input events is String, and the output is also String. Finally the configuration also specifies which Codec to use on the wire for this stage's output. You can use this configuration to specify concurrency for this stage as well, but we've not elected to do so here. public static ScalarToScalar . Config String , String scalarToScalarConfig () { return new ScalarToScalar . Config String , String () . codec ( Codecs . string ()); } The Job Provider The MantisJobProvider interface is what the Mantis runtime expects to load. The runtime reads resources/META-INF/services/io.mantisrx.runtime.MantisJobProvider to discover the fully qualified classname of the MantisJobProvider to be used as an entry point for the application. Main Method The main method invokes the LocalJobExecutorNetworked execute method to run our job locally. The first three tutorials will take advantage of the ability to run jobs locally. In the fourth tutorial we will explore uploading and submitting our job on a Mantis cloud deployment for greater scalability. We can and should run this main method by invoking ./gradlew :mantis-examples-wordcount:execute at the root of the mantis-examples directory. public static void main ( String [] args ) { LocalJobExecutorNetworked . execute ( new WordCountJob (). getJobInstance ()); } Conclusions and Future Work We've implemented a complete end-to-end Mantis job which counts words from The Illiad repeatedly. This leaves much to be desired. If you inspect our source we're really just iterating over the same data set every ten seconds. In the next tutorial we'll explore the task of writing our own custom source to pull external data from Twitter into Mantis and designing this source in a templated fashion so that it can be used with different queries and API keys. As an extra credit task see if you can modify the stage in this job to print the top 10 words instead of the entire list.","title":"Writing Your First Job"},{"location":"develop/writing-jobs/word-count/#writing-your-first-mantis-job","text":"We'll be doing the classic word count example for streaming data for the tutorial section. For this example we'll be keeping it simple and focusing on the processing logic and job provider. The tutorials are structured progressively to allow us to incrementally build some experience writing jobs without getting overwhelmed with details. We'll stream text from a Project Gutenberg book, perform some application logic on the stream, and then write the data to a sink for consumption by other Mantis jobs. If you want to follow along check out the Word Count project in the mantis-examples repository. There are a few things to keep in mind when implementing a Mantis Job; We're just writing Java and there are a few interfaces necessary for Mantis Mantis jobs are composed of a source, n stages, and a sink. Mantis makes heavy use of Reactive Streams as a DSL for implementing processing logic.","title":"Writing Your First Mantis Job"},{"location":"develop/writing-jobs/word-count/#wordcountjob","text":"The full source of the WordCountJob class is included below with imports elided. This class implements the io.mantisrx.runtime.MantisJobProvider interface which the Mantis runtime loads. MantisJobProvider#getJobInstance() provides the runtime with an entry point to your job's code. /** * This sample demonstrates ingesting data from a text file and counting the number of occurrences of words within a 10 * sec hopping window. * Run the main method of this class and then look for a the SSE port in the output * E.g * code Serving modern HTTP SSE server sink on port: 8650 /code * You can curl this port code curl localhost:8650 /code to view the output of the job. * * To run via gradle * /gradlew :mantis-examples-wordcount:execute */ @Slf4j public class WordCountJob extends MantisJobProvider String { @Override public Job String getJobInstance () { return MantisJob . source ( new IlliadSource ()) // Ignore for now, we ll implement one in the next tutorial. . stage (( context , dataO ) - dataO // Tokenize . flatMap (( text ) - Observable . from ( tokenize ( text ))) // On a hopping window of 10 seconds . window ( 10 , TimeUnit . SECONDS ) . flatMap (( wordCountPairObservable ) - wordCountPairObservable // count how many times a word appears . groupBy ( WordCountPair :: getWord ) . flatMap (( groupO ) - groupO . reduce ( 0 , ( cnt , wordCntPair ) - cnt + 1 ) . map (( cnt ) - new WordCountPair ( groupO . getKey (), cnt )))) . map ( WordCountPair :: toString ) , StageConfigs . scalarToScalarConfig ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ))) . metadata ( new Metadata . Builder () . name ( WordCount ) . description ( Reads Homer s The Illiad faster than we can. ) . build ()) . create (); } private List WordCountPair tokenize ( String text ) { StringTokenizer tokenizer = new StringTokenizer ( text ); List WordCountPair wordCountPairs = new ArrayList (); while ( tokenizer . hasMoreTokens ()) { String word = tokenizer . nextToken (). replaceAll ( \\\\s* , ). toLowerCase (); wordCountPairs . add ( new WordCountPair ( word , 1 )); } return wordCountPairs ; } public static void main ( String [] args ) { LocalJobExecutorNetworked . execute ( new WordCountJob (). getJobInstance ()); } } There are several things going on here, let's examine them one at a time...","title":"WordCountJob"},{"location":"develop/writing-jobs/word-count/#the-source","text":"We specify our source in the line .source(new IlliadSource()) . The source handles data ingestion and it is very common to use a pre-existing parameterized source when writing jobs. Mantis provides several sources which handle managing connections and queries to other jobs. In the next tutorial we'll learn how to implement our own source which ingests data from Twitter.","title":"The Source"},{"location":"develop/writing-jobs/word-count/#the-stage","text":"Our stage implements the bulk of the processing logic for the streaming job. Recall that a Mantis job has 1..n stages which can be used to create a topology for data processing. This stage is a ScalarComputation but we'll learn about other stage types in the third tutorial when we make word counting a distributed job. We'll take advantage of Java's lambda syntax to implement this stage inline. The call method receives a Context object and an Observable String provided by our source. The stage's responsibility is to produce an Observable R for consumption by down stream stages or the sink if this is the last stage. . stage (( context , dataO ) - dataO // Tokenize the string . flatMap (( text ) - Observable . from ( tokenize ( text ))) // Hopping / Tumbling window of 10 seconds . window ( 10 , TimeUnit . SECONDS ) // Reduce each window . flatMap (( wordCountPairObservable ) - wordCountPairObservable // count how many times a word appears . groupBy ( WordCountPair :: getWord ) . flatMap (( groupO ) - groupO . reduce ( 0 , ( cnt , wordCntPair ) - cnt + 1 ) . map (( cnt ) - new WordCountPair ( groupO . getKey (), cnt )))) // Convert the result to a string . map ( WordCountPair :: toString ) If you're familiar with reactive stream processing the above should be fairly easy to comprehend. Unfortunately if you aren't then an introduction to this is outside of the scope of this tutorial. Head over to reactivex.io to learn more about the concept. The stage configuration below specifies a few things; First that this stage is a scalar to scalar stage in that it ingests single events, and produces single events. The type of the input events is String, and the output is also String. Finally the configuration also specifies which Codec to use on the wire for this stage's output. You can use this configuration to specify concurrency for this stage as well, but we've not elected to do so here. public static ScalarToScalar . Config String , String scalarToScalarConfig () { return new ScalarToScalar . Config String , String () . codec ( Codecs . string ()); }","title":"The Stage"},{"location":"develop/writing-jobs/word-count/#the-job-provider","text":"The MantisJobProvider interface is what the Mantis runtime expects to load. The runtime reads resources/META-INF/services/io.mantisrx.runtime.MantisJobProvider to discover the fully qualified classname of the MantisJobProvider to be used as an entry point for the application.","title":"The Job Provider"},{"location":"develop/writing-jobs/word-count/#main-method","text":"The main method invokes the LocalJobExecutorNetworked execute method to run our job locally. The first three tutorials will take advantage of the ability to run jobs locally. In the fourth tutorial we will explore uploading and submitting our job on a Mantis cloud deployment for greater scalability. We can and should run this main method by invoking ./gradlew :mantis-examples-wordcount:execute at the root of the mantis-examples directory. public static void main ( String [] args ) { LocalJobExecutorNetworked . execute ( new WordCountJob (). getJobInstance ()); }","title":"Main Method"},{"location":"develop/writing-jobs/word-count/#conclusions-and-future-work","text":"We've implemented a complete end-to-end Mantis job which counts words from The Illiad repeatedly. This leaves much to be desired. If you inspect our source we're really just iterating over the same data set every ten seconds. In the next tutorial we'll explore the task of writing our own custom source to pull external data from Twitter into Mantis and designing this source in a templated fashion so that it can be used with different queries and API keys. As an extra credit task see if you can modify the stage in this job to print the top 10 words instead of the entire list.","title":"Conclusions and Future Work"},{"location":"getting-started/concepts/","text":"Mantis Concepts Mantis provides Stream-processing-As-a-Service. It is a self contained platform that manages all tasks associated with running thousands of stream processing jobs. Take a look at Infrastructure Overview to get an understanding of the physical components of the Mantis Platform. Let us walk through some of the key concepts and terminologies used in Mantis. Mantis Job Cluster A Mantis Job cluster represents metadata (artifact, configuration, resource requirements) associated with a job. A job can be thought of as a running instance of Job Cluster (like a Java Object is an instance of a Java class). A Job cluster can have 0 or more running instances of a job at any given time. Users can control how many jobs can be running at any given time by specifying the SLA for this cluster. E.g. SLA of Min 1 / Max 1 means there is exactly one instance of job running at any given time. Users can also setup a Cron spec that can be used to submit jobs periodically. Mantis Jobs At the core of Mantis is the Mantis Job. Stream processing applications in Mantis are called Mantis Jobs. Logically a job represents the business logic to transform a stream of events from one or more sources and generate results. A developer writes a job using Mantis primitives and builds an artifact which is then deployed into the Mantis platform for execution. Each Mantis Job belongs to exactly one Mantis Job Cluster. Lets take a closer look at a Mantis Job. Worker A worker is the smallest unit of execution in Mantis. Physically a worker executes in a resource isolated Mesos container on the Mantis Agent fleet of servers. Each worker is assigned a unique monotonically increasing MantisWorkerIndex and a unique MantisWorkerNumber If a Mantis worker terminates abnormally, Mantis ensures a replacement worker gets launched with the same MantisWorkerIndex and a new MantisWorkerNumber . The resources like CPU, Memory, Network allocated to a worker are configured at Job submit time and cannot be updated later. Each Worker belongs to exactly one Mantis Stage. Stages (Group of workers) A Mantis job is logically divided into one or more stages . A stage is a collection of homogeneous Mantis Workers that perform the same computation. A typical map-reduce style job would be represented by three stages in Mantis (shuffle, window/aggregate and collect) Workers belonging to a stage establish network connections with all workers of the previous stage (if one exists) Workers of the same stage do not connect to each other. The presence of multiple stages imply network hops which allows users to distribute the job logic across several workers allowing for more scalability. Mantis allows each stage to dynamically scale the number of workers in the stage independently with the help of an autoscaling policy. Info Autoscaling is only recommended on stateless stages. For stateful stages the user would need to implement logic to move state to new workers. In cases where the state can be rebuilt rapidly this is not a concern. Mantis Runtime The Mantis runtime is execution environment for Mantis Jobs. Broadly speaking it covers all aspects of the running a Mantis Job which includes Operators based Reactive Extensions used by Jobs to implement their business logic. Job topology management which builds and maintains the job execution DAG Exchanging control messages with the Mantis Master including heartbeats among other things. Source Job A source job is a type of Mantis Job that makes data available to other Mantis Jobs via an MQL interface. Downstream jobs connect to the Sink (Server Sent Event) of the Source job with an MQL query which denotes what data the job is interested in. Each event flowing through the Source job is evaluated against these MQL queries. Events matching a particular query are then streamed to the corresponding downstream job. The source jobs have several advantages Pluggable source : By abstracting out where the data is coming from, the source jobs allow the downstream jobs to focus on just their processing logic. The same job can then work with data from different sources by simply connecting to a different source jobs. E.g There can be one source job (say A) backed by a Kafka topic and another backed by S3 (say B) and the downstream job can either connect to A or B based on whether they want to process realtime data or historical data. Data re-use : Often a lot of jobs are interested in data from the same source. Just that they maybe interested in different subsets of this data. Instead of each job having to re-fetch the same data again and again from the same external source, The source jobs fetch the data once and then make it available for any other job to use. Cost Efficiency : A direct consequence of data re-use is fewer resources are required. E.g Let us assume there are 3 jobs interested in processing data from the same Kafka topic. But each is interested in different subsets of this data. Traditionally, each job would have to read the topic in its entirety and then filter out the data they are not interested in. This means 3x fanout on Kafka, and additionally each job now has to have enough resources to process the entire topic. If this topic is high volume then these jobs would have to be sufficiently scaled to keep up. All while throwing away large portions of the data. With a source job the Kafka fan out is just 1 and the three downstream jobs need to be scaled just enough to process their anticipated subset of the stream. Less operational load : Source jobs are stateless and thus good candidates for Autoscaling. With autoscaling enabled the Mantis administrators do not have to worry about right sizing the resources, the job will just adapt its size to meet the demands. Mantis OSS comes with the following source jobs Kafka Source job : Reads data from one or more Kafka topics and makes it available for downstream consumers to query via MQL. Publish Source Job : Works the the mantis-publish library to fetch data on-demand from external applications and make it available to downstream consumers. See the On-Demand Sample to see this in action. Users can also build their own source jobs see the Synthetic Source Job example. Job Chaining One of the unique capabilities of Mantis is the ability for Jobs to communicate with each other to form a kind of streaming microservices architecture. The Source Job - Downstream job flow is an example of this Job chaining. In this case the source of the downstream job is the output of the upstream Data source job. All a job needs to do to connect to the sink of another job is to include the in-built Job Connnector See the Job Connector Sample to see this in action. These job to job communications happen directly via in memory socket connections with no intermediate disk persistence. If buffering/persistence of results is desired then it is recommended to sink the data into persistence queue like Kafka using the Kafka Connector Job chaining has proven to be extremely useful while operating at scale. It is widely used in the Netflix deployment of Mantis. Mantis Query Language (MQL) MQL is a SQL like language that allows users to work with streaming data without having to write Java code. Example MQL query: select * from defaultStream where status == 500 MQL is used in various parts of Mantis platform including the mantis-publish library, Source Jobs and sometimes directly as a library within a Job that can benefit from the query and aggregation features it brings. Mantis Master The Mantis Master is a leader elected control plane for the Mantis platform. It is responsible for managing the life cycle of Job Clusters, Jobs and workers. It also acts as a Resource scheduler to optimally allocate and schedule resources required by the Jobs. The master stores its meta-data into an external source. The OSS version ships with a sample file based store. For production deployments a highly available store is recommended. The Master is built using Akka principles, where each Job Cluster, Job etc are modelled as Actors. For scheduling of resources Mantis relies on the Mesos Framework The Master registers itself as a Mesos Framework . It receives resource offers from Mesos and uses Fenzo to optimally match workers to these offers. Mantis API The Mantis API is almost like a traditional API server which proxies request to the Mantis Master. But has additional capabilities such as: Allowing users to stream the output of a job via web sockets ( /api/v1/jobConnectbyid/jobID API) Acts as a discovery server for Jobs, allowing consumers to get a stream of scheduling information (like host, port for workers belonging to a job)","title":"Concepts"},{"location":"getting-started/concepts/#mantis-concepts","text":"Mantis provides Stream-processing-As-a-Service. It is a self contained platform that manages all tasks associated with running thousands of stream processing jobs. Take a look at Infrastructure Overview to get an understanding of the physical components of the Mantis Platform. Let us walk through some of the key concepts and terminologies used in Mantis.","title":"Mantis Concepts"},{"location":"getting-started/concepts/#mantis-job-cluster","text":"A Mantis Job cluster represents metadata (artifact, configuration, resource requirements) associated with a job. A job can be thought of as a running instance of Job Cluster (like a Java Object is an instance of a Java class). A Job cluster can have 0 or more running instances of a job at any given time. Users can control how many jobs can be running at any given time by specifying the SLA for this cluster. E.g. SLA of Min 1 / Max 1 means there is exactly one instance of job running at any given time. Users can also setup a Cron spec that can be used to submit jobs periodically.","title":"Mantis Job Cluster"},{"location":"getting-started/concepts/#mantis-jobs","text":"At the core of Mantis is the Mantis Job. Stream processing applications in Mantis are called Mantis Jobs. Logically a job represents the business logic to transform a stream of events from one or more sources and generate results. A developer writes a job using Mantis primitives and builds an artifact which is then deployed into the Mantis platform for execution. Each Mantis Job belongs to exactly one Mantis Job Cluster. Lets take a closer look at a Mantis Job.","title":"Mantis Jobs"},{"location":"getting-started/concepts/#worker","text":"A worker is the smallest unit of execution in Mantis. Physically a worker executes in a resource isolated Mesos container on the Mantis Agent fleet of servers. Each worker is assigned a unique monotonically increasing MantisWorkerIndex and a unique MantisWorkerNumber If a Mantis worker terminates abnormally, Mantis ensures a replacement worker gets launched with the same MantisWorkerIndex and a new MantisWorkerNumber . The resources like CPU, Memory, Network allocated to a worker are configured at Job submit time and cannot be updated later. Each Worker belongs to exactly one Mantis Stage.","title":"Worker"},{"location":"getting-started/concepts/#stages-group-of-workers","text":"A Mantis job is logically divided into one or more stages . A stage is a collection of homogeneous Mantis Workers that perform the same computation. A typical map-reduce style job would be represented by three stages in Mantis (shuffle, window/aggregate and collect) Workers belonging to a stage establish network connections with all workers of the previous stage (if one exists) Workers of the same stage do not connect to each other. The presence of multiple stages imply network hops which allows users to distribute the job logic across several workers allowing for more scalability. Mantis allows each stage to dynamically scale the number of workers in the stage independently with the help of an autoscaling policy. Info Autoscaling is only recommended on stateless stages. For stateful stages the user would need to implement logic to move state to new workers. In cases where the state can be rebuilt rapidly this is not a concern.","title":"Stages (Group of workers)"},{"location":"getting-started/concepts/#mantis-runtime","text":"The Mantis runtime is execution environment for Mantis Jobs. Broadly speaking it covers all aspects of the running a Mantis Job which includes Operators based Reactive Extensions used by Jobs to implement their business logic. Job topology management which builds and maintains the job execution DAG Exchanging control messages with the Mantis Master including heartbeats among other things.","title":"Mantis Runtime"},{"location":"getting-started/concepts/#source-job","text":"A source job is a type of Mantis Job that makes data available to other Mantis Jobs via an MQL interface. Downstream jobs connect to the Sink (Server Sent Event) of the Source job with an MQL query which denotes what data the job is interested in. Each event flowing through the Source job is evaluated against these MQL queries. Events matching a particular query are then streamed to the corresponding downstream job. The source jobs have several advantages Pluggable source : By abstracting out where the data is coming from, the source jobs allow the downstream jobs to focus on just their processing logic. The same job can then work with data from different sources by simply connecting to a different source jobs. E.g There can be one source job (say A) backed by a Kafka topic and another backed by S3 (say B) and the downstream job can either connect to A or B based on whether they want to process realtime data or historical data. Data re-use : Often a lot of jobs are interested in data from the same source. Just that they maybe interested in different subsets of this data. Instead of each job having to re-fetch the same data again and again from the same external source, The source jobs fetch the data once and then make it available for any other job to use. Cost Efficiency : A direct consequence of data re-use is fewer resources are required. E.g Let us assume there are 3 jobs interested in processing data from the same Kafka topic. But each is interested in different subsets of this data. Traditionally, each job would have to read the topic in its entirety and then filter out the data they are not interested in. This means 3x fanout on Kafka, and additionally each job now has to have enough resources to process the entire topic. If this topic is high volume then these jobs would have to be sufficiently scaled to keep up. All while throwing away large portions of the data. With a source job the Kafka fan out is just 1 and the three downstream jobs need to be scaled just enough to process their anticipated subset of the stream. Less operational load : Source jobs are stateless and thus good candidates for Autoscaling. With autoscaling enabled the Mantis administrators do not have to worry about right sizing the resources, the job will just adapt its size to meet the demands. Mantis OSS comes with the following source jobs Kafka Source job : Reads data from one or more Kafka topics and makes it available for downstream consumers to query via MQL. Publish Source Job : Works the the mantis-publish library to fetch data on-demand from external applications and make it available to downstream consumers. See the On-Demand Sample to see this in action. Users can also build their own source jobs see the Synthetic Source Job example.","title":"Source Job"},{"location":"getting-started/concepts/#job-chaining","text":"One of the unique capabilities of Mantis is the ability for Jobs to communicate with each other to form a kind of streaming microservices architecture. The Source Job - Downstream job flow is an example of this Job chaining. In this case the source of the downstream job is the output of the upstream Data source job. All a job needs to do to connect to the sink of another job is to include the in-built Job Connnector See the Job Connector Sample to see this in action. These job to job communications happen directly via in memory socket connections with no intermediate disk persistence. If buffering/persistence of results is desired then it is recommended to sink the data into persistence queue like Kafka using the Kafka Connector Job chaining has proven to be extremely useful while operating at scale. It is widely used in the Netflix deployment of Mantis.","title":"Job Chaining"},{"location":"getting-started/concepts/#mantis-query-language-mql","text":"MQL is a SQL like language that allows users to work with streaming data without having to write Java code. Example MQL query: select * from defaultStream where status == 500 MQL is used in various parts of Mantis platform including the mantis-publish library, Source Jobs and sometimes directly as a library within a Job that can benefit from the query and aggregation features it brings.","title":"Mantis Query Language (MQL)"},{"location":"getting-started/concepts/#mantis-master","text":"The Mantis Master is a leader elected control plane for the Mantis platform. It is responsible for managing the life cycle of Job Clusters, Jobs and workers. It also acts as a Resource scheduler to optimally allocate and schedule resources required by the Jobs. The master stores its meta-data into an external source. The OSS version ships with a sample file based store. For production deployments a highly available store is recommended. The Master is built using Akka principles, where each Job Cluster, Job etc are modelled as Actors. For scheduling of resources Mantis relies on the Mesos Framework The Master registers itself as a Mesos Framework . It receives resource offers from Mesos and uses Fenzo to optimally match workers to these offers.","title":"Mantis Master"},{"location":"getting-started/concepts/#mantis-api","text":"The Mantis API is almost like a traditional API server which proxies request to the Mantis Master. But has additional capabilities such as: Allowing users to stream the output of a job via web sockets ( /api/v1/jobConnectbyid/jobID API) Acts as a discovery server for Jobs, allowing consumers to get a stream of scheduling information (like host, port for workers belonging to a job)","title":"Mantis API"},{"location":"getting-started/use-cases/","text":"Mantis Use Cases Here is a non-comprehensive list of use-cases powered by Mantis to give you an idea of the type of applications that can be built on Mantis. Realtime monitoring of Netflix streaming health At Netflix Stream Starts per Second (SPS) is a key metric used to track the health of the Netflix streaming service. Streaming starts per second tracks the number of people successfully hitting play on their streaming devices. Any abnormal change in the trend of this metric signifies a negative impact on user's viewing experience. This Mantis application monitors the SPS trend by processing data sourced directly from thousands of Netflix servers (using the mantis-publish library) in realtime. Using a version of Double Exponential Smoothing (DES) it can detect abnormal deviations in seconds and alerts key teams at Netflix. Contextual Alerting As Netflix has grown over the years so has the number of microservices. Getting alerted for outages for your own service is often not sufficient to root-cause issues. Engineers need to understand what is happening with downstream and upstream services as well to be able to quickly narrow down the root of the issue. The Contextual alerting application analyzes millions of interactions between dozens of Netflix microservices in realtime to identify anomalies and provide operators with rich and relevant context. The realtime nature of these Mantis-backed aggregations allows the Mean-Time-To-Detect to be cut down from tens of minutes to a few seconds. Given the scale of Netflix this makes a huge impact. Raven In large distributed systems, often there are cases where a user reports a problem but the overall health of the system is green. In such cases there is a need to explore events associated with the user/device/service in realtime to find a smoking gun. With the potential of a user request landing across thousands of servers it is often a laborious task to find the right servers and inspect their logs. The Raven applications makes this task trivial, The raven jobs work with the mantis-publish library to look for events matching a certain criterion (user-id/device-id etc) right at the server and stream matching results in realtime. It provides an intuitive UI that allows SREs to construct and submit simple MQL queries. Cassandra and Elastic Search Health Monitoring Netflix maintains hundreds of Cassandra and Elastic Search clusters. These clusters are critical for the day to day operation of Netflix. The Cassandra and Elastic Search health application analyzes rich operational events sent by the Priam side car in realtime to generate a holistic picture of the health of every Cassandra cluster at Netflix. Since this system has gone into operation the number of false pages has dropped down significantly. Alerting on Log The Alerting on Logs application allows users to create alerts which page when a certain pattern is detected within the application logs. This application analyzes logs from thousands of servers in realtime. Chaos Experimentation monitoring Chaos Testing is one of the pillars of resilience at Netflix. Dozens of chaos experiments are run daily to test the resilience of variety of applications. The Chaos experimentation application tracks user experience by analyzing client and server side events during a Chaos exercise in realtime and triggers an abort of the chaos exercise in case of an adverse impact. Realtime Personally Identifiable Information (PII) data detection With trillions of events flowing through Netflix data systems daily it is critical to ensure no sensitive data is accidentally passed along. This application samples data across all streaming sources and applies custom pattern detection algorithms to identify presence of such data.","title":"Use Cases"},{"location":"getting-started/use-cases/#mantis-use-cases","text":"Here is a non-comprehensive list of use-cases powered by Mantis to give you an idea of the type of applications that can be built on Mantis.","title":"Mantis Use Cases"},{"location":"getting-started/use-cases/#realtime-monitoring-of-netflix-streaming-health","text":"At Netflix Stream Starts per Second (SPS) is a key metric used to track the health of the Netflix streaming service. Streaming starts per second tracks the number of people successfully hitting play on their streaming devices. Any abnormal change in the trend of this metric signifies a negative impact on user's viewing experience. This Mantis application monitors the SPS trend by processing data sourced directly from thousands of Netflix servers (using the mantis-publish library) in realtime. Using a version of Double Exponential Smoothing (DES) it can detect abnormal deviations in seconds and alerts key teams at Netflix.","title":"Realtime monitoring of Netflix streaming health"},{"location":"getting-started/use-cases/#contextual-alerting","text":"As Netflix has grown over the years so has the number of microservices. Getting alerted for outages for your own service is often not sufficient to root-cause issues. Engineers need to understand what is happening with downstream and upstream services as well to be able to quickly narrow down the root of the issue. The Contextual alerting application analyzes millions of interactions between dozens of Netflix microservices in realtime to identify anomalies and provide operators with rich and relevant context. The realtime nature of these Mantis-backed aggregations allows the Mean-Time-To-Detect to be cut down from tens of minutes to a few seconds. Given the scale of Netflix this makes a huge impact.","title":"Contextual Alerting"},{"location":"getting-started/use-cases/#raven","text":"In large distributed systems, often there are cases where a user reports a problem but the overall health of the system is green. In such cases there is a need to explore events associated with the user/device/service in realtime to find a smoking gun. With the potential of a user request landing across thousands of servers it is often a laborious task to find the right servers and inspect their logs. The Raven applications makes this task trivial, The raven jobs work with the mantis-publish library to look for events matching a certain criterion (user-id/device-id etc) right at the server and stream matching results in realtime. It provides an intuitive UI that allows SREs to construct and submit simple MQL queries.","title":"Raven"},{"location":"getting-started/use-cases/#cassandra-and-elastic-search-health-monitoring","text":"Netflix maintains hundreds of Cassandra and Elastic Search clusters. These clusters are critical for the day to day operation of Netflix. The Cassandra and Elastic Search health application analyzes rich operational events sent by the Priam side car in realtime to generate a holistic picture of the health of every Cassandra cluster at Netflix. Since this system has gone into operation the number of false pages has dropped down significantly.","title":"Cassandra and Elastic Search Health Monitoring"},{"location":"getting-started/use-cases/#alerting-on-log","text":"The Alerting on Logs application allows users to create alerts which page when a certain pattern is detected within the application logs. This application analyzes logs from thousands of servers in realtime.","title":"Alerting on Log"},{"location":"getting-started/use-cases/#chaos-experimentation-monitoring","text":"Chaos Testing is one of the pillars of resilience at Netflix. Dozens of chaos experiments are run daily to test the resilience of variety of applications. The Chaos experimentation application tracks user experience by analyzing client and server side events during a Chaos exercise in realtime and triggers an abort of the chaos exercise in case of an adverse impact.","title":"Chaos Experimentation monitoring"},{"location":"getting-started/use-cases/#realtime-personally-identifiable-information-pii-data-detection","text":"With trillions of events flowing through Netflix data systems daily it is critical to ensure no sensitive data is accidentally passed along. This application samples data across all streaming sources and applies custom pattern detection algorithms to identify presence of such data.","title":"Realtime Personally Identifiable Information (PII) data detection"},{"location":"getting-started/samples/on-demand/","text":"One of the key features of Mantis is the ability to stream filtered events on-demand from external applications. In this example we walk through publishing data to Mantis from a simple Java application using the mantis-publish library. Followed by setting up a Data Source Job that will act as a broker and a simple Mantis Job that when launched will trigger on-demand streaming of data matching a certain criterion. This end-to-end example highlights two powerful Mantis concepts On demand streaming of filtered data from external applications directly into Mantis Job Chaining where one Job connects to the output of another job. Prerequisites SharedMrePublishEventSource Job Cluster exists. JobConnectorSample Job cluster exists Java Sample is setup and running. Note If you are following the Mantis Cluster using Docker instructions all of this will be already set up. Publishing events into Mantis Note : The local docker setup has already preconfigured a simple Java Sample application to publish events to Mantis. Setting up a Publish Data Source Job A Publish Source Job is a special kind of a Mantis Job that interacts with the mantis-publish library on behalf of a downstream job to push subscriptions up to the mantis-publish library and receive events matching the subscription from the mantis-publish library. Submit the SharedMrePublishEventSource Part of the docker setup we have preconfigured the SharedMrePublishEventSource cluster. So all we have to do is submit an instance of it. Go to the clusters page and click on SharedMrePublishEventSource Click submit on the top right corner of the screen This will open up the Job Detail page. Now wait for the Job to go into Launched state You are all set. Now the Java application referenced in the previous section should be able to communicate with this source job to exchange subscriptions and data. At this point however there are no active subscriptions so no data is actually being sent out from our Java application. If you look at the shell window where the docker is running you should see output like mantispublish_1 | 2019 -10-16 17 :55:46 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = SKIPPED_NO_SUBSCRIPTIONS ( PRECONDITION_FAILED ) Next step. Launch a new job to query for Data generated by the Java application. Query data generated by the Java application. Our Java application generates a stream of Events representing requests made to it by an external client. Say we want to look at all events that are failing i.e have status=500 Submit a query job Now we launch a simple Mantis Job to query data generated by our Java Application. Go to clusters page and click on JobConnectorSample Click the green submit button on the top right corner of the screen. On the Job submit page scroll down and click on Override Defaults to configure our query. Enter the following as the value for parameter target json {\"targets\":[{\"sourceJobName\":\"SharedMrePublishEventSource\",\"criterion\":\"select * from defaultStream where status==500\"}]} and hit submit. Two key things to note: We set the sourceJobName to SharedMrePublishEventSource which is the source job we configured in the previous step. We set the criterion key in the payload to our MQL query select * from defaultStream where status==500 Click on Submit . On the Job Detail page scroll down to the Job Output section and click on Start. In a few seconds you should see events matching our query flow through. If you go back to the shell that is running the docker images you should now see output like mantispublish_1 | 2019 -10-16 17 :58:32 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = ENQUEUED ( SENDING ) If we now terminate our JobConnector Job, then our Java application will again revert to not sending any data. Take aways By integrating the mantis-publish library with their applications, users can get access to rich data generated by their applications in realtime and in a cost-effective manner for analysis into their jobs.","title":"On-Demand"},{"location":"getting-started/samples/on-demand/#prerequisites","text":"SharedMrePublishEventSource Job Cluster exists. JobConnectorSample Job cluster exists Java Sample is setup and running. Note If you are following the Mantis Cluster using Docker instructions all of this will be already set up.","title":"Prerequisites"},{"location":"getting-started/samples/on-demand/#publishing-events-into-mantis","text":"Note : The local docker setup has already preconfigured a simple Java Sample application to publish events to Mantis.","title":"Publishing events into Mantis"},{"location":"getting-started/samples/on-demand/#setting-up-a-publish-data-source-job","text":"A Publish Source Job is a special kind of a Mantis Job that interacts with the mantis-publish library on behalf of a downstream job to push subscriptions up to the mantis-publish library and receive events matching the subscription from the mantis-publish library.","title":"Setting up a Publish Data Source Job"},{"location":"getting-started/samples/on-demand/#submit-the-sharedmrepublisheventsource","text":"Part of the docker setup we have preconfigured the SharedMrePublishEventSource cluster. So all we have to do is submit an instance of it. Go to the clusters page and click on SharedMrePublishEventSource Click submit on the top right corner of the screen This will open up the Job Detail page. Now wait for the Job to go into Launched state You are all set. Now the Java application referenced in the previous section should be able to communicate with this source job to exchange subscriptions and data. At this point however there are no active subscriptions so no data is actually being sent out from our Java application. If you look at the shell window where the docker is running you should see output like mantispublish_1 | 2019 -10-16 17 :55:46 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = SKIPPED_NO_SUBSCRIPTIONS ( PRECONDITION_FAILED ) Next step. Launch a new job to query for Data generated by the Java application.","title":"Submit the SharedMrePublishEventSource"},{"location":"getting-started/samples/on-demand/#query-data-generated-by-the-java-application","text":"Our Java application generates a stream of Events representing requests made to it by an external client. Say we want to look at all events that are failing i.e have status=500","title":"Query data generated by the Java application."},{"location":"getting-started/samples/on-demand/#submit-a-query-job","text":"Now we launch a simple Mantis Job to query data generated by our Java Application. Go to clusters page and click on JobConnectorSample Click the green submit button on the top right corner of the screen. On the Job submit page scroll down and click on Override Defaults to configure our query. Enter the following as the value for parameter target json {\"targets\":[{\"sourceJobName\":\"SharedMrePublishEventSource\",\"criterion\":\"select * from defaultStream where status==500\"}]} and hit submit. Two key things to note: We set the sourceJobName to SharedMrePublishEventSource which is the source job we configured in the previous step. We set the criterion key in the payload to our MQL query select * from defaultStream where status==500 Click on Submit . On the Job Detail page scroll down to the Job Output section and click on Start. In a few seconds you should see events matching our query flow through. If you go back to the shell that is running the docker images you should now see output like mantispublish_1 | 2019 -10-16 17 :58:32 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = ENQUEUED ( SENDING ) If we now terminate our JobConnector Job, then our Java application will again revert to not sending any data.","title":"Submit a query job"},{"location":"getting-started/samples/on-demand/#take-aways","text":"By integrating the mantis-publish library with their applications, users can get access to rich data generated by their applications in realtime and in a cost-effective manner for analysis into their jobs.","title":"Take aways"},{"location":"getting-started/samples/sine-function/","text":"The Sine function sample is a very simple job that generates a set of x and y coordinates of a sine wave. Prerequisites A SineFunction Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this will be already set up. Running the sample Go to the clusters page page and Click on SineFunction On the Job Cluster detail page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us skip all that and scroll directly to the bottom and hit the Submit button on the bottom left. View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - { x : 26.000000, y : 7.625585} Terminate the job To stop the job click on the red Kill Job button on the top right corner. Next Steps Explore the code Checkout out the other samples","title":"Sine Function"},{"location":"getting-started/samples/sine-function/#prerequisites","text":"A SineFunction Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this will be already set up.","title":"Prerequisites"},{"location":"getting-started/samples/sine-function/#running-the-sample","text":"Go to the clusters page page and Click on SineFunction On the Job Cluster detail page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us skip all that and scroll directly to the bottom and hit the Submit button on the bottom left. View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - { x : 26.000000, y : 7.625585}","title":"Running the sample"},{"location":"getting-started/samples/sine-function/#terminate-the-job","text":"To stop the job click on the red Kill Job button on the top right corner.","title":"Terminate the job"},{"location":"getting-started/samples/sine-function/#next-steps","text":"Explore the code Checkout out the other samples","title":"Next Steps"},{"location":"getting-started/samples/twitter/","text":"The Twitter sample demonstrates sourcing data from an external source (twitter in this case) and calculates the number of occurrences of a word within a hopping window. Prerequisites A TwitterSample Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this will be already set up. Twitter credentials to be used to connect to Twitter. If you don't already have a twitter application You can create one here here The Keys and Tokens section should list the credentials needed for this application. Running the sample Let us try submitting this job. Click on TwitterSample from the clusters page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us scroll down to the parameters section. Here we fill in the required parameters for this job. These include Twitter consumer key Twitter consumer secret Twitter token Twitter token secret View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and click on Start You should see output of the Twitter job being streamed below Terminate the job To stop the job click on the red Kill Job button on the top right corner. Explore the code Checkout out the other samples","title":"Twitter"},{"location":"getting-started/samples/twitter/#prerequisites","text":"A TwitterSample Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this will be already set up. Twitter credentials to be used to connect to Twitter. If you don't already have a twitter application You can create one here here The Keys and Tokens section should list the credentials needed for this application.","title":"Prerequisites"},{"location":"getting-started/samples/twitter/#running-the-sample","text":"Let us try submitting this job. Click on TwitterSample from the clusters page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us scroll down to the parameters section. Here we fill in the required parameters for this job. These include Twitter consumer key Twitter consumer secret Twitter token Twitter token secret","title":"Running the sample"},{"location":"getting-started/samples/twitter/#view-output-of-the-job","text":"If all goes well your job would go into Launched state. Scroll to the bottom and click on Start You should see output of the Twitter job being streamed below","title":"View output of the job"},{"location":"getting-started/samples/twitter/#terminate-the-job","text":"To stop the job click on the red Kill Job button on the top right corner. Explore the code Checkout out the other samples","title":"Terminate the job"},{"location":"getting-started/tutorials/cloud/","text":"Spinning up your first Mantis cluster in the cloud using the Mantis CLI Prerequisites The Mantis CLI currently supports AWS by spinning up a minimal cluster using T2 micro instances. This is meant for basic testing and not meant to run production traffic. In order for you to spin up a cluster in AWS, you will need to create or use an existing AWS account. You can follow AWS's account creation instructions for more information. Once your account is created, you will need to create and download your AWS Access Keys . You can follow AWS's instructions on creating access keys for more information. Now that you have your AWS account and access keys on hand, you're ready to bootstrap your first Mantis cluster using the Mantis CLI. Bootstrapping your first Mantis cluster in AWS Download and install the Mantis CLI First, you'll need to download the Mantis CLI app. If you're on Mac OS, it's recommended that you download and install using the Mac package: mantis-v0.1.1.pkg If you're on other systems such as Linux or Windows, choose the appropriate package from the Release Assets: v0.1.1 Release Assets Configure AWS credentials Now that you have the Mantis CLI installed, you'll need to tell it about your AWS credentials: $ mantis aws:configure AWS access key id: input access key id AWS secret access key: input secret access key Configuring AWS credentials... done This command stores your AWS credentials in the same exact format and location as the AWS SDK. More on this at the Mantis CLI page . Bootstrap your cluster With AWS credentials configured, you can bootstrap your cluster in a single command: $ mantis aws:bootstrap ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2193 Create key pair [ skipped ] \u2192 Key-pair file already exists ( mantis-us-east-2 ) \u2193 Create default VPC [ skipped ] \u2192 Default VPC already exists \u2193 Create zookeeper security group [ skipped ] \u2192 Security group already exists ( zookeeper ) \u2193 Authorize zookeeper security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2193 Authorize zookeeper security group zookeeper port ingress [ skipped ] \u2192 Ingress rule already exists \u2714 Bootstrap Zookeeper node \u2193 Create mesos-master security group [ skipped ] \u2192 Security group already exists ( mesos-master ) \u2193 Authorize mesos-master security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2839 Authorize mesos-master security group mesos-master port 5050 ingress Bootstrap Mesos Master node Create mesos-slave security group Authorize mesos-slave security group ssh port ingress Authorize mesos-slave security group mantis-agent port 7104 ingress Authorize mesos-slave security group mantis-agent port 7150 -7400 ingress Authorize mesos-slave security group mesos-slave resource port ingress Bootstrap Mesos Slave node Create mantis-control-plane security group Authorize mantis-control-plane security group ssh port ingress Authorize mantis-control-plane security group remote debug port 5050 ingress Authorize mantis-control-plane security group api port 8100 ingress Authorize mantis-control-plane security group api v2 port 8075 ingress Authorize mantis-control-plane security group scheduling info port 8076 ingress Authorize mantis-control-plane security group metrics port 8082 ingress Authorize mantis-control-plane security group console port 9090 ingress Bootstrap Mantis Control Plane service Create mantis-api security group Authorize mantis-api security group ssh port ingress Authorize mantis-api security group web port 80 ingress Authorize mantis-api security group ssl port 443 ingress Authorize mantis-api security group api port 7101 ingress Authorize mantis-api security group websocket port 7102 ingress Authorize mantis-api security group tunnel port 7001 ingress \u2714 Bootstrap Mantis API service Mantis API will be provisioned in a few minutes with public DNS available at ec2 address :7101 Input this URL into your local Mantis UI to connect to the Mantis cluster. Mesos Master will be provisioned in a few minutes with public DNS available at ec2 address :5050 Input this URL into your local Mantis UI so it can connect to Mesos logs. This will launch and configure 5 AWS EC2 instances. Notice at the end of the bootstrap are 2 EC2 address. You will need these to input into the Mantis UI. Using the Mantis UI On your browser, navigate to the Mantis UI at: https://netflix.github.io/mantis-ui And fill out the Registration form as follows: Name: Example Email: example@example.com Master Name: Example Mantis API URL: your ec2 Mantis API URL outputted from the Mantis CLI Mesos URL: your ec2 Mesos URL outputted from the Mantis CLI Launching a Mantis Job When you go into the UI, you'll notice that the Mantis CLI has automatically preloaded a Job Cluster for you to try out. Simply click on the SineTest Job Cluster to go into the cluster details page. Once in the cluster details page, click on the green Submit latest version button on the top right to bring you to the Job Submit page. On the Job Submit page, everything has already been configured for you. All you have to do is hit the green Submit to Mantis button at the bottom of the page to launch your first Mantis Job. Now you can view the output of this job. If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - { x : 26.000000, y : 7.625585} Tearing down your cluster Once you're done, you can clean up all of your AWS resources by tearing down instances and deleting security groups. $ aws:teardown ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2714 Terminate instances Debugging your cluster You can debug your cluster by looking at the logs. To look at the logs, you'll need to go into your AWS EC2 Console and find instances with the Application: Mantis tag. From there, you can look at the instances with the following security groups: zookeeper mesos-slave mesos-master mantis-control-plane mantis-api You can connect to your EC2 instances by following instructions from the Connect button at the top. Info The Mantis CLI puts your EC2 .pem keys in the same folder as your AWS credentials, typically located in $HOME/.aws . Tip Application logs, e.g. Mantis-related or Zookeeper, for all instances will be located in /logs . Tip Mesos-related logs for mesos-master and mesos-slave will be located in /var/run/mesos .","title":"Explore in the Cloud"},{"location":"getting-started/tutorials/cloud/#spinning-up-your-first-mantis-cluster-in-the-cloud-using-the-mantis-cli","text":"","title":"Spinning up your first Mantis cluster in the cloud using the Mantis CLI"},{"location":"getting-started/tutorials/cloud/#prerequisites","text":"The Mantis CLI currently supports AWS by spinning up a minimal cluster using T2 micro instances. This is meant for basic testing and not meant to run production traffic. In order for you to spin up a cluster in AWS, you will need to create or use an existing AWS account. You can follow AWS's account creation instructions for more information. Once your account is created, you will need to create and download your AWS Access Keys . You can follow AWS's instructions on creating access keys for more information. Now that you have your AWS account and access keys on hand, you're ready to bootstrap your first Mantis cluster using the Mantis CLI.","title":"Prerequisites"},{"location":"getting-started/tutorials/cloud/#bootstrapping-your-first-mantis-cluster-in-aws","text":"","title":"Bootstrapping your first Mantis cluster in AWS"},{"location":"getting-started/tutorials/cloud/#download-and-install-the-mantis-cli","text":"First, you'll need to download the Mantis CLI app. If you're on Mac OS, it's recommended that you download and install using the Mac package: mantis-v0.1.1.pkg If you're on other systems such as Linux or Windows, choose the appropriate package from the Release Assets: v0.1.1 Release Assets","title":"Download and install the Mantis CLI"},{"location":"getting-started/tutorials/cloud/#configure-aws-credentials","text":"Now that you have the Mantis CLI installed, you'll need to tell it about your AWS credentials: $ mantis aws:configure AWS access key id: input access key id AWS secret access key: input secret access key Configuring AWS credentials... done This command stores your AWS credentials in the same exact format and location as the AWS SDK. More on this at the Mantis CLI page .","title":"Configure AWS credentials"},{"location":"getting-started/tutorials/cloud/#bootstrap-your-cluster","text":"With AWS credentials configured, you can bootstrap your cluster in a single command: $ mantis aws:bootstrap ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2193 Create key pair [ skipped ] \u2192 Key-pair file already exists ( mantis-us-east-2 ) \u2193 Create default VPC [ skipped ] \u2192 Default VPC already exists \u2193 Create zookeeper security group [ skipped ] \u2192 Security group already exists ( zookeeper ) \u2193 Authorize zookeeper security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2193 Authorize zookeeper security group zookeeper port ingress [ skipped ] \u2192 Ingress rule already exists \u2714 Bootstrap Zookeeper node \u2193 Create mesos-master security group [ skipped ] \u2192 Security group already exists ( mesos-master ) \u2193 Authorize mesos-master security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2839 Authorize mesos-master security group mesos-master port 5050 ingress Bootstrap Mesos Master node Create mesos-slave security group Authorize mesos-slave security group ssh port ingress Authorize mesos-slave security group mantis-agent port 7104 ingress Authorize mesos-slave security group mantis-agent port 7150 -7400 ingress Authorize mesos-slave security group mesos-slave resource port ingress Bootstrap Mesos Slave node Create mantis-control-plane security group Authorize mantis-control-plane security group ssh port ingress Authorize mantis-control-plane security group remote debug port 5050 ingress Authorize mantis-control-plane security group api port 8100 ingress Authorize mantis-control-plane security group api v2 port 8075 ingress Authorize mantis-control-plane security group scheduling info port 8076 ingress Authorize mantis-control-plane security group metrics port 8082 ingress Authorize mantis-control-plane security group console port 9090 ingress Bootstrap Mantis Control Plane service Create mantis-api security group Authorize mantis-api security group ssh port ingress Authorize mantis-api security group web port 80 ingress Authorize mantis-api security group ssl port 443 ingress Authorize mantis-api security group api port 7101 ingress Authorize mantis-api security group websocket port 7102 ingress Authorize mantis-api security group tunnel port 7001 ingress \u2714 Bootstrap Mantis API service Mantis API will be provisioned in a few minutes with public DNS available at ec2 address :7101 Input this URL into your local Mantis UI to connect to the Mantis cluster. Mesos Master will be provisioned in a few minutes with public DNS available at ec2 address :5050 Input this URL into your local Mantis UI so it can connect to Mesos logs. This will launch and configure 5 AWS EC2 instances. Notice at the end of the bootstrap are 2 EC2 address. You will need these to input into the Mantis UI.","title":"Bootstrap your cluster"},{"location":"getting-started/tutorials/cloud/#using-the-mantis-ui","text":"On your browser, navigate to the Mantis UI at: https://netflix.github.io/mantis-ui And fill out the Registration form as follows: Name: Example Email: example@example.com Master Name: Example Mantis API URL: your ec2 Mantis API URL outputted from the Mantis CLI Mesos URL: your ec2 Mesos URL outputted from the Mantis CLI","title":"Using the Mantis UI"},{"location":"getting-started/tutorials/cloud/#launching-a-mantis-job","text":"When you go into the UI, you'll notice that the Mantis CLI has automatically preloaded a Job Cluster for you to try out. Simply click on the SineTest Job Cluster to go into the cluster details page. Once in the cluster details page, click on the green Submit latest version button on the top right to bring you to the Job Submit page. On the Job Submit page, everything has already been configured for you. All you have to do is hit the green Submit to Mantis button at the bottom of the page to launch your first Mantis Job. Now you can view the output of this job. If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - { x : 26.000000, y : 7.625585}","title":"Launching a Mantis Job"},{"location":"getting-started/tutorials/cloud/#tearing-down-your-cluster","text":"Once you're done, you can clean up all of your AWS resources by tearing down instances and deleting security groups. $ aws:teardown ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2714 Terminate instances","title":"Tearing down your cluster"},{"location":"getting-started/tutorials/cloud/#debugging-your-cluster","text":"You can debug your cluster by looking at the logs. To look at the logs, you'll need to go into your AWS EC2 Console and find instances with the Application: Mantis tag. From there, you can look at the instances with the following security groups: zookeeper mesos-slave mesos-master mantis-control-plane mantis-api You can connect to your EC2 instances by following instructions from the Connect button at the top. Info The Mantis CLI puts your EC2 .pem keys in the same folder as your AWS credentials, typically located in $HOME/.aws . Tip Application logs, e.g. Mantis-related or Zookeeper, for all instances will be located in /logs . Tip Mesos-related logs for mesos-master and mesos-slave will be located in /var/run/mesos .","title":"Debugging your cluster"},{"location":"getting-started/tutorials/docker/","text":"Spinning up your first Mantis cluster using Docker Prerequisites Install Docker on your local machine (if you don't already have it) Mac Windows Linux Bootstraping your first Mantis Cluster in Docker Download the docker-compose file Download the docker-compose.yml to a local folder mantis $ cd mantis $ docker-compose -f docker-compose.yml up This starts up the following Docker containers: Zookeeper Mesos Master Mantis Master Mantis API Mesos Slave and Mantis Worker run on a single container (mantisagent) A simple hello world web application that sends events to Mantis A simple Java application that sends events to Manits Mantis Admin UI The Mantis Admin UI allows you to manage your Mantis Jobs. Open the Mantis UI in a new browser window. Fill out the Registration form as follows Name: Example Email: example@example.com Master Name: Example Mantis API URL: http://localhost:7101 Mesos URL: http://localhost:5050 Click on Create The Mantis Admin page should be pre-populated with all the Mantis examples. Try out Mantis Jobs Now that you have setup a Mantis cluster locally try running some of the preconfigured Mantis samples Sine Function Sample - A simple job that generates x and y coordinates of a sine wave. Twitter Sample - Connects to a twitter stream using consumer and token keys specified and performs a streaming word count. On Demand Sample - Demonstrates how Mantis Jobs can pull events on demand from external applications. Next steps Setup Mantis in AWS and run the samples Write your first Mantis Job To teardown the Mantis cluster, issue the following command $ cd mantis $ docker-compose -f docker-compose.yml down","title":"Explore Using Docker"},{"location":"getting-started/tutorials/docker/#spinning-up-your-first-mantis-cluster-using-docker","text":"","title":"Spinning up your first Mantis cluster using Docker"},{"location":"getting-started/tutorials/docker/#prerequisites","text":"Install Docker on your local machine (if you don't already have it) Mac Windows Linux","title":"Prerequisites"},{"location":"getting-started/tutorials/docker/#bootstraping-your-first-mantis-cluster-in-docker","text":"","title":"Bootstraping your first Mantis Cluster in Docker"},{"location":"getting-started/tutorials/docker/#download-the-docker-compose-file","text":"Download the docker-compose.yml to a local folder mantis $ cd mantis $ docker-compose -f docker-compose.yml up This starts up the following Docker containers: Zookeeper Mesos Master Mantis Master Mantis API Mesos Slave and Mantis Worker run on a single container (mantisagent) A simple hello world web application that sends events to Mantis A simple Java application that sends events to Manits","title":"Download the docker-compose file"},{"location":"getting-started/tutorials/docker/#mantis-admin-ui","text":"The Mantis Admin UI allows you to manage your Mantis Jobs. Open the Mantis UI in a new browser window. Fill out the Registration form as follows Name: Example Email: example@example.com Master Name: Example Mantis API URL: http://localhost:7101 Mesos URL: http://localhost:5050 Click on Create The Mantis Admin page should be pre-populated with all the Mantis examples.","title":"Mantis Admin UI"},{"location":"getting-started/tutorials/docker/#try-out-mantis-jobs","text":"Now that you have setup a Mantis cluster locally try running some of the preconfigured Mantis samples Sine Function Sample - A simple job that generates x and y coordinates of a sine wave. Twitter Sample - Connects to a twitter stream using consumer and token keys specified and performs a streaming word count. On Demand Sample - Demonstrates how Mantis Jobs can pull events on demand from external applications.","title":"Try out Mantis Jobs"},{"location":"getting-started/tutorials/docker/#next-steps","text":"Setup Mantis in AWS and run the samples Write your first Mantis Job To teardown the Mantis cluster, issue the following command $ cd mantis $ docker-compose -f docker-compose.yml down","title":"Next steps"},{"location":"getting-started/tutorials/local/","text":"Explore a Mantis Job locally Prerequisites JDK 8 or higher Build and run the synthetic-sourcejob sample Clone the mantis-examples repo: $ git clone https://github.com/Netflix/mantis-examples.git Run the synthetic-sourcejob sample via gradle. This job outputs request events sourced from an imaginary service. The RequestEvent data has information such as uri, status, userId, country etc. Data Source Jobs are mantis jobs that allow consumers to filter the raw stream down to just the events they are interested in. This filtering is done by specifying an MQL query while connecting to the sink. To run the sample execute the following command. $ cd mantis-examples/synthetic-sourcejob $ ../gradlew execute This will launch the job and you would see output like 2019 -10-06 14 :14:07 INFO StageExecutors:254 main - initializing io.mantisrx.sourcejob.synthetic.stage.TaggingStage 2019 -10-06 14 :14:07 INFO SinkPublisher:82 main - Got sink subscription, onSubscribe = null 2019 -10-06 14 :14:07 INFO ServerSentEventsSink:141 main - Serving modern HTTP SSE server sink on port: 8436 The default Mantis sink is a ServerSentEvent sink that opens a port allowing anyone to connect to it and stream the results of the job. Look for a line like Serving modern HTTP SSE server sink on port: 8436 The source job is now up and ready to serve data. Let us query for requests from countries where the status code is 500. Such an MQL query would like like this. select country from stream where status == 500 In another terminal window curl this port $ curl localhost:8436?subscriptionId=nj criterion=select%20country%20from%20stream%20where%20status%3D%3D500 clientId=nj2 Here the subscriptionId and clientId are any valid strings. They are used to tag events that match the query. The criterion parameter is the URLEncoded MQL query. You should see events matching your query appear in your terminal data: { country : Ecuador , mantis.meta.sourceName : SyntheticRequestSource , mantis.meta.timestamp :1570396602599, status :500 } data: { country : Solomon Islands , mantis.meta.sourceName : SyntheticRequestSource , mantis.meta.timestamp :1570396603342, status :500 } data: { country : Liberia , mantis.meta.sourceName : SyntheticRequestSource , mantis.meta.timestamp :1570396603844, status :500 } Next Steps Import the project into your IDE to explore the code. Try out other samples from the Mantis examples repository . Set up Mantis locally using Docker and run the samples. Set up Mantis in AWS and run the samples. Learn to write your first Mantis Job .","title":"Explore Locally"},{"location":"getting-started/tutorials/local/#explore-a-mantis-job-locally","text":"","title":"Explore a Mantis Job locally"},{"location":"getting-started/tutorials/local/#prerequisites","text":"JDK 8 or higher","title":"Prerequisites"},{"location":"getting-started/tutorials/local/#build-and-run-the-synthetic-sourcejob-sample","text":"Clone the mantis-examples repo: $ git clone https://github.com/Netflix/mantis-examples.git Run the synthetic-sourcejob sample via gradle. This job outputs request events sourced from an imaginary service. The RequestEvent data has information such as uri, status, userId, country etc. Data Source Jobs are mantis jobs that allow consumers to filter the raw stream down to just the events they are interested in. This filtering is done by specifying an MQL query while connecting to the sink. To run the sample execute the following command. $ cd mantis-examples/synthetic-sourcejob $ ../gradlew execute This will launch the job and you would see output like 2019 -10-06 14 :14:07 INFO StageExecutors:254 main - initializing io.mantisrx.sourcejob.synthetic.stage.TaggingStage 2019 -10-06 14 :14:07 INFO SinkPublisher:82 main - Got sink subscription, onSubscribe = null 2019 -10-06 14 :14:07 INFO ServerSentEventsSink:141 main - Serving modern HTTP SSE server sink on port: 8436 The default Mantis sink is a ServerSentEvent sink that opens a port allowing anyone to connect to it and stream the results of the job. Look for a line like Serving modern HTTP SSE server sink on port: 8436 The source job is now up and ready to serve data. Let us query for requests from countries where the status code is 500. Such an MQL query would like like this. select country from stream where status == 500 In another terminal window curl this port $ curl localhost:8436?subscriptionId=nj criterion=select%20country%20from%20stream%20where%20status%3D%3D500 clientId=nj2 Here the subscriptionId and clientId are any valid strings. They are used to tag events that match the query. The criterion parameter is the URLEncoded MQL query. You should see events matching your query appear in your terminal data: { country : Ecuador , mantis.meta.sourceName : SyntheticRequestSource , mantis.meta.timestamp :1570396602599, status :500 } data: { country : Solomon Islands , mantis.meta.sourceName : SyntheticRequestSource , mantis.meta.timestamp :1570396603342, status :500 } data: { country : Liberia , mantis.meta.sourceName : SyntheticRequestSource , mantis.meta.timestamp :1570396603844, status :500 }","title":"Build and run the synthetic-sourcejob sample"},{"location":"getting-started/tutorials/local/#next-steps","text":"Import the project into your IDE to explore the code. Try out other samples from the Mantis examples repository . Set up Mantis locally using Docker and run the samples. Set up Mantis in AWS and run the samples. Learn to write your first Mantis Job .","title":"Next Steps"},{"location":"internals/infrastructure-overview/","text":"Infrastructure Overview At a high level the Mantis Architecture consists of the Control plane (Mantis Master) and the Mantis runtime. The Master is responsible for managing the life cycle of Jobs, including creation, scheduling and deletion of jobs. The Mantis runtime is responsible for the actual execution of Mantis jobs. Major components Here is a brief description of the major components of the Mantis Architecture Mantis control plane (Master) The Control plane plane is responsible for managing the lifecycle of Mantis Jobs. Metadata related to the jobs is stored in the Metadata store. The Open source version of Mantis ships with a simple file based store. For production use of a highly available store is recommended. The default resource manager for scheduling Job tasks is Mesos. The control plane registers itself as a framework into Mesos. The control plane uses Fenzo to match the job to the available Mesos offers in an optimal manner. Mantis Runtime Mantis jobs are executed on Mesos agent(s) as one or more tasks (Job Workers) based on the job topology. The Mantis runtime is responsible for the actual execution of the job. This includes bootstrapping user code, exchanging control messages with the Master and establishing/maintaining upstream and downstream network connections per the Job execution DAG. One of the unique features of Mantis is that it allows Mantis Jobs to discover other Mantis Jobs and stream results of one job as input to another. The Mantis runtime does the work of discovering, connecting and maintaining these intra-job network flows. Mantis API Mantis API provides an easy REST interface for interacting with the Mantis system. It allows users to create/submit/kill Mantis Jobs and Mantis Job Clusters. Additionally, it also allows users to stream the output of any running jobs via WebSocket or SSE. Zookeeper Mantis usage of Zookeeper is fairly minimum. The zookeeper is used primarily for Master leader election both by the Mantis Control Plane and the Mesos Master. External Dependencies Zookeeper : Used for leader election of Mantis and Mesos Masters. RxJava 1.x : Provides the fluent, functional programming model for Mantis Jobs. Mesos 1.x : Powers the underlying resource management system. RxNetty 0.4 : Used for the intra-task network communication.","title":"Infrastructure Overview"},{"location":"internals/infrastructure-overview/#infrastructure-overview","text":"At a high level the Mantis Architecture consists of the Control plane (Mantis Master) and the Mantis runtime. The Master is responsible for managing the life cycle of Jobs, including creation, scheduling and deletion of jobs. The Mantis runtime is responsible for the actual execution of Mantis jobs.","title":"Infrastructure Overview"},{"location":"internals/infrastructure-overview/#major-components","text":"Here is a brief description of the major components of the Mantis Architecture","title":"Major components"},{"location":"internals/infrastructure-overview/#mantis-control-plane-master","text":"The Control plane plane is responsible for managing the lifecycle of Mantis Jobs. Metadata related to the jobs is stored in the Metadata store. The Open source version of Mantis ships with a simple file based store. For production use of a highly available store is recommended. The default resource manager for scheduling Job tasks is Mesos. The control plane registers itself as a framework into Mesos. The control plane uses Fenzo to match the job to the available Mesos offers in an optimal manner.","title":"Mantis control plane (Master)"},{"location":"internals/infrastructure-overview/#mantis-runtime","text":"Mantis jobs are executed on Mesos agent(s) as one or more tasks (Job Workers) based on the job topology. The Mantis runtime is responsible for the actual execution of the job. This includes bootstrapping user code, exchanging control messages with the Master and establishing/maintaining upstream and downstream network connections per the Job execution DAG. One of the unique features of Mantis is that it allows Mantis Jobs to discover other Mantis Jobs and stream results of one job as input to another. The Mantis runtime does the work of discovering, connecting and maintaining these intra-job network flows.","title":"Mantis Runtime"},{"location":"internals/infrastructure-overview/#mantis-api","text":"Mantis API provides an easy REST interface for interacting with the Mantis system. It allows users to create/submit/kill Mantis Jobs and Mantis Job Clusters. Additionally, it also allows users to stream the output of any running jobs via WebSocket or SSE.","title":"Mantis API"},{"location":"internals/infrastructure-overview/#zookeeper","text":"Mantis usage of Zookeeper is fairly minimum. The zookeeper is used primarily for Master leader election both by the Mantis Control Plane and the Mesos Master.","title":"Zookeeper"},{"location":"internals/infrastructure-overview/#external-dependencies","text":"Zookeeper : Used for leader election of Mantis and Mesos Masters. RxJava 1.x : Provides the fluent, functional programming model for Mantis Jobs. Mesos 1.x : Powers the underlying resource management system. RxNetty 0.4 : Used for the intra-task network communication.","title":"External Dependencies"},{"location":"internals/mantis-publish/","text":"The Mantis Publish runtime flow consists of three phases: connecting, event processing, and event delivery. Phase 1: Connecting Mantis Publish will only stream an event from your application into Mantis if there is a subscriber to that specific application instance with an MQL query that matches the event. Any [Mantis Job] can connect to these applications. However, it is a best practice to have [Source Jobs] connect to Mantis Publish applications. This is because Source Jobs provide several conveniences over regular Mantis Jobs, such as multiplexing events and connection management. By leveraging Source Jobs as an intermediary, Mantis Jobs are able to consume events from an external source without having to worry about lower-level details of that external source. This is possible through job chaining, which Mantis provides by default. When connecting to a Mantis Publish application, downstream Mantis Jobs will send subscription requests with an MQL query via HTTP to a Source Job. The Source Job will store these subscriptions in memory. These subscriptions are then fetched by upstream applications at the edge running the Mantis Publish library. Once the upstream edge Mantis Publish application is aware of the subscription, it will start pushing events downstream into the Source Job. Tip The Mantis Publish library not only handles subscriptions, but also takes care of discovering Source Job workers so you do not have to worry about Source Job rebalancing/autoscaling. For more information about Source Jobs see Mantis Source Jobs . Creating Stream Subscriptions Clients such as Mantis Jobs connect to a Mantis Publish application by submitting a subscription represented by an HTTP request. Mantis Publish\u2019s StreamManager maintains these subscriptions in-memory. The StreamManager manages internal resources such as subscriptions, streams, and internal processing queues. Clients can create subscriptions to different event streams. There are two types: default streams contain events emitted by applications that use the Mantis Publish library. log streams contain events which may not be core to the application, such as log or general infrastructure events. Phase 2: Event Processing Event processing within Mantis Publish takes place in two steps: event ingestion and event dispatch. Event Ingestion Event ingestion begins at the edge, in your application, by invoking EventPublisher#publish which places the event onto an internal queue for dispatching. Event Dispatch Events are dispatched by a drainer thread created by the Event Publisher. The drainer will drain events from the internal queue previously populated by EventPublisher#publish , perform some transformations, and finally dispatch events over the network and into Mantis. Events are transformed by an EventProcessor which processes events one at a time. Transformation includes the following steps: Masks sensitive fields in the event. Sensitive fields are referenced by a blacklist defined by a configuration. This blacklist is a comma-delimited string of keys you wish to blacklist in your event. Evaluates the MQL query of each subscription and builds a list of matching subscriptions. For each matching subscription, enriches the event with a superset of fields from the MQL query from all the other matching subscriptions (see the following diagram). Sends this enriched event to all of the subscribers (see Event Delivery below for the details). Info More Mantis Publish configuration options can be found in the mantis-publish configuration class . Phase 3: Event Delivery Mantis Publish delivers events on-demand. When a client subscribes to a Mantis Job that issues an MQL query, the Event Publisher delivers the event using non-blocking I/O.","title":"Mantis Publish"},{"location":"internals/mantis-publish/#phase-1-connecting","text":"Mantis Publish will only stream an event from your application into Mantis if there is a subscriber to that specific application instance with an MQL query that matches the event. Any [Mantis Job] can connect to these applications. However, it is a best practice to have [Source Jobs] connect to Mantis Publish applications. This is because Source Jobs provide several conveniences over regular Mantis Jobs, such as multiplexing events and connection management. By leveraging Source Jobs as an intermediary, Mantis Jobs are able to consume events from an external source without having to worry about lower-level details of that external source. This is possible through job chaining, which Mantis provides by default. When connecting to a Mantis Publish application, downstream Mantis Jobs will send subscription requests with an MQL query via HTTP to a Source Job. The Source Job will store these subscriptions in memory. These subscriptions are then fetched by upstream applications at the edge running the Mantis Publish library. Once the upstream edge Mantis Publish application is aware of the subscription, it will start pushing events downstream into the Source Job. Tip The Mantis Publish library not only handles subscriptions, but also takes care of discovering Source Job workers so you do not have to worry about Source Job rebalancing/autoscaling. For more information about Source Jobs see Mantis Source Jobs .","title":"Phase 1: Connecting"},{"location":"internals/mantis-publish/#creating-stream-subscriptions","text":"Clients such as Mantis Jobs connect to a Mantis Publish application by submitting a subscription represented by an HTTP request. Mantis Publish\u2019s StreamManager maintains these subscriptions in-memory. The StreamManager manages internal resources such as subscriptions, streams, and internal processing queues. Clients can create subscriptions to different event streams. There are two types: default streams contain events emitted by applications that use the Mantis Publish library. log streams contain events which may not be core to the application, such as log or general infrastructure events.","title":"Creating Stream Subscriptions"},{"location":"internals/mantis-publish/#phase-2-event-processing","text":"Event processing within Mantis Publish takes place in two steps: event ingestion and event dispatch.","title":"Phase 2: Event Processing"},{"location":"internals/mantis-publish/#event-ingestion","text":"Event ingestion begins at the edge, in your application, by invoking EventPublisher#publish which places the event onto an internal queue for dispatching.","title":"Event Ingestion"},{"location":"internals/mantis-publish/#event-dispatch","text":"Events are dispatched by a drainer thread created by the Event Publisher. The drainer will drain events from the internal queue previously populated by EventPublisher#publish , perform some transformations, and finally dispatch events over the network and into Mantis. Events are transformed by an EventProcessor which processes events one at a time. Transformation includes the following steps: Masks sensitive fields in the event. Sensitive fields are referenced by a blacklist defined by a configuration. This blacklist is a comma-delimited string of keys you wish to blacklist in your event. Evaluates the MQL query of each subscription and builds a list of matching subscriptions. For each matching subscription, enriches the event with a superset of fields from the MQL query from all the other matching subscriptions (see the following diagram). Sends this enriched event to all of the subscribers (see Event Delivery below for the details). Info More Mantis Publish configuration options can be found in the mantis-publish configuration class .","title":"Event Dispatch"},{"location":"internals/mantis-publish/#phase-3-event-delivery","text":"Mantis Publish delivers events on-demand. When a client subscribes to a Mantis Job that issues an MQL query, the Event Publisher delivers the event using non-blocking I/O.","title":"Phase 3: Event Delivery"},{"location":"internals/runtime/","text":"The Mantis Runtime is consists of two components: A single Mantis Master which coordinates the execution of Mantis Jobs . Independent Mantis Jobs which receive streams of events as input, transform events one at a time, and produce streams of events as output. This page assumes familiarity with Mantis Job high-level concepts. An introduction can be found in Writing Mantis Jobs . This page presents internal details for Mantis Jobs. Mantis Job Components A Mantis Job consists of three components . Each one is based on a cold Observable that emits events to the next Observer in the Observable chain: Source The Source component is an RxFunction that consumes data in a streaming, non-blocking, backpressure -aware manner from an external service. Processing Stage A Processing Stage component is based on an RxFunction . This is where event transformations take place. There can be many Processing Stages in a Mantis Job. Sink The Sink component is based on an RxAction . It asynchronously emits results of the final Processing Stage to an external service. Note Mantis Jobs can consume events from typical external services such as APIs, databases, and Kafka topics. Mantis Jobs can also consume events emitted by other Mantis Jobs. This is referred to in Mantis as job chaining . Runtime Lifecycle The entry point for a Mantis Job is the Mantis Worker . The Mantis Master starts three primary services on a Mantis Worker when the Master boots the Worker up: The virtual machine worker service interacts with the underlying substrate, currently Mesos . This service subscribes to task updates and registers the Mantis Worker with Mesos executor callbacks to launch Mantis Jobs. The heartbeat service sends HTTP heartbeat requests to notify the Mantis Master that the worker is alive and available to process events. The stage executor dynamically loads bytecode for a Mantis Job, creates an in-memory representation of all the metadata required to execute events for that Mantis Job, and processes events for the current Processing Stage. Job Master Stage The Job Master autoscales Processing Stages. It can autoscale such stages independently of each other. If the configuration of a Job indicates that any Processing Stage is autoscalable, Mantis will automatically add a Job Master as the initial processing stage of the Job. This is a hidden stage that Job owners do not explicitly manage; instead, Mantis will create and configure a JobMasterService . This service creates a subscription to worker metrics via the WorkerMetricHandler and a MetricsClient which receives metrics over HTTP via SSE and sends them over to the JobAutoScaler . Job Autoscaler The Job autoscaler is based on a PID controller . Within this autoscaler are three controllers for CPU, memory, and network resources which continuously calculate an error value and apply corrections. Once the autoscaler makes a prediction, it delegates an API call to the Mantis Master to perform the scaling action on resources for a Processing stage. Single-Stage and Multi-Stage Jobs A Job with only one Processing Stage is a single-stage Job. In such a case, the entire Job (Source, Processing Stage, and Sink) will execute on the current worker node. A Job with more than one Processing Stage is a multi-stage Job. In such a Job, the stage executor will first inspect the current component. If the current component is a Source, then the executor will execute it as a Source. Otherwise, it will inspect the context again to determine if current component is a Sink. If so, it will acquire a port and create a SinkPublisher to publish events to the next Job. Finally, if the component is a normal Processing Stage, then the executor will execute its transformations.","title":"Runtime"},{"location":"internals/runtime/#mantis-job-components","text":"A Mantis Job consists of three components . Each one is based on a cold Observable that emits events to the next Observer in the Observable chain: Source The Source component is an RxFunction that consumes data in a streaming, non-blocking, backpressure -aware manner from an external service. Processing Stage A Processing Stage component is based on an RxFunction . This is where event transformations take place. There can be many Processing Stages in a Mantis Job. Sink The Sink component is based on an RxAction . It asynchronously emits results of the final Processing Stage to an external service. Note Mantis Jobs can consume events from typical external services such as APIs, databases, and Kafka topics. Mantis Jobs can also consume events emitted by other Mantis Jobs. This is referred to in Mantis as job chaining .","title":"Mantis Job Components"},{"location":"internals/runtime/#runtime-lifecycle","text":"The entry point for a Mantis Job is the Mantis Worker . The Mantis Master starts three primary services on a Mantis Worker when the Master boots the Worker up: The virtual machine worker service interacts with the underlying substrate, currently Mesos . This service subscribes to task updates and registers the Mantis Worker with Mesos executor callbacks to launch Mantis Jobs. The heartbeat service sends HTTP heartbeat requests to notify the Mantis Master that the worker is alive and available to process events. The stage executor dynamically loads bytecode for a Mantis Job, creates an in-memory representation of all the metadata required to execute events for that Mantis Job, and processes events for the current Processing Stage.","title":"Runtime Lifecycle"},{"location":"internals/runtime/#job-master-stage","text":"The Job Master autoscales Processing Stages. It can autoscale such stages independently of each other. If the configuration of a Job indicates that any Processing Stage is autoscalable, Mantis will automatically add a Job Master as the initial processing stage of the Job. This is a hidden stage that Job owners do not explicitly manage; instead, Mantis will create and configure a JobMasterService . This service creates a subscription to worker metrics via the WorkerMetricHandler and a MetricsClient which receives metrics over HTTP via SSE and sends them over to the JobAutoScaler .","title":"Job Master Stage"},{"location":"internals/runtime/#job-autoscaler","text":"The Job autoscaler is based on a PID controller . Within this autoscaler are three controllers for CPU, memory, and network resources which continuously calculate an error value and apply corrections. Once the autoscaler makes a prediction, it delegates an API call to the Mantis Master to perform the scaling action on resources for a Processing stage.","title":"Job Autoscaler"},{"location":"internals/runtime/#single-stage-and-multi-stage-jobs","text":"A Job with only one Processing Stage is a single-stage Job. In such a case, the entire Job (Source, Processing Stage, and Sink) will execute on the current worker node. A Job with more than one Processing Stage is a multi-stage Job. In such a Job, the stage executor will first inspect the current component. If the current component is a Source, then the executor will execute it as a Source. Otherwise, it will inspect the context again to determine if current component is a Sink. If so, it will acquire a port and create a SinkPublisher to publish events to the next Job. Finally, if the component is a normal Processing Stage, then the executor will execute its transformations.","title":"Single-Stage and Multi-Stage Jobs"},{"location":"internals/source-jobs/","text":"Mantis Source Jobs are Mantis Jobs that fetch data from external sources. There are four types of Source Jobs: Mantis Publish Source Jobs read from their sources by using the Mantis Publish client library. As such, they do not apply MQL on events themselves. Instead, they propagate the MQL queries upstream to Mantis Publish running on the external source, which then applies the MQL queries to the events it produces and only then pushes those events downstream to the Request Source Job. Kafka Source Jobs consume events from Kafka and apply MQL to each incoming event. Composition of a Source Job A Source Job is composed of three components: the default Source custom Processing Stages including a tagging operator the default Sink consisting of an SSE operator For example, here is how you might declare a Kafka Source Job: MantisJob . source ( KafkaSource ) . stage ( getAckableTaggingStage (), CustomizedAutoAckTaggingStage . config ()) . sink ( new TaggedDataSourceSink ( new QueryRequestPreProcessor (), new QueryRequestPostProcessor ())) . lifecycle (...) . create (); Source (RxFunction) The Source in this example contains code that creates and manages connections to Kafka using the 0.10 high level consumer. It creates an Observable with backpressure semantics by leveraging the SyncOnSubscribe class. Processing Stage (RxFunction) The next stage in this Job is the Processing Stage which enriches events with metadata . This stage transforms events in the following way: Applies a user-defined pre-mapping function. This is a Groovy function that takes a Map String, Object and returns a Map String, Object referenced by a variable named e . Filters out empty events. Inspects its internal subscription registry and enriches each event with all matching subscriptions. Subscriptions are represented by an MQL query and are registered when a consumer (e.g. Mantis Job) subscribes to the Source Job. Each event is enriched with fields specified by the projections of a subscription\u2019s MQL query, as in the following illustration: Sink (RxAction) In order for a consumer to consume events from a Source Job, the consumer connects to the Job\u2019s Sink. Consumers subscribe to a Source Job by sending a subscription request over HTTP to the Source Job\u2019s Sink. When a consumer connects to a Sink, the consumer must provide three query parameters: criterion \u2014 An MQL query string clientId \u2014 This is automatically generated if you use the Mantis client library; it defaults to the Mantis Job ID subscriptionId \u2014 This is used as a load-balancing mechanism for clientId A consumer (represented as a client through clientId ) may have many consumer instances (represented as susbcriptions through subscriptionId ). Source Jobs use clientId and subscriptionId to broadcast and/or load balance events to consumers. Source Jobs will broadcast an event to all clientId s. This means that consumer instances with different clientId s will each receive the same event. However, Source Jobs will load balance an event within a clientId . This means that consumer instances with the same clientId but different subscriptionId s are effectively grouped together. Events with the same clientId are load balanced among its subscriptionId s. Example of subscribing to a Source Job\u2019s Sink which outputs the results of a sine wave function: curl \"http:// instance-address : port ?clientId= myId = mySubscription =select%20*%20where%20true\" data: {\"x\": 60.000000, \"y\": -3.048106} data: {\"x\": 100.000000, \"y\": -5.063656} data: {\"x\": 26.000000, \"y\": 7.625585} \u22ee Sinks have a pre-processor ( QueryRequestPreProcessor ), a post-processor ( QueryRequestPostProcessor ), and a router: The pre-processor is an RxFunction that registers the consumer\u2019s query, with their clientId and subscriptionId , into an in-memory cache called a QueryRefCountMap when a consumer instance connects to the Sink. This registers queries so that the Source Job can apply them to events as those events are ingested by the Source Job. The post-processor is an RxFunction that de-registers subscriptions from the QueryRefCountMap when a consumer instance disconnects from the Sink. The Source Job removes removes the clientId entirely from the QueryRefCountMap only when all of its subscriptionId s have been removed. The router routes incoming events for a clientId s to its subscriptions. It does this by using a drainer called a ChunkProcessor to drain events from an internal queue on an interval and randomly distribute the events to subscriptions. Note Typically, subscriptions to a Source Job come from other Mantis Jobs. However, because subscriptions are SSE endpoints, you can subscribe to Source Jobs over that same SSE endpoint to view the Job\u2019s output for debugging purposes. Caveats Source Jobs are single-stage Mantis Jobs that perform projection and filtering operations. MQL queries containing groupBy , orderBy , and window are ignored. These clauses are interpreted into RxJava operations and run by downstream Mantis Jobs. Mantis Publish-based Source Jobs do not autoscale . Autoscaling Mantis Publish-based Source Jobs requires future work to reshuffle connections among all Source Job instances and their upstream Mantis Publish connections.","title":"Source Jobs"},{"location":"internals/source-jobs/#composition-of-a-source-job","text":"A Source Job is composed of three components: the default Source custom Processing Stages including a tagging operator the default Sink consisting of an SSE operator For example, here is how you might declare a Kafka Source Job: MantisJob . source ( KafkaSource ) . stage ( getAckableTaggingStage (), CustomizedAutoAckTaggingStage . config ()) . sink ( new TaggedDataSourceSink ( new QueryRequestPreProcessor (), new QueryRequestPostProcessor ())) . lifecycle (...) . create ();","title":"Composition of a Source Job"},{"location":"internals/source-jobs/#source-rxfunction","text":"The Source in this example contains code that creates and manages connections to Kafka using the 0.10 high level consumer. It creates an Observable with backpressure semantics by leveraging the SyncOnSubscribe class.","title":"Source (RxFunction)"},{"location":"internals/source-jobs/#processing-stage-rxfunction","text":"The next stage in this Job is the Processing Stage which enriches events with metadata . This stage transforms events in the following way: Applies a user-defined pre-mapping function. This is a Groovy function that takes a Map String, Object and returns a Map String, Object referenced by a variable named e . Filters out empty events. Inspects its internal subscription registry and enriches each event with all matching subscriptions. Subscriptions are represented by an MQL query and are registered when a consumer (e.g. Mantis Job) subscribes to the Source Job. Each event is enriched with fields specified by the projections of a subscription\u2019s MQL query, as in the following illustration:","title":"Processing Stage (RxFunction)"},{"location":"internals/source-jobs/#sink-rxaction","text":"In order for a consumer to consume events from a Source Job, the consumer connects to the Job\u2019s Sink. Consumers subscribe to a Source Job by sending a subscription request over HTTP to the Source Job\u2019s Sink. When a consumer connects to a Sink, the consumer must provide three query parameters: criterion \u2014 An MQL query string clientId \u2014 This is automatically generated if you use the Mantis client library; it defaults to the Mantis Job ID subscriptionId \u2014 This is used as a load-balancing mechanism for clientId A consumer (represented as a client through clientId ) may have many consumer instances (represented as susbcriptions through subscriptionId ). Source Jobs use clientId and subscriptionId to broadcast and/or load balance events to consumers. Source Jobs will broadcast an event to all clientId s. This means that consumer instances with different clientId s will each receive the same event. However, Source Jobs will load balance an event within a clientId . This means that consumer instances with the same clientId but different subscriptionId s are effectively grouped together. Events with the same clientId are load balanced among its subscriptionId s. Example of subscribing to a Source Job\u2019s Sink which outputs the results of a sine wave function: curl \"http:// instance-address : port ?clientId= myId = mySubscription =select%20*%20where%20true\" data: {\"x\": 60.000000, \"y\": -3.048106} data: {\"x\": 100.000000, \"y\": -5.063656} data: {\"x\": 26.000000, \"y\": 7.625585} \u22ee Sinks have a pre-processor ( QueryRequestPreProcessor ), a post-processor ( QueryRequestPostProcessor ), and a router: The pre-processor is an RxFunction that registers the consumer\u2019s query, with their clientId and subscriptionId , into an in-memory cache called a QueryRefCountMap when a consumer instance connects to the Sink. This registers queries so that the Source Job can apply them to events as those events are ingested by the Source Job. The post-processor is an RxFunction that de-registers subscriptions from the QueryRefCountMap when a consumer instance disconnects from the Sink. The Source Job removes removes the clientId entirely from the QueryRefCountMap only when all of its subscriptionId s have been removed. The router routes incoming events for a clientId s to its subscriptions. It does this by using a drainer called a ChunkProcessor to drain events from an internal queue on an interval and randomly distribute the events to subscriptions. Note Typically, subscriptions to a Source Job come from other Mantis Jobs. However, because subscriptions are SSE endpoints, you can subscribe to Source Jobs over that same SSE endpoint to view the Job\u2019s output for debugging purposes.","title":"Sink (RxAction)"},{"location":"internals/source-jobs/#caveats","text":"Source Jobs are single-stage Mantis Jobs that perform projection and filtering operations. MQL queries containing groupBy , orderBy , and window are ignored. These clauses are interpreted into RxJava operations and run by downstream Mantis Jobs. Mantis Publish-based Source Jobs do not autoscale . Autoscaling Mantis Publish-based Source Jobs requires future work to reshuffle connections among all Source Job instances and their upstream Mantis Publish connections.","title":"Caveats"},{"location":"internals/mantis-jobs/introduction/","text":"A Mantis Job is a JVM-based stream processing application that takes in an Observable stream of data items, transforms this stream by using RxJava operators, and then outputs the results as another Observable stream. RxJava Observables can be visualized by using \u201cmarble diagrams\u201d: You can combine multiple RxJava operators to transform an Observable stream of items in many ways: The above diagram shows a Mantis Job composed of two operators that process an input stream to compose an output stream. The first operator, map , emits a new Observable for each item emitted by the source Observable. The second operator, merge , emits each item emitted by those Observables as a fresh Observable stream. There is an enormous wealth of ways in which you can transform streams of data by using RxJava Observable operators. If the volume of data to be processed is too large for a single worker to handle, you can \u201cdivide and conquer\u201d by grouping and dividing the operators across various processing stages , as in the following diagram: Mantis Job Clusters You define a Job Cluster before you submit Jobs . A Job Cluster is a containing entity for Jobs. It defines metadata and certain service-level agreements ( SLA s). Job Clusters ease Job lifecycle management and Job revisioning. For example, by setting the SLA of a Job Cluster to Min=1 and Max=1, you ensure that exactly one Job instance is always running for that Cluster. The Job Cluster also has default Job parameters that any new Jobs submitted to the Cluster inherit. You can update new Job artifacts into the Job Cluster so that the next Job submission picks up the latest version. Components of a Mantis Job A Mantis Job has three components , each of which has a corresponding chapter in this documentation: Source \u2014 Fetches data from an external source and makes it available in the form of an Observable. Processing Stage \u2014 Transforms the Observable stream by means of a variety of operators. Sink \u2014 Pushes the resulting Observable out in the form of a fresh stream. Directory Structure of a Mantis Job In addition to the source files, Mantis requires some meta-files to be present in the Job artifact. Here is a sample directory structure: src/ - main/ - java/ - com/ - myorg/ - MyJob.java - resources/ - META-INF/ - services/ - io.mantisrx.runtime.MantisJobProvider - job.properties - job-test.properties io.mantisrx.runtime.MantisJobProvider (required) \u2014 lists the fully-qualified name of the Java class that implements the MantisJobProvider interface job.properties and job-test.properties (optional) \u2014 required only if you are initializing the platform via the .lifecycle() method Creating a Mantis Job To create a Mantis Job, call MantisJob\u2026create() . When you do so, interpolate the following builder methods. The first three \u2014 .source() , .stage() , and .sink() \u2014 are required, they must be the first three of the methods that you call, and you must call them in that order: .source( AbstractJobSource ) \u2014 required, see The Source Component .stage( Stage , stageConfig ) \u2014 required, (call this one or more times) see The Processing Stage Component .sink( Sink ) \u2014 required, see The Sink Component .lifecycle( Lifecycle ) \u2014 optional, allows for start-up and shut-down hooks .parameterDefinition( ParameterDefinition ) \u2014 optional, (call this zero to many times) define any parameters your job requires here .metadata( Metadata ) \u2014 optional, (call this zero to many times) define your job name and description here of this class this method returns an object of this class MantisJob \u27f6 source() \u27f6 SourceHolder \u2936 SourceHolder \u27f6 stage() \u27f6 Stages \u2936 [ Stages \u27f6 stage() \u27f6 Stages ] \u2936 Stages \u27f6 sink() \u27f6 Config \u2936 [ Config \u27f6 lifecycle() \u27f6 Config ] \u2936 [ Config \u27f6 parameterDefinition() \u27f6 Config ] \u2936 [ Config \u27f6 metadata() \u27f6 Config ] \u2936 Config \u27f6 create() \u27f6 Job Lifecycle Management You can establish start-up and shut-down procedures for your Mantis Job by means of the .lifecycle(). builder method. Pass this method a Lifecycle object, that is to say, an object that implements the following methods: startup() \u2014 initialize arbitrary application configs, perform dependency injection, and any long running or shared service libraries. shutdown() \u2014 gracefully close connections, shut down long running or shared service libraries, and general process cleanup. getServiceLocator() \u2014 returns a ServiceLocator that implements the service(key) method. Implement this method to return your dependency injection object such as Guice. Defining Parameters To create a Parameter in order to pass it to the .parameterDefinition() builder method of the MantisJob builder, use the following new ParameterVariety ()\u2026build() builder methods: .name( string ) \u2014 a user-friendly name for your Parameter .description( string ) \u2014 a user-friendly description of your Parameter .defaultValue( value ) \u2014 the value of this Parameter if the Job does not override it .validator( Validator ) \u2014 a way of checking the proposed Parameter values for validity so bad values can be rejected before you submit the Job .required() \u2014 call this builder method if the Job must provide a value for this Parameter There are some built-in Parameter varieties you can choose from that correspond to common data types: BooleanParameter DoubleParameter IntParameter StringParameter Validators There are some standard Validators you can choose from that cover some common varieties of parameters: Validators.range( start , end ) \u2014 will reject as invalid Parameter values that do not lie between the indicated start and end numerical values (where start and end themselves are valid Parameter values) Validators.notNullOrEmpty() \u2014 will reject empty strings or null values Validators.alwaysPass() \u2014 will not reject any Parameter values as invalid Example For example: myStringParameter = new StringParameter (). name ( MyParameter ) . description ( This is a human-friendly description of my parameter ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( SomeValue ) . required () . build (); Defining Metadata To create metadata in order to pass it to the .metadata() builder method of the MantisJob builder, use the following new Metadata.Builder()\u2026build() builder methods: .name( string ) .description( string ) For example: myMetadata = new Metadata . Builder (). name ( MyMetadata ) . description ( Description of my metadata ) . build ();","title":"Introduction"},{"location":"internals/mantis-jobs/introduction/#mantis-job-clusters","text":"You define a Job Cluster before you submit Jobs . A Job Cluster is a containing entity for Jobs. It defines metadata and certain service-level agreements ( SLA s). Job Clusters ease Job lifecycle management and Job revisioning. For example, by setting the SLA of a Job Cluster to Min=1 and Max=1, you ensure that exactly one Job instance is always running for that Cluster. The Job Cluster also has default Job parameters that any new Jobs submitted to the Cluster inherit. You can update new Job artifacts into the Job Cluster so that the next Job submission picks up the latest version.","title":"Mantis Job Clusters"},{"location":"internals/mantis-jobs/introduction/#components-of-a-mantis-job","text":"A Mantis Job has three components , each of which has a corresponding chapter in this documentation: Source \u2014 Fetches data from an external source and makes it available in the form of an Observable. Processing Stage \u2014 Transforms the Observable stream by means of a variety of operators. Sink \u2014 Pushes the resulting Observable out in the form of a fresh stream.","title":"Components of a Mantis Job"},{"location":"internals/mantis-jobs/introduction/#directory-structure-of-a-mantis-job","text":"In addition to the source files, Mantis requires some meta-files to be present in the Job artifact. Here is a sample directory structure: src/ - main/ - java/ - com/ - myorg/ - MyJob.java - resources/ - META-INF/ - services/ - io.mantisrx.runtime.MantisJobProvider - job.properties - job-test.properties io.mantisrx.runtime.MantisJobProvider (required) \u2014 lists the fully-qualified name of the Java class that implements the MantisJobProvider interface job.properties and job-test.properties (optional) \u2014 required only if you are initializing the platform via the .lifecycle() method","title":"Directory Structure of a Mantis Job"},{"location":"internals/mantis-jobs/introduction/#creating-a-mantis-job","text":"To create a Mantis Job, call MantisJob\u2026create() . When you do so, interpolate the following builder methods. The first three \u2014 .source() , .stage() , and .sink() \u2014 are required, they must be the first three of the methods that you call, and you must call them in that order: .source( AbstractJobSource ) \u2014 required, see The Source Component .stage( Stage , stageConfig ) \u2014 required, (call this one or more times) see The Processing Stage Component .sink( Sink ) \u2014 required, see The Sink Component .lifecycle( Lifecycle ) \u2014 optional, allows for start-up and shut-down hooks .parameterDefinition( ParameterDefinition ) \u2014 optional, (call this zero to many times) define any parameters your job requires here .metadata( Metadata ) \u2014 optional, (call this zero to many times) define your job name and description here of this class this method returns an object of this class MantisJob \u27f6 source() \u27f6 SourceHolder \u2936 SourceHolder \u27f6 stage() \u27f6 Stages \u2936 [ Stages \u27f6 stage() \u27f6 Stages ] \u2936 Stages \u27f6 sink() \u27f6 Config \u2936 [ Config \u27f6 lifecycle() \u27f6 Config ] \u2936 [ Config \u27f6 parameterDefinition() \u27f6 Config ] \u2936 [ Config \u27f6 metadata() \u27f6 Config ] \u2936 Config \u27f6 create() \u27f6 Job","title":"Creating a Mantis Job"},{"location":"internals/mantis-jobs/introduction/#lifecycle-management","text":"You can establish start-up and shut-down procedures for your Mantis Job by means of the .lifecycle(). builder method. Pass this method a Lifecycle object, that is to say, an object that implements the following methods: startup() \u2014 initialize arbitrary application configs, perform dependency injection, and any long running or shared service libraries. shutdown() \u2014 gracefully close connections, shut down long running or shared service libraries, and general process cleanup. getServiceLocator() \u2014 returns a ServiceLocator that implements the service(key) method. Implement this method to return your dependency injection object such as Guice.","title":"Lifecycle Management"},{"location":"internals/mantis-jobs/introduction/#defining-parameters","text":"To create a Parameter in order to pass it to the .parameterDefinition() builder method of the MantisJob builder, use the following new ParameterVariety ()\u2026build() builder methods: .name( string ) \u2014 a user-friendly name for your Parameter .description( string ) \u2014 a user-friendly description of your Parameter .defaultValue( value ) \u2014 the value of this Parameter if the Job does not override it .validator( Validator ) \u2014 a way of checking the proposed Parameter values for validity so bad values can be rejected before you submit the Job .required() \u2014 call this builder method if the Job must provide a value for this Parameter There are some built-in Parameter varieties you can choose from that correspond to common data types: BooleanParameter DoubleParameter IntParameter StringParameter","title":"Defining Parameters"},{"location":"internals/mantis-jobs/introduction/#validators","text":"There are some standard Validators you can choose from that cover some common varieties of parameters: Validators.range( start , end ) \u2014 will reject as invalid Parameter values that do not lie between the indicated start and end numerical values (where start and end themselves are valid Parameter values) Validators.notNullOrEmpty() \u2014 will reject empty strings or null values Validators.alwaysPass() \u2014 will not reject any Parameter values as invalid","title":"Validators"},{"location":"internals/mantis-jobs/introduction/#example","text":"For example: myStringParameter = new StringParameter (). name ( MyParameter ) . description ( This is a human-friendly description of my parameter ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( SomeValue ) . required () . build ();","title":"Example"},{"location":"internals/mantis-jobs/introduction/#defining-metadata","text":"To create metadata in order to pass it to the .metadata() builder method of the MantisJob builder, use the following new Metadata.Builder()\u2026build() builder methods: .name( string ) .description( string ) For example: myMetadata = new Metadata . Builder (). name ( MyMetadata ) . description ( Description of my metadata ) . build ();","title":"Defining Metadata"},{"location":"internals/mantis-jobs/processing-stage/","text":"A Processing Stage component of a Mantis Job processes the stream of data by [transforming] it in some way. You can combine multiple Processing Stages into a single Job, or you can create a Job that consists of a single Processing Stage. In its simplest form, a Processing Stage is a chain of RxJava operators operating on the Observable provided by the Source component. Transformations can be broadly categorized into two types: Scalar or Grouped . There are four varieties of Processing Stage, based on what type of transformation they accomplish: Scalar-to-Scalar \u2014 Also known as \u201cnarrow transformation,\u201d this variety of Stage converts an Observable T into an Observable R . Such a Stage extends the ScalarToScalar class . Scalar-to-Key / Scalar-to-Group \u2014 Also known as \u201cwidening transformation,\u201d this variety of Stage groups each element emitted by the source Observable by key. Typically you use such a Stage when you build a map/reduce-style job in which you need to perform stateful computations but the volume of data is too large to fit on a single worker. In such a model, the incoming data is divided into multiple streams, one per group. A subsequent State will typically to the stateful computation (for instance, calculating percentiles for the group). The purpose of this Scalar-to-Key State is to tag each incoming element with the group that it belongs to. Mantis then takes care of routing all traffic for the same group to the same worker in the subsequent stage. Scalar-to-Key \u2014 This is the legacy way of grouping data (it is more elegant but comes with a performance penalty). You extend the ScalarToKey class and transform an Observable T into an Observable GroupedObservable K,R (where K is the key). See the RxJava groupBy operator for more information. Scalar-to-Group \u2014 This is a more efficient way to group data. You extend the ScalarToGroup class and transform an Observable T into an Observable MantisGroup K,R (where K is the key). This avoids the overhead associated with creating a GroupedObservable which can limit the number of groups it is possible to create. Key-to-Scalar / Group-to-Scalar \u2014 Once you have split a stream, you need a stage that can take grouped data and return it to a scalar form. Key-to-Scalar \u2014 This is the legacy method and is designed for streams that have been split via a ScalarToKey Stage. You extend the KeyToScalar class and transform a GroupedObservable K,T into an Observable T . Group-to-Scalar \u2014 This is the newer, faster method. It is less elegant, in that you must transform an Observable MantisGroup that contains payloads from all groups, and you must therefore manage the per-group state. Typically you would do this via a map that holds per-group state and evicts entries that haven\u2019t been touched recently. This method has much better performance than the Key-to-Scalar method because it omits the overhead around RxJava\u2019s GroupedObservable . Key-to-Key / Group-to-Group \u2014 You can further split a keyed stream by grouping it again if you have some use case that requires this.","title":"Processing Stage"},{"location":"internals/mantis-jobs/sink/","text":"The Sink component of a Mantis Job serves two purposes: Trigger job execution when a client subscribes to a Job RxJava cold Observables have a lazy execution model. Execution begins only when someone subscribes to the Observable. A Mantis Job is a complex Observable chain, and to trigger the execution of such a Job, somebody needs to subscribe to it. This happens in the Sink component. Output the results of job execution in a streaming fashion Once you are done processing the input stream in the Processing Stage component, you need to figure out what to do with the results. Most jobs that write to some other system do so within the Processing Stage component itself (in such a case, the Sink component is usually used for debugging purposes). To create a Sink component, you implement the Sink interface: import io.mantisrx.runtime.Context ; import io.mantisrx.runtime.PortRequest ; import rx.Observable ; import rx.functions.Action3 ; public interface Sink T extends Action3 Context , PortRequest , Observable T { } Built-in Sinks Some Sinks are provided by Mantis. To get access to these Sinks, add this line to your source: import io.mantisrx.runtime.sink.Sinks ; SSE Sink The SSE Sink is commonly used. It makes the results of your Stage transformation available in the form of an SSE stream. To get an SSE Sink, pass an encoder function to the sse() method that accepts the data to be streamed as input and outputs data encoded as needed. The following example attaches a sink to a Mantis Job that passes the data from the last Processing Stage of the job, unchanged, to the SSE stream: return MantisJob . \u2026 . sink ( Sinks . sse (( String data ) - data )). \u2026 . create (); sysout Sink The sysout Sink simply outputs the results from the previous Processing Stage directly to sysout. For example: return MantisJob . \u2026 . sink ( Sinks . sysout ()). \u2026 . create (); eagerSubscribe Sink A typical Sink subscribes to the output of the previous Processing Stage at some point during the call to its call() method. You may want some of your Mantis Jobs to start executing as soon as you launch them. These include Jobs that run perpetually and power things like alerts, dashboards, and so forth. You can modify a Sink with eagerSubscribe() to create a Sink that that instead subscribes to the output of the previous Processing Stage immediately when its call() method is called, even if it has more processing to do within that method before it can respond to the output. This will start your Mantis Job more quickly, but may mean some of its initial data is dropped. For example: return MantisJob . \u2026 . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ))). \u2026 . create (); Here is a more complete example: return MantisJob // Reuse existing class that does all the plumbing of connecting to source jobs . source ( new JobSource ()) // Groups requests by ESN . stage ( new GroupByStage (), GroupByStage . config ()) // Computes sum over a window . stage ( new FastAggregateStage (), FastAggregateStage . config ()) // Collects the data and makes it availabe over SSE . stage ( new CollectStage (), CollectStage . config ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ) )) // Here we define the job parameter overrides // The query sent to the source job. // Here we fetch the esn for all requests hitting the source . parameterDefinition ( new StringParameter () . name ( MantisSourceJobConnector . MANTIS_SOURCEJOB_CRITERION ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( SELECT customer_id, client_ip WHERE true ) . build ()) . metadata ( new Metadata . Builder () . name ( GroupByIp ) . description ( Connects to a source job and counts the number of requests sent from each ip within a window ) . build ()) . create (); toMany Sink You can hook up multiple Sinks to the same final Stage of a Job by using the toMany Sink. To do this, pass each Sink to the toMany() method. For example: return MantisJob . \u2026 . sink ( Sinks . toMany ( Sinks . sysout (), Sinks . sse (( String data ) - data ))). \u2026 . create (); Custom Sinks If you do not want to use one of the provided Sinks, or if you need to customize one of those (for instance, if you need access to the query parameters supplied by a client who is connecting into your Job, or if you need pre and post hooks to perform operations as a new client connects or disconnects), you can create your own Sink. To do so, implement the Sink interface. The ServerSentEventsSink builder takes three parameters: Preprocess function this callback gives you access to the query parameters; it is invoked before job execution begins Postprocess function this callback allows you to perform any clean-up actions; it is invoked just after the client connection is terminated Predicate function allows you to filter your output stream based on the given predicate, which allows you to filter based on query parameters sent by the client Here is an example of a custom Sink that uses the ServerSentEventsSink builder: package com.netflix.mantis.sourcejob ; import io.mantisrx.runtime.Context ; import io.mantisrx.runtime.PortRequest ; import io.mantisrx.runtime.sink.ServerSentEventsSink ; import io.mantisrx.runtime.sink.Sink ; import io.mantisrx.runtime.sink.predicate.Predicate ; import java.util.List ; import java.util.Map ; import rx.Observable ; import rx.functions.Func1 ; import rx.functions.Func2 ; import com.google.common.base.Charsets ; public class SourceSink implements Sink String { Func2 Map String , List String , Context , Void preProcessor = new NoOpProcessor (); Func2 Map String , List String , Context , Void postProcessor = new NoOpProcessor (); private String clientId = DEFAULT_CLIENT_ID ; static class NoOpProcessor implements Func2 Map String , List String , Context , Void { @Override public Void call ( Map String , List String t1 , Context t2 ) { return null ; } } public SourceSink () { } public SourceSink ( Func2 Map String , List String , Context , Void preProcessor , Func2 Map String , List String , Context , Void postProcessor , String mantisClientId ) { this . postProcessor = postProcessor ; this . preProcessor = preProcessor ; this . clientId = mantisClientId ; } @Override public void call ( Context context , PortRequest portRequest , Observable String observable ) { observable = observable . filter ( new Func1 String , Boolean () { @Override public Boolean call ( String t1 ) { return ! t1 . isEmpty (); } }); ServerSentEventsSink String sink = new ServerSentEventsSink . Builder String () . withEncoder ( new Func1 String , String () { @Override public String call ( String data ) { return data ; } }) . withPredicate ( new Predicate String ( description , new EventFilter ( clientId ))) . withRequestPreprocessor ( preProcessor ) . withRequestPostprocessor ( postProcessor ) . build (); observable . subscribe (); sink . call ( context , portRequest , observable ); } public static void main ( String [] args ) { String s = {\\ amazon.availability-zone\\ :\\ us-east-1e\\ ,\\ status\\ :200,\\ type\\ :\\ EVENT\\ ,\\ matched-clients\\ :\\ client6\\ ,\\ currentTime\\ :1409595016697,\\ duration-millis\\ :172} ; byte [] barr = s . getBytes ( Charsets . UTF_8 ); System . out . println ( size: + barr . length ); } }","title":"Sink"},{"location":"internals/mantis-jobs/sink/#built-in-sinks","text":"Some Sinks are provided by Mantis. To get access to these Sinks, add this line to your source: import io.mantisrx.runtime.sink.Sinks ;","title":"Built-in Sinks"},{"location":"internals/mantis-jobs/sink/#sse-sink","text":"The SSE Sink is commonly used. It makes the results of your Stage transformation available in the form of an SSE stream. To get an SSE Sink, pass an encoder function to the sse() method that accepts the data to be streamed as input and outputs data encoded as needed. The following example attaches a sink to a Mantis Job that passes the data from the last Processing Stage of the job, unchanged, to the SSE stream: return MantisJob . \u2026 . sink ( Sinks . sse (( String data ) - data )). \u2026 . create ();","title":"SSE Sink"},{"location":"internals/mantis-jobs/sink/#sysout-sink","text":"The sysout Sink simply outputs the results from the previous Processing Stage directly to sysout. For example: return MantisJob . \u2026 . sink ( Sinks . sysout ()). \u2026 . create ();","title":"sysout Sink"},{"location":"internals/mantis-jobs/sink/#eagersubscribe-sink","text":"A typical Sink subscribes to the output of the previous Processing Stage at some point during the call to its call() method. You may want some of your Mantis Jobs to start executing as soon as you launch them. These include Jobs that run perpetually and power things like alerts, dashboards, and so forth. You can modify a Sink with eagerSubscribe() to create a Sink that that instead subscribes to the output of the previous Processing Stage immediately when its call() method is called, even if it has more processing to do within that method before it can respond to the output. This will start your Mantis Job more quickly, but may mean some of its initial data is dropped. For example: return MantisJob . \u2026 . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ))). \u2026 . create (); Here is a more complete example: return MantisJob // Reuse existing class that does all the plumbing of connecting to source jobs . source ( new JobSource ()) // Groups requests by ESN . stage ( new GroupByStage (), GroupByStage . config ()) // Computes sum over a window . stage ( new FastAggregateStage (), FastAggregateStage . config ()) // Collects the data and makes it availabe over SSE . stage ( new CollectStage (), CollectStage . config ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) - data ) )) // Here we define the job parameter overrides // The query sent to the source job. // Here we fetch the esn for all requests hitting the source . parameterDefinition ( new StringParameter () . name ( MantisSourceJobConnector . MANTIS_SOURCEJOB_CRITERION ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( SELECT customer_id, client_ip WHERE true ) . build ()) . metadata ( new Metadata . Builder () . name ( GroupByIp ) . description ( Connects to a source job and counts the number of requests sent from each ip within a window ) . build ()) . create ();","title":"eagerSubscribe Sink"},{"location":"internals/mantis-jobs/sink/#tomany-sink","text":"You can hook up multiple Sinks to the same final Stage of a Job by using the toMany Sink. To do this, pass each Sink to the toMany() method. For example: return MantisJob . \u2026 . sink ( Sinks . toMany ( Sinks . sysout (), Sinks . sse (( String data ) - data ))). \u2026 . create ();","title":"toMany Sink"},{"location":"internals/mantis-jobs/sink/#custom-sinks","text":"If you do not want to use one of the provided Sinks, or if you need to customize one of those (for instance, if you need access to the query parameters supplied by a client who is connecting into your Job, or if you need pre and post hooks to perform operations as a new client connects or disconnects), you can create your own Sink. To do so, implement the Sink interface. The ServerSentEventsSink builder takes three parameters: Preprocess function this callback gives you access to the query parameters; it is invoked before job execution begins Postprocess function this callback allows you to perform any clean-up actions; it is invoked just after the client connection is terminated Predicate function allows you to filter your output stream based on the given predicate, which allows you to filter based on query parameters sent by the client Here is an example of a custom Sink that uses the ServerSentEventsSink builder: package com.netflix.mantis.sourcejob ; import io.mantisrx.runtime.Context ; import io.mantisrx.runtime.PortRequest ; import io.mantisrx.runtime.sink.ServerSentEventsSink ; import io.mantisrx.runtime.sink.Sink ; import io.mantisrx.runtime.sink.predicate.Predicate ; import java.util.List ; import java.util.Map ; import rx.Observable ; import rx.functions.Func1 ; import rx.functions.Func2 ; import com.google.common.base.Charsets ; public class SourceSink implements Sink String { Func2 Map String , List String , Context , Void preProcessor = new NoOpProcessor (); Func2 Map String , List String , Context , Void postProcessor = new NoOpProcessor (); private String clientId = DEFAULT_CLIENT_ID ; static class NoOpProcessor implements Func2 Map String , List String , Context , Void { @Override public Void call ( Map String , List String t1 , Context t2 ) { return null ; } } public SourceSink () { } public SourceSink ( Func2 Map String , List String , Context , Void preProcessor , Func2 Map String , List String , Context , Void postProcessor , String mantisClientId ) { this . postProcessor = postProcessor ; this . preProcessor = preProcessor ; this . clientId = mantisClientId ; } @Override public void call ( Context context , PortRequest portRequest , Observable String observable ) { observable = observable . filter ( new Func1 String , Boolean () { @Override public Boolean call ( String t1 ) { return ! t1 . isEmpty (); } }); ServerSentEventsSink String sink = new ServerSentEventsSink . Builder String () . withEncoder ( new Func1 String , String () { @Override public String call ( String data ) { return data ; } }) . withPredicate ( new Predicate String ( description , new EventFilter ( clientId ))) . withRequestPreprocessor ( preProcessor ) . withRequestPostprocessor ( postProcessor ) . build (); observable . subscribe (); sink . call ( context , portRequest , observable ); } public static void main ( String [] args ) { String s = {\\ amazon.availability-zone\\ :\\ us-east-1e\\ ,\\ status\\ :200,\\ type\\ :\\ EVENT\\ ,\\ matched-clients\\ :\\ client6\\ ,\\ currentTime\\ :1409595016697,\\ duration-millis\\ :172} ; byte [] barr = s . getBytes ( Charsets . UTF_8 ); System . out . println ( size: + barr . length ); } }","title":"Custom Sinks"},{"location":"internals/mantis-jobs/source/","text":"To implement a Mantis Job Source component , you must implement the io.mantisrx.runtime.Source interface. A Source returns Observable Observable T , that is, an Observable that emits Observables. Each of the emitted Observables represents a stream of data from a single target server. Varieties of Sources Sources can be roughly divided into two categories: Sources that read data from the output of other Mantis Jobs this may include Source Jobs (more on this below) or ordinary Mantis Jobs Custom sources that read data directly from Amazon S3, SQS, Apache Kafka , etc. Mantis Job Sources You can string Mantis Jobs together by using the output of one Mantis Job as the input to another. This is useful if you want to break up your processing into multiple, reusable components and to take advantage of code and data reuse. In such a case, you do not have access to the complete set of Mantis Query Language (MQL) capabilities that you do in the case of a Source Job, but you can use MQL in client mode. Connecting to a Mantis Job To connect to a Mantis Job, use a JobSource when you call MantisJob.create() \u2014 declare the following parameters when you use this class: sourceJobName (required) \u2014 the name of any valid Job Cluster (not necessarily a \u201cSource Job\u201d) sample (optional) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( sourceJobName ) . description ( The name of the job ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( MyDefaultJob ) . build ()) . parameterDefinition ( new IntParameter (). name ( sample ) . description ( The number of samples per second ) . validator ( Validators . range ( - 1 , 10000 )) . defaultValue ( - 1 ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create (); Source Job Sources Mantis has a concept of Source Jobs which are Mantis Jobs with added conveniences and efficiences that simplify accessing data from certain sources. Your job can simply connect to a source job as its data source rather than trying to retrieve the data from its native home. There are two advantages to this approach: Source Jobs handle all of the implementation details around interacting with the native data source. Source Jobs come with a simple query interface based on the Mantis Query Language (MQL) , which allows you to filter the data from the source before processing it. In the case of source jobs that fetch data from application servers directly, this filter gets pushed all the way to those target servers so that no data flows unless someone is asking for it. Source Jobs reuse data so that multiple matching MQL queries are forwarded downstream instead of paying the cost to fetch and serialize/deserialize the same data multiple times from the upstream source. Broadcast Mode By default, Mantis will distribute the data that is output from the Source Job among the various workers in the processing stage of your Mantis Job. Each of those workers will get a subset of the complete data from the Source Job. You can override this by instructing the Source Job to use \u201cbroadcast mode\u201d. If you do this, Mantis will send the complete set of data from the Source Job to every worker in your Job. Connecting to a Source Job Since Source Jobs are fundamentally Mantis Jobs, you should use a JobSource when you call MantisJob.create() to connect to a particular Source Job. The difference is that you should pass in additional parameters: sourceJobName (required) \u2014 the name of the source Job Cluster you want to connect to sample (required) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling criterion (required) \u2014 a query expression in MQL to filter the source clientId (optional) \u2014 by default, the jobId of the client Job; the Source Job uses this to distribute data between all the subscriptions of the client Job enableMetaMessages (optional) \u2014 the source job may occasionally inject meta messages (with the prefix mantis.meta. ) that indicate things like data drops on the Source Job side. For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( sourceJobName ) . description ( The name of the job ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( MyDefaultSourceJob ) . build ()) . parameterDefinition ( new IntParameter (). name ( sample ) . description ( The number of samples per second ) . validator ( Validators . range ( - 1 , 10000 )) . defaultValue ( - 1 ) . build ()) . parameterDefinition ( new StringParameter (). name ( criterion ) . description ( Filter the source with this MQL statement ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( true ) . build ()) . parameterDefinition ( new StringParameter (). name ( clientId ) . description ( the ID of the client job ) . validator ( Validators . alwaysPass ()) . build ()) . parameterDefinition ( new BooleanParameter (). name ( enableMetaMessages ) . description ( Is the source allowed to inject meta messages ) . validator ( Validators . alwaysPass ()) . defaultValue ( true ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create (); Custom Sources Custom sources may be implemented and used to access data sources for which Mantis does not have a Source Job. Implementers are free to implement the Source interface to fetch data from an external source. Here is an example in a source which implements the Source interface to consume data from Kafka. Learning When Source Data is Incomplete You may want to know whether or not the stream you are receiving from your source is complete. Streams may be incomplete for a number of reasons: A connection to one or more of the Source Job workers is lost. A connection exists but no data is flowing. Data is intentionally dropped from a source because of the backpressure strategy you are using. You can use the following boolean method within your JobSource#call method to determine whether or not all of your client connections are complete: DefaultSinkConnectionStatusObserver . getInstance ( true ). isConnectedToAllSinks ()","title":"Source"},{"location":"internals/mantis-jobs/source/#varieties-of-sources","text":"Sources can be roughly divided into two categories: Sources that read data from the output of other Mantis Jobs this may include Source Jobs (more on this below) or ordinary Mantis Jobs Custom sources that read data directly from Amazon S3, SQS, Apache Kafka , etc.","title":"Varieties of Sources"},{"location":"internals/mantis-jobs/source/#mantis-job-sources","text":"You can string Mantis Jobs together by using the output of one Mantis Job as the input to another. This is useful if you want to break up your processing into multiple, reusable components and to take advantage of code and data reuse. In such a case, you do not have access to the complete set of Mantis Query Language (MQL) capabilities that you do in the case of a Source Job, but you can use MQL in client mode.","title":"Mantis Job Sources"},{"location":"internals/mantis-jobs/source/#connecting-to-a-mantis-job","text":"To connect to a Mantis Job, use a JobSource when you call MantisJob.create() \u2014 declare the following parameters when you use this class: sourceJobName (required) \u2014 the name of any valid Job Cluster (not necessarily a \u201cSource Job\u201d) sample (optional) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( sourceJobName ) . description ( The name of the job ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( MyDefaultJob ) . build ()) . parameterDefinition ( new IntParameter (). name ( sample ) . description ( The number of samples per second ) . validator ( Validators . range ( - 1 , 10000 )) . defaultValue ( - 1 ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create ();","title":"Connecting to a Mantis Job"},{"location":"internals/mantis-jobs/source/#source-job-sources","text":"Mantis has a concept of Source Jobs which are Mantis Jobs with added conveniences and efficiences that simplify accessing data from certain sources. Your job can simply connect to a source job as its data source rather than trying to retrieve the data from its native home. There are two advantages to this approach: Source Jobs handle all of the implementation details around interacting with the native data source. Source Jobs come with a simple query interface based on the Mantis Query Language (MQL) , which allows you to filter the data from the source before processing it. In the case of source jobs that fetch data from application servers directly, this filter gets pushed all the way to those target servers so that no data flows unless someone is asking for it. Source Jobs reuse data so that multiple matching MQL queries are forwarded downstream instead of paying the cost to fetch and serialize/deserialize the same data multiple times from the upstream source.","title":"Source Job Sources"},{"location":"internals/mantis-jobs/source/#broadcast-mode","text":"By default, Mantis will distribute the data that is output from the Source Job among the various workers in the processing stage of your Mantis Job. Each of those workers will get a subset of the complete data from the Source Job. You can override this by instructing the Source Job to use \u201cbroadcast mode\u201d. If you do this, Mantis will send the complete set of data from the Source Job to every worker in your Job.","title":"Broadcast Mode"},{"location":"internals/mantis-jobs/source/#connecting-to-a-source-job","text":"Since Source Jobs are fundamentally Mantis Jobs, you should use a JobSource when you call MantisJob.create() to connect to a particular Source Job. The difference is that you should pass in additional parameters: sourceJobName (required) \u2014 the name of the source Job Cluster you want to connect to sample (required) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling criterion (required) \u2014 a query expression in MQL to filter the source clientId (optional) \u2014 by default, the jobId of the client Job; the Source Job uses this to distribute data between all the subscriptions of the client Job enableMetaMessages (optional) \u2014 the source job may occasionally inject meta messages (with the prefix mantis.meta. ) that indicate things like data drops on the Source Job side. For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( sourceJobName ) . description ( The name of the job ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( MyDefaultSourceJob ) . build ()) . parameterDefinition ( new IntParameter (). name ( sample ) . description ( The number of samples per second ) . validator ( Validators . range ( - 1 , 10000 )) . defaultValue ( - 1 ) . build ()) . parameterDefinition ( new StringParameter (). name ( criterion ) . description ( Filter the source with this MQL statement ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( true ) . build ()) . parameterDefinition ( new StringParameter (). name ( clientId ) . description ( the ID of the client job ) . validator ( Validators . alwaysPass ()) . build ()) . parameterDefinition ( new BooleanParameter (). name ( enableMetaMessages ) . description ( Is the source allowed to inject meta messages ) . validator ( Validators . alwaysPass ()) . defaultValue ( true ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create ();","title":"Connecting to a Source Job"},{"location":"internals/mantis-jobs/source/#custom-sources","text":"Custom sources may be implemented and used to access data sources for which Mantis does not have a Source Job. Implementers are free to implement the Source interface to fetch data from an external source. Here is an example in a source which implements the Source interface to consume data from Kafka.","title":"Custom Sources"},{"location":"internals/mantis-jobs/source/#learning-when-source-data-is-incomplete","text":"You may want to know whether or not the stream you are receiving from your source is complete. Streams may be incomplete for a number of reasons: A connection to one or more of the Source Job workers is lost. A connection exists but no data is flowing. Data is intentionally dropped from a source because of the backpressure strategy you are using. You can use the following boolean method within your JobSource#call method to determine whether or not all of your client connections are complete: DefaultSinkConnectionStatusObserver . getInstance ( true ). isConnectedToAllSinks ()","title":"Learning When Source Data is Incomplete"},{"location":"operate/autoscaling/","text":"Being a cloud-native platform Mantis supports autoscaling out-of-the-box. Both the agent cluster and the Mantis Jobs can be configured to autoscale. You can define a policy for your Jobs in which they autoscale their resources based on the dynamic needs resulting from variation in the input data they process. This provides two benefits: You can define Jobs to process data without provisioning for peak usage all the time. Mantis uses cluster resources optimally without leaving resources idle. Horizontal Scaling Your Mantis Jobs are composed of in part of Processing Stages , with each stage responsible for a different stream processing task. Because different stages may have different computational needs, each stage has its own autoscaling policy. A Processing Stage is further subdivided into workers . A worker is the smallest unit of work that is scheduled. Each worker requests a certain number of CPUs, some amount of memory, and a certain amount of network bandwidth. When a Processing Stage scales, the number of workers in that stage increases or decreases (the resources that Mantis allocates to an individual worker in the stage do not change as a result of scaling). Scaling a Processing Stage Manually You may define a Processing Stage as scalable without defining an autoscaling policy for it. In such a case the stage is considered manually scalable and you can scale it by means of the Mantis UI or the Mantis API . Setting an Autoscaling Policy Warning You should take care that your autoscaling strategies do not contradict each other. For example, if you set a CPU-based strategy and a network-based strategy, one may want to trigger a scale-up and the other a scale-down at the same time. You define the autoscaling policy for a Processing Stage by setting the following parameters: Min and Max number of workers \u2014 This sets how many workers Mantis will guarantee to be working within the Processing Stage at any particular time. Increment and decrement values \u2014 This indicates how many workers are added to or removed from a stage each time the stage autoscales up or down. Cooldown seconds \u2014 This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. Strategies \u2014 See autoscaling strategies for details. The following example shows how you might establish the autoscaling policy for a stage in the Mantis UI: The illustration above shows a stage with an autoscaling policy that specifies a minimum of 5 and a maximum of 20 workers. It uses a single strategy, that of network bandwidth usage. Autoscaling Scenarios There are four varieties of autoscaling scenarios that you should consider for your Mantis Job: The Processing Stage connects to a cold Source , such as a Kafka topic. Autoscaling works well for this type of stage (the initial stage in a Job that connects to the Source). For example, if your stage connects to a Kafka source, a change in the number of workers in the first stage of your Mantis Job causes the Kafka client to redistribute the partitions of the topic among the new number of workers. The Processing Stage connects to a hot source, such as a working server. The Source stage (stage #1) will have to re-partition the Source servers after an autoscale event on the Processing Stage. This is mainly a concern for Source Jobs . Upon receipt of a modified number of workers, each worker re-partitions the current servers into its own index of the new total. This results in a new list of servers to connect to (for both scale up and scale down), some of which may be already connected. Making a new connection to a Source server evicts any old existing connection from the same Job. This guarantees that no duplicate messages are sent to a Mantis Job. (The solution for this scenario is currently in development.) Rewrite this; it\u2019s not very clear. The Processing Stage connects to another Mantis Job. In this case, the initial Processing Stage in a Job that connects to the output of the previous Mantis Job has strong connectivity into the Source Job via the use of Mantis Java client. In suc a case, all workers from this Processing Stage connect to all workers of the source Job\u2019s Sink . Therefore, autoscaling this type of Job works well. The Processing Stage connects to a previous Processing Stage in the same Mantis Job. Each Processing Stage is strongly connected to its previous Processing Stage. Therefore, autoscaling of this type typically works well. However, a Processing Stage following a grouped stage (a Processing Stage that does a group by operation) receives a grouped Observable or MantisGroup . When Mantis scales such a grouped stage, these groups are repartitioned on to the new number of workers. The Processing Stage following such a grouped stage must, therefore, be prepared to potentially receive a different set of groups after a rescale operation. How does a subsequent stage learn that the previous stage has autoscaled? Note that the number of groups resulting from a group by operation is the maximum limit on the number of workers that can be expected to work on such groups (unless a subsequent processing stage subdivides those groups). In the following illustrations, a processing stage that does a group by operation groups the incoming data into three groups, each one of which is handled by a single worker in the subsequent processing stage. When that second stage scales up and adds another worker, that worker remains idle and does not assist in processing data because there are not enough groups to distribute among the larger number of workers. Before autoscaling: After autoscaling: Updating Autoscalable Jobs To upload a Mantis Job when you have new code to push, upload the .jar or .zip artifact file to Mantis, and make any necessary adjustments to its behavior and policies by using the Mantis UI. You can also do this in two ways via the Mantis API: Update the Job Cluster with a new version for its artifact file along with new scheduling information. This updated JAR and scheduling info are available to use with the next Job submission. However, currently-running Jobs continue to run with whatever artifact file they were started with. Quick update the Job Cluster with only a new artifact file version and submit a new Job with it. The new Job is submitted by using the scheduling info from the last Job submitted. The latter of the above two is convenient not only because you provide the minimal information needed to update the Job. But, also, because when it picks up the scheduling info from the last Job submitted, if it is running, the new Job is started with the same number of workers as the last one. That is, if it had scaled up, the new Job starts scaled up as well.","title":"Autoscaling"},{"location":"operate/autoscaling/#horizontal-scaling","text":"Your Mantis Jobs are composed of in part of Processing Stages , with each stage responsible for a different stream processing task. Because different stages may have different computational needs, each stage has its own autoscaling policy. A Processing Stage is further subdivided into workers . A worker is the smallest unit of work that is scheduled. Each worker requests a certain number of CPUs, some amount of memory, and a certain amount of network bandwidth. When a Processing Stage scales, the number of workers in that stage increases or decreases (the resources that Mantis allocates to an individual worker in the stage do not change as a result of scaling).","title":"Horizontal Scaling"},{"location":"operate/autoscaling/#scaling-a-processing-stage-manually","text":"You may define a Processing Stage as scalable without defining an autoscaling policy for it. In such a case the stage is considered manually scalable and you can scale it by means of the Mantis UI or the Mantis API .","title":"Scaling a Processing Stage Manually"},{"location":"operate/autoscaling/#setting-an-autoscaling-policy","text":"Warning You should take care that your autoscaling strategies do not contradict each other. For example, if you set a CPU-based strategy and a network-based strategy, one may want to trigger a scale-up and the other a scale-down at the same time. You define the autoscaling policy for a Processing Stage by setting the following parameters: Min and Max number of workers \u2014 This sets how many workers Mantis will guarantee to be working within the Processing Stage at any particular time. Increment and decrement values \u2014 This indicates how many workers are added to or removed from a stage each time the stage autoscales up or down. Cooldown seconds \u2014 This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. Strategies \u2014 See autoscaling strategies for details. The following example shows how you might establish the autoscaling policy for a stage in the Mantis UI: The illustration above shows a stage with an autoscaling policy that specifies a minimum of 5 and a maximum of 20 workers. It uses a single strategy, that of network bandwidth usage.","title":"Setting an Autoscaling Policy"},{"location":"operate/autoscaling/#autoscaling-scenarios","text":"There are four varieties of autoscaling scenarios that you should consider for your Mantis Job: The Processing Stage connects to a cold Source , such as a Kafka topic. Autoscaling works well for this type of stage (the initial stage in a Job that connects to the Source). For example, if your stage connects to a Kafka source, a change in the number of workers in the first stage of your Mantis Job causes the Kafka client to redistribute the partitions of the topic among the new number of workers. The Processing Stage connects to a hot source, such as a working server. The Source stage (stage #1) will have to re-partition the Source servers after an autoscale event on the Processing Stage. This is mainly a concern for Source Jobs . Upon receipt of a modified number of workers, each worker re-partitions the current servers into its own index of the new total. This results in a new list of servers to connect to (for both scale up and scale down), some of which may be already connected. Making a new connection to a Source server evicts any old existing connection from the same Job. This guarantees that no duplicate messages are sent to a Mantis Job. (The solution for this scenario is currently in development.) Rewrite this; it\u2019s not very clear. The Processing Stage connects to another Mantis Job. In this case, the initial Processing Stage in a Job that connects to the output of the previous Mantis Job has strong connectivity into the Source Job via the use of Mantis Java client. In suc a case, all workers from this Processing Stage connect to all workers of the source Job\u2019s Sink . Therefore, autoscaling this type of Job works well. The Processing Stage connects to a previous Processing Stage in the same Mantis Job. Each Processing Stage is strongly connected to its previous Processing Stage. Therefore, autoscaling of this type typically works well. However, a Processing Stage following a grouped stage (a Processing Stage that does a group by operation) receives a grouped Observable or MantisGroup . When Mantis scales such a grouped stage, these groups are repartitioned on to the new number of workers. The Processing Stage following such a grouped stage must, therefore, be prepared to potentially receive a different set of groups after a rescale operation. How does a subsequent stage learn that the previous stage has autoscaled? Note that the number of groups resulting from a group by operation is the maximum limit on the number of workers that can be expected to work on such groups (unless a subsequent processing stage subdivides those groups). In the following illustrations, a processing stage that does a group by operation groups the incoming data into three groups, each one of which is handled by a single worker in the subsequent processing stage. When that second stage scales up and adds another worker, that worker remains idle and does not assist in processing data because there are not enough groups to distribute among the larger number of workers. Before autoscaling: After autoscaling:","title":"Autoscaling Scenarios"},{"location":"operate/autoscaling/#updating-autoscalable-jobs","text":"To upload a Mantis Job when you have new code to push, upload the .jar or .zip artifact file to Mantis, and make any necessary adjustments to its behavior and policies by using the Mantis UI. You can also do this in two ways via the Mantis API: Update the Job Cluster with a new version for its artifact file along with new scheduling information. This updated JAR and scheduling info are available to use with the next Job submission. However, currently-running Jobs continue to run with whatever artifact file they were started with. Quick update the Job Cluster with only a new artifact file version and submit a new Job with it. The new Job is submitted by using the scheduling info from the last Job submitted. The latter of the above two is convenient not only because you provide the minimal information needed to update the Job. But, also, because when it picks up the scheduling info from the last Job submitted, if it is running, the new Job is started with the same number of workers as the last one. That is, if it had scaled up, the new Job starts scaled up as well.","title":"Updating Autoscalable Jobs"},{"location":"operate/autoscalingstrategies/","text":"There are various strategies for autoscaling. These strategies affect when an autoscale activity will occur and also how many workers to scale up/down a stage. They fall into 2 main categories. Rule based strategies monitor a specific resource and scale up/down when a certain threshold is reached. PID control based strategies pick a resource utilization level and scale up/down dynamically to maintain that level. Rule Based Strategy Rule based strategy can be defined for the following resources: Resource Metric CPU group: ResourceUsage name: cpuPctUsageCurr aggregation: AVG Memory group: ResourceUsage name: totMemUsageCurr aggregation: AVG Network group: ResourceUsage name: nwBytesUsageCurr aggregation: AVG JVMMemory group: ResourceUsage name: jvmMemoryUsedBytes aggregation: AVG DataDrop group: DataDrop name: dropCount aggregation: AVG KafkaLag group: consumer-fetch-manager-metrics name: records-lag-max aggregation: MAX KafkaProcessed group: consumer-fetch-manager-metrics name: records-consumed-rate aggregation: AVG UserDefined Metric is defined by user with job parameter mantis.jobmaster.autoscale.metric in this format {group}::{name}::{aggregation} . Each strategy has the following parameters: Name Description Scale down below percentage When the aggregated value for all workers falls below this value, the stage will scale down. It will scale down by the decrement value specified in the policy. For data drop, this is calculated as the number of data items dropped divided by the total number of data items, dropped+processed. For CPU, Memory, etc., it is calculated as a percentage of allocated resource when you defined the worker. Scale up above percentage When the aggregated value for all workers rises above this value, the stage will scale up. Rolling count This value helps to keep jitter out of the autoscaling process. Instead of scaling immediately the first time values fall outside of the scale-down and scale-up percentage thresholds you define, Mantis will wait until the thresholds are exceeded a certain number of times within a certain window. For example, a rolling count of \u201c6 of 10\u201d means that only if in ten consecutive observations six or more of the observations fall below the scale-down threshold will the stage be scaled down. It is possible to employ multiple rule based strategies for a stage. In this case, as soon as 1 strategy triggers a scaling action, the cooldown will prevent subsequent strategies from scaling for that duration. Note Ideally, there should be zero data drop, so there isn\u2019t an elegant way to express \u201cscale down below percentage\u201d for data drop. Specifying \u201c0%\u201d as the \u201cscale down below percentage\u201d effectively means the data drop percentage never trigger a scale down. For this reason, it is best to use the data drop strategy in conjunction with another strategy that provides the scale-down trigger. PID Control Based Strategy PID control system uses a continuous feedback loop to maintain a signal at a target level (set point). Mantis offers variations of this strategy that operates on different signals. Additionally, they try to learn the appropriate target over time without the need for user input. The PID controller computes the magnitude of scale up/down based on the drift between the observed signal and the target. Thus, this strategy can react quicker to big changes compared to rule based strategies, since rule based strategies use fixed step size. Cooldown still applies between scaling activities. Clutch The strategy operates on CPU, Memory, Network and UserDefined. Every 24 hours, it will pick 1 dominant resource and use the P99 value as the target set point. For the next 24 hours, it will monitor that resource metric and scale the stage to keep the metric close to the target set point. In the initial 24 hours after the job is first launched, this strategy will scale the stage to max in order to learn the first dominant resource and set point. This also happens if the job is restarted. Clutch with User Defined Configs With this strategy, the user defines the target for each resource without relying on the system to learn it. There is no need for an initial 24 hour pin high period, the PID controller can start working right away. You can supply the configuration as JSON in the job parameter mantis.jobmaster.clutch.config . Example: { minSize : 3 , maxSize : 25 , cooldownSeconds : 300 , rps : 8000 , cpu : { setPoint : 60.0 , rope : [ 25.0 , 0.0 ], kp : 0.01 , kd : 0.01 }, memory : { setPoint : 100.0 , rope : [ 0.0 , 0.0 ], kp : 0.01 , kd : 0.01 }, network : { setPoint : 60.0 , rope : [ 25.0 , 0.0 ], kp : 0.01 , kd : 0.01 } } Field Description minSize Minimum number of workers in the stage. It will not scale down below this number. maxSize Maximum number of workers in the stage. It will not scale up above this number. cooldownSeconds This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. maxAdjustment Optional. The maximum number of workers to scale up/down in a single operation. rps Expected RPS per worker. Must be 0. cpu , memory , network Configure PID controller for each resource. setPoint Target set point for the resource. This is expressed as a percentage of allocated resource to the worker. For example, 60.0 on network means network bytes should be 60% of the network limit on machine definition. rope Lower and upper buffer around the set point. Metric values within this buffer are assumed to be at set point, and thus contributes an error of 0 to the PID controller. kp Multiplier for the proportional term of the PID controller. This will affect the size of scaling actions. kd Multiplier for the derivative term of the PID controller. This will affect the size of scaling actions. Clutch RPS This strategy scales the stage base on number of events processed. The target set point is a percentile of RPS. The signal is the sum of RPS, inbound drops, Kafka lag, and outbound drops from source jobs. Therefore, it effectively tries to keep drops and lag at 0. It takes the first 10 minutes after job launch to learn the first RPS set point. This also applies if the job is restarted, the set point does not carry over. Afterwards, it may adjust the set point once every hour. Set point should become stable the longer a job runs, since it simply takes a percentile of historical RPS metric. The source job drop metric is not enabled by default. It is only applicable if your job connects to an upstream job as input. You can enable this metric by setting the job parameter mantis.jobmaster.autoscale.sourcejob.metric.enabled to true. Further, you need to specify the source job targets in the job parameter mantis.jobmaster.autoscale.sourcejob.target . You can omit this if your job already has a target parameter for connecting to source jobs, the auto scaler will pick that up automatically. Example: { targets : [ { sourceJobName : ConsolidatedLoggingEventKafkaSource } ] } Optionally, it is possible to further customize the behavior of the PID controller. You can supply the configuration as JSON in the job parameter mantis.jobmaster.clutch.config . Example: { rpsConfig : { setPointPercentile : 50.0 , rope : [ 30.0 , 0.0 ], scaleDownBelowPct : 40.0 , scaleUpAbovePct : 10.0 , scaleDownMultiplier : 0.5 , scaleDownMultiplier : 3.0 } } Field Description setPointPercentile Percentile of historical RPS metric to use as the set point. Valid input is between [1.0, 100.0] Default is 75.0 . rope Lower and upper buffer around the set point. The value is interpreted as percentage of set point. For example, [30.0, 30.0] means values within 30% of set point is considered to have 0 error. Valid input is between [0.0, 100.0] Default is [30.0, 0.0] . scaleDownBelowPct Only scale down if the PID controller output is below this number. It can be used to delay a scaling action. Valid input is between [0.0, 100.0] . Default is 0.0 . scaleUpAbovePct Only scale up if the PID controller output is above this number. It can be used to delay a scaling action. Valid input is between [0.0, 100.0] . Default is 0.0 . scaleDownMultiplier Artificially increase/decrease the size of scale down by this factor. Default is 1.0 . scaleUpMultiplier Artificially increase/decrease the size of scale up by this factor. Default is 1.0 . Clutch Experimental (Developmental Use Only) This strategy is internally used for testing new Clutch implementations. It should not be used for production jobs.","title":"Autoscaling Strategies"},{"location":"operate/autoscalingstrategies/#rule-based-strategy","text":"Rule based strategy can be defined for the following resources: Resource Metric CPU group: ResourceUsage name: cpuPctUsageCurr aggregation: AVG Memory group: ResourceUsage name: totMemUsageCurr aggregation: AVG Network group: ResourceUsage name: nwBytesUsageCurr aggregation: AVG JVMMemory group: ResourceUsage name: jvmMemoryUsedBytes aggregation: AVG DataDrop group: DataDrop name: dropCount aggregation: AVG KafkaLag group: consumer-fetch-manager-metrics name: records-lag-max aggregation: MAX KafkaProcessed group: consumer-fetch-manager-metrics name: records-consumed-rate aggregation: AVG UserDefined Metric is defined by user with job parameter mantis.jobmaster.autoscale.metric in this format {group}::{name}::{aggregation} . Each strategy has the following parameters: Name Description Scale down below percentage When the aggregated value for all workers falls below this value, the stage will scale down. It will scale down by the decrement value specified in the policy. For data drop, this is calculated as the number of data items dropped divided by the total number of data items, dropped+processed. For CPU, Memory, etc., it is calculated as a percentage of allocated resource when you defined the worker. Scale up above percentage When the aggregated value for all workers rises above this value, the stage will scale up. Rolling count This value helps to keep jitter out of the autoscaling process. Instead of scaling immediately the first time values fall outside of the scale-down and scale-up percentage thresholds you define, Mantis will wait until the thresholds are exceeded a certain number of times within a certain window. For example, a rolling count of \u201c6 of 10\u201d means that only if in ten consecutive observations six or more of the observations fall below the scale-down threshold will the stage be scaled down. It is possible to employ multiple rule based strategies for a stage. In this case, as soon as 1 strategy triggers a scaling action, the cooldown will prevent subsequent strategies from scaling for that duration. Note Ideally, there should be zero data drop, so there isn\u2019t an elegant way to express \u201cscale down below percentage\u201d for data drop. Specifying \u201c0%\u201d as the \u201cscale down below percentage\u201d effectively means the data drop percentage never trigger a scale down. For this reason, it is best to use the data drop strategy in conjunction with another strategy that provides the scale-down trigger.","title":"Rule Based Strategy"},{"location":"operate/autoscalingstrategies/#pid-control-based-strategy","text":"PID control system uses a continuous feedback loop to maintain a signal at a target level (set point). Mantis offers variations of this strategy that operates on different signals. Additionally, they try to learn the appropriate target over time without the need for user input. The PID controller computes the magnitude of scale up/down based on the drift between the observed signal and the target. Thus, this strategy can react quicker to big changes compared to rule based strategies, since rule based strategies use fixed step size. Cooldown still applies between scaling activities.","title":"PID Control Based Strategy"},{"location":"operate/autoscalingstrategies/#clutch","text":"The strategy operates on CPU, Memory, Network and UserDefined. Every 24 hours, it will pick 1 dominant resource and use the P99 value as the target set point. For the next 24 hours, it will monitor that resource metric and scale the stage to keep the metric close to the target set point. In the initial 24 hours after the job is first launched, this strategy will scale the stage to max in order to learn the first dominant resource and set point. This also happens if the job is restarted.","title":"Clutch"},{"location":"operate/autoscalingstrategies/#clutch-with-user-defined-configs","text":"With this strategy, the user defines the target for each resource without relying on the system to learn it. There is no need for an initial 24 hour pin high period, the PID controller can start working right away. You can supply the configuration as JSON in the job parameter mantis.jobmaster.clutch.config . Example: { minSize : 3 , maxSize : 25 , cooldownSeconds : 300 , rps : 8000 , cpu : { setPoint : 60.0 , rope : [ 25.0 , 0.0 ], kp : 0.01 , kd : 0.01 }, memory : { setPoint : 100.0 , rope : [ 0.0 , 0.0 ], kp : 0.01 , kd : 0.01 }, network : { setPoint : 60.0 , rope : [ 25.0 , 0.0 ], kp : 0.01 , kd : 0.01 } } Field Description minSize Minimum number of workers in the stage. It will not scale down below this number. maxSize Maximum number of workers in the stage. It will not scale up above this number. cooldownSeconds This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. maxAdjustment Optional. The maximum number of workers to scale up/down in a single operation. rps Expected RPS per worker. Must be 0. cpu , memory , network Configure PID controller for each resource. setPoint Target set point for the resource. This is expressed as a percentage of allocated resource to the worker. For example, 60.0 on network means network bytes should be 60% of the network limit on machine definition. rope Lower and upper buffer around the set point. Metric values within this buffer are assumed to be at set point, and thus contributes an error of 0 to the PID controller. kp Multiplier for the proportional term of the PID controller. This will affect the size of scaling actions. kd Multiplier for the derivative term of the PID controller. This will affect the size of scaling actions.","title":"Clutch with User Defined Configs"},{"location":"operate/autoscalingstrategies/#clutch-rps","text":"This strategy scales the stage base on number of events processed. The target set point is a percentile of RPS. The signal is the sum of RPS, inbound drops, Kafka lag, and outbound drops from source jobs. Therefore, it effectively tries to keep drops and lag at 0. It takes the first 10 minutes after job launch to learn the first RPS set point. This also applies if the job is restarted, the set point does not carry over. Afterwards, it may adjust the set point once every hour. Set point should become stable the longer a job runs, since it simply takes a percentile of historical RPS metric. The source job drop metric is not enabled by default. It is only applicable if your job connects to an upstream job as input. You can enable this metric by setting the job parameter mantis.jobmaster.autoscale.sourcejob.metric.enabled to true. Further, you need to specify the source job targets in the job parameter mantis.jobmaster.autoscale.sourcejob.target . You can omit this if your job already has a target parameter for connecting to source jobs, the auto scaler will pick that up automatically. Example: { targets : [ { sourceJobName : ConsolidatedLoggingEventKafkaSource } ] } Optionally, it is possible to further customize the behavior of the PID controller. You can supply the configuration as JSON in the job parameter mantis.jobmaster.clutch.config . Example: { rpsConfig : { setPointPercentile : 50.0 , rope : [ 30.0 , 0.0 ], scaleDownBelowPct : 40.0 , scaleUpAbovePct : 10.0 , scaleDownMultiplier : 0.5 , scaleDownMultiplier : 3.0 } } Field Description setPointPercentile Percentile of historical RPS metric to use as the set point. Valid input is between [1.0, 100.0] Default is 75.0 . rope Lower and upper buffer around the set point. The value is interpreted as percentage of set point. For example, [30.0, 30.0] means values within 30% of set point is considered to have 0 error. Valid input is between [0.0, 100.0] Default is [30.0, 0.0] . scaleDownBelowPct Only scale down if the PID controller output is below this number. It can be used to delay a scaling action. Valid input is between [0.0, 100.0] . Default is 0.0 . scaleUpAbovePct Only scale up if the PID controller output is above this number. It can be used to delay a scaling action. Valid input is between [0.0, 100.0] . Default is 0.0 . scaleDownMultiplier Artificially increase/decrease the size of scale down by this factor. Default is 1.0 . scaleUpMultiplier Artificially increase/decrease the size of scale up by this factor. Default is 1.0 .","title":"Clutch RPS"},{"location":"operate/autoscalingstrategies/#clutch-experimental-developmental-use-only","text":"This strategy is internally used for testing new Clutch implementations. It should not be used for production jobs.","title":"Clutch Experimental (Developmental Use Only)"},{"location":"operate/tuning/","text":"Background Mantis Jobs consist of one or more independent stages . Since stages are independent of one another, they are also sized independently. Stages have one or more workers , which are the fundamental unit of parallelism for a stage. Workers execute individual instances of a stage and are each allocated CPU, memory, disk, and network resources. Workers are also isolated from one another, which means they process events independently of each other using their own dedicated resources. You can horizontally scale your Mantis Jobs by modifying the number of workers of your stages. You can also vertically scale your Mantis Jobs by tuning the number of resources for each worker. The following sections are guidelines for sizing different types of Mantis Jobs. Note You can modify worker resources any time, even after you have already launched the Mantis Job. This is useful in cases where you might want to adjust your Mantis Job to new traffic patterns. You can do this by marking your stage as Stage is Scalable . You can also mark the stage as AutoScale this stage to have Mantis automatically scale the stage. You can horizontally scale your Mantis Jobs without restarting the entire job. However, vertically scaling your jobs requires a new job submission for changes to take effect. Mantis Jobs In addition to scaling the number of workers of a Mantis Job, you should consider tuning your workers, which have two tunable properties: processing resources and processing parallelism. For processing resources , you should generally determine if CPU , memory , or network resources will be a bottleneck in your processing. You should increase: the number of CPUs if your job has CPU-intensive transformations such as serialization and deserialization memory if your job plans to hold many objects or large objects in memory network resources if your job needs high throughput for network I/O such as external API calls For processing parallelism , you must choose between serial or concurrent input processing for each stage. Serial input processing means your Processing Stage will receive and process events within a single thread. With serial input, you lose parallelism but your processing logic becomes straightforward without race conditions. // Specifying serial input for a stage. public class SerialStage implements ScalarComputation T1 , T2 { static public ScalarToScalar . Config T1 , T2 config () { return new ScalarToScalar . Config T1 , T2 (). serialInput (); } } Concurrent input means your Processing Stage will have multiple threads which each receive and process events. With concurrent input enabled, race conditions must be considered because the stage\u2019s threads operate independently from one another and may process events at different speeds. // Specifying concurrent input for a stage. public class SerialStage implements ScalarComputation T1 , T2 { static public ScalarToScalar . Config T1 , T2 config () { return new ScalarToScalar . Config T1 , T2 (). concurrentInput (); } } Note Workers use serial input by default. Kafka Source Jobs Kafka Source Jobs are Mantis Jobs that share the same properties described above, except Kafka Source Jobs use concurrent input processing by default. There are additional job parameters to consider when tuning Kafka Source Jobs: numConsumerInstances and stageConcurrency . The numConsumerInstances property determines how many Kafka consumers will be created for each worker. For example, if you have numConsumerInstances set to 2 and have 5 workers, then you will have 10 Kafka consumers in total for your Mantis Job consuming from a Kafka topic. The stageConcurrency property determines a pool of threads which receive events by the Kafka consumers. You can control your processing parallelism with this property. For example, if you have numConsumerInstances set to 2 and stageConcurrency set to 5 on a worker, then two Kafka consumers will read events from a Kafka topic and asynchronously send them to a pool of 5 processing threads. There are three considerations for using numConsumerInstances and stageConcurrency to tune your Kafka Source Job. First, you can pin numConsumerInstances to 1 and add more workers to load balance Kafka consumers across worker instances. If you find that your workers are under-utilized, you can increase numConsumerInstances for each worker. You can also give each worker more CPU, memory, and network resources and increase numConsumerInstances further so that you have fewer workers doing more work. Lastly, if you find that your Kafka topic\u2019s lag is increasing, then processing might be a bottleneck. In this case, you can increase stageConcurrency to increase processing throughput. Note Adding more workers to scale your Kafka Source Jobs to increase throughput or address lag has a hard upper limit. Ensure that you do not have more Kafka consumers than the number of partitions of the Kafka topic that your job is reading from. Specifically, ensure that: (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions This is because the number of partitions is a Kafka topic\u2019s upper bound for parallelism. If you exceed this number, then you will have idle consumers wasting resources which means adding more workers to your Kafka Source Job will not have any positive effect.","title":"Tuning"},{"location":"operate/tuning/#background","text":"Mantis Jobs consist of one or more independent stages . Since stages are independent of one another, they are also sized independently. Stages have one or more workers , which are the fundamental unit of parallelism for a stage. Workers execute individual instances of a stage and are each allocated CPU, memory, disk, and network resources. Workers are also isolated from one another, which means they process events independently of each other using their own dedicated resources. You can horizontally scale your Mantis Jobs by modifying the number of workers of your stages. You can also vertically scale your Mantis Jobs by tuning the number of resources for each worker. The following sections are guidelines for sizing different types of Mantis Jobs. Note You can modify worker resources any time, even after you have already launched the Mantis Job. This is useful in cases where you might want to adjust your Mantis Job to new traffic patterns. You can do this by marking your stage as Stage is Scalable . You can also mark the stage as AutoScale this stage to have Mantis automatically scale the stage. You can horizontally scale your Mantis Jobs without restarting the entire job. However, vertically scaling your jobs requires a new job submission for changes to take effect.","title":"Background"},{"location":"operate/tuning/#mantis-jobs","text":"In addition to scaling the number of workers of a Mantis Job, you should consider tuning your workers, which have two tunable properties: processing resources and processing parallelism. For processing resources , you should generally determine if CPU , memory , or network resources will be a bottleneck in your processing. You should increase: the number of CPUs if your job has CPU-intensive transformations such as serialization and deserialization memory if your job plans to hold many objects or large objects in memory network resources if your job needs high throughput for network I/O such as external API calls For processing parallelism , you must choose between serial or concurrent input processing for each stage. Serial input processing means your Processing Stage will receive and process events within a single thread. With serial input, you lose parallelism but your processing logic becomes straightforward without race conditions. // Specifying serial input for a stage. public class SerialStage implements ScalarComputation T1 , T2 { static public ScalarToScalar . Config T1 , T2 config () { return new ScalarToScalar . Config T1 , T2 (). serialInput (); } } Concurrent input means your Processing Stage will have multiple threads which each receive and process events. With concurrent input enabled, race conditions must be considered because the stage\u2019s threads operate independently from one another and may process events at different speeds. // Specifying concurrent input for a stage. public class SerialStage implements ScalarComputation T1 , T2 { static public ScalarToScalar . Config T1 , T2 config () { return new ScalarToScalar . Config T1 , T2 (). concurrentInput (); } } Note Workers use serial input by default.","title":"Mantis Jobs"},{"location":"operate/tuning/#kafka-source-jobs","text":"Kafka Source Jobs are Mantis Jobs that share the same properties described above, except Kafka Source Jobs use concurrent input processing by default. There are additional job parameters to consider when tuning Kafka Source Jobs: numConsumerInstances and stageConcurrency . The numConsumerInstances property determines how many Kafka consumers will be created for each worker. For example, if you have numConsumerInstances set to 2 and have 5 workers, then you will have 10 Kafka consumers in total for your Mantis Job consuming from a Kafka topic. The stageConcurrency property determines a pool of threads which receive events by the Kafka consumers. You can control your processing parallelism with this property. For example, if you have numConsumerInstances set to 2 and stageConcurrency set to 5 on a worker, then two Kafka consumers will read events from a Kafka topic and asynchronously send them to a pool of 5 processing threads. There are three considerations for using numConsumerInstances and stageConcurrency to tune your Kafka Source Job. First, you can pin numConsumerInstances to 1 and add more workers to load balance Kafka consumers across worker instances. If you find that your workers are under-utilized, you can increase numConsumerInstances for each worker. You can also give each worker more CPU, memory, and network resources and increase numConsumerInstances further so that you have fewer workers doing more work. Lastly, if you find that your Kafka topic\u2019s lag is increasing, then processing might be a bottleneck. In this case, you can increase stageConcurrency to increase processing throughput. Note Adding more workers to scale your Kafka Source Jobs to increase throughput or address lag has a hard upper limit. Ensure that you do not have more Kafka consumers than the number of partitions of the Kafka topic that your job is reading from. Specifically, ensure that: (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions This is because the number of partitions is a Kafka topic\u2019s upper bound for parallelism. If you exceed this number, then you will have idle consumers wasting resources which means adding more workers to your Kafka Source Job will not have any positive effect.","title":"Kafka Source Jobs"},{"location":"reference/api/","text":"In addition to the Mantis UI, there is a REST API with which you can maintain Mantis Jobs and Job Clusters . This page describes the open source version of the Mantis REST API. You can use the Mantis REST API to submit Jobs based on existing Job Cluster or to connect to the output of running Jobs. It is easier to set up or update new Job Clusters by using the Mantis UI, but you can also do this with the Mantis REST API. Response Content Type Mantis API endpoints always return JSON, and do not respect content-type headers in API requests. Summary of REST API Cluster APIs endpoint verb purpose /api/v1/jobClusters GET Return a list of Mantis clusters . /api/v1/jobClusters POST Create a new cluster. /api/v1/jobClusters/ clusterName GET Return information about a single cluster by name. /api/v1/jobClusters/ clusterName PUT Update the information about a particular cluster. /api/v1/jobClusters/ clusterName DELETE Permanently delete a paticular cluster. /api/v1/jobClusters/ clusterName /actions/updateArtifact POST Update the job cluster artifact and optionally resubmit the job . /api/v1/jobClusters/ clusterName /actions/updateSla POST Update cluster SLA information. /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy POST Update the cluster migration strategy . /api/v1/jobClusters/ clusterName /actions/updateLabel POST Update cluster labels . /api/v1/jobClusters/ clusterName /actions/enableCluster POST Enable a disabled cluster. /api/v1/jobClusters/ clusterName /actions/disableCluster POST Disable a cluster. /api/v1/mantis/publish/streamJobClusterMap GET Return a mapping of Mantis Publish push-based streams to clusters. Job APIs endpoint verb purpose /api/v1/jobs GET Return a list of jobs. /api/v1/jobClusters/ clusterName /jobs GET Return a list of jobs for a particular cluster. /api/v1/jobs/ jobID GET Return information about a particular job. /api/v1/jobClusters/ clusterName /jobs/ jobID GET Return information about a particular job. /api/v1/jobs/ jobID DELETE Permanently kill a particular job. /api/v1/jobClusters/ clusterName /jobs POST Submit a new job. /api/v1/jobs/actions/quickSubmit POST Update a job cluster and submit a new job at the same time. /api/v1/jobs/ jobID /actions/postJobStatus POST Post job heartbeat status. /api/v1/jobs/ jobID /actions/scaleStage POST Horizontally scale a stage. /api/v1/jobs/ jobID /actions/resubmitWorker POST Resubmit a worker . Admin APIs endpoint verb purpose /api/v1/masterInfo GET Return Job Master information. /api/v1/masterConfigs GET Return Job Master configs. /api/v1/agentClusters/ GET Get information about active agent clusters. /api/v1/agentClusters/ POST Activate or deactivate an agent cluster. /api/v1/agentClusters/jobs GET Get jobs and host information for an agent cluster. /api/v1/agentClusters/autoScalePolicy GET Retrieve the Agent Cluster Scaling Policy. Streaming WebSocket/SSE APIs endpoint verb purpose ws:// masterHost :7101/api/v1/jobStatusStream/ jobID n/a Stream Job Status Changes. /api/v1/jobDiscoveryStream/ jobID ( SSE ) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobs/schedulingInfo/ jobID (SSE) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobClusters/discoveryInfoStream/ clusterName (SSE) GET Return streaming (SSE) discovery info for the given job cluster. /api/v1/lastSubmittedJobIdStream/ clusterName (SSE) GET Return streaming (SSE) job information for a particular cluster. /api/v1/jobConnectbyid/ jobID (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobConnectbyname/ jobName (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobsubmitandconnect (SSE) POST Submit a job, collect messages from the job sinks, and merge them into a single websocket or SSE stream. Cluster Tasks TBD Get a List of Clusters /api/v1/jobClusters ( GET ) To retrieve a list of JSON objects that include details about the available Job Clusters, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. For example ?fields=name . offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. matching (optional) You can set this to a regular expression, and Mantis will filter the list of Job Clusters on the server side, and will return only those that match this expression. Example Response Format GET /api/v1/jobClusters?pageSize=5 fields=name sortBy=name ascending=false { list : [{ name : jschmoeSLATest }, { name : sinefn }, { name : Validation_ZV93BAWR2 }, { name : Validation_Z8NB06L1A }, { name : Validation_Y828QAI01 }], prev :null, next : /api/v1/jobClusters?pageSize=5 fields=name sortBy=name ascending=false offset=5 } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Create a New Cluster /api/v1/jobClusters ( POST ) Before you submit a Mantis Job , you must first have set up a Job Cluster . A Job Cluster contains a unique name for the Job, a URL of the Job\u2019s .jar or .zip artifact file, resource requirements to run your Job, and other optional information such as SLA values for minimum and maximum Jobs to keep active for this Cluster, or a cron-based schedule to launch a Job for this Cluster. Each new Job can be considered as an instance of the Job Cluster, and is given a unique ID by appending a number suffix to the Cluster name. The Job inherits the resource requirements from the Cluster unless whoever submits the Job overrides these at submit time. A Job Cluster name must match this regular expression: ^[A-Za-z]+[A-Za-z0-9+-_=:;]* To create a new Job Cluster, issue a POST command to the Mantis REST API endpoint /api/v1/jobClusters with a request body that matches format of the following example: Example Request Body { jobDefinition : { name : jschmoe_validation , user : jschmoe , jobJarFileLocation : https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , version : 0.2.9 2019-03-19 17:01:36 , schedulingInfo : { stages : { 1 : { numberOfInstances : 1 , machineDefinition : { cpuCores : 1 , memoryMB : 1024 , diskMB : 1024 , networkMbps : 128 , numPorts : 1 }, scalable : false, softConstraints : [], hardConstraints : [] } } }, parameters : [], labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : mantis-examples-sine-function }, { name : _mantis.artifact.version , value : 0.2.9 } ], migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, slaMin : 0 , slaMax : 0 , cronSpec : null, cronPolicy : KEEP_EXISTING }, owner : { contactEmail : jschmoe@netflix.com , description : , name : Joe Schmoe , repo : , teamName : } } Example Response Format { name : jschmoe_validation1 , jars : [ { url : https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , uploadedAt : 1553040262171, version : 0.2.9 2019-03-19 17:01:36 , schedulingInfo : { stages : { 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 1, memoryMB : 1024, networkMbps : 128, diskMB : 1024, numPorts : 1 }, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } } } } ], sla : { min : 0, max : 0, cronSpec : null, cronPolicy : null }, parameters : [], owner : { name : Joe Schmoe , teamName : , description : , contactEmail : jschmoe@netflix.com , repo : }, lastJobCount : 0, disabled : false, isReadyForJobMaster : false, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : mantis-examples-sine-function }, { name : _mantis.artifact.version , value : 0.2.9 } ], cronActive : false, latestVersion : 0.2.9 2019-03-19 17:01:36 } Possible Response Codes Response Code Reason 201 normal response 405 incorrect HTTP verb (use POST instead) 409 cluster name already exists 500 unknown server error Setting Jobs to Launch at Particular Times You can use the cronSpec field in the body of this request to specify when to launch the Jobs in the Cluster. By default this is blank ( \"\" ). If you set cronSpec to a non-blank value, this also sets the min and max values for the Job Cluster to 0 and 1 respectively. That is to say, you can have no more than one Job running at any one time for that Cluster. Optionally, you can provide a policy ( cronpolicy ) to use when a cron trigger fires while a previosuly submitted Job for the Job Cluster is still running. The possible policy values are KEEP_EXISTING (do not replace the current Job) and KEEP_NEW (replace the current Job with a new one). The default policy is KEEP_EXISTING . Note If the Mantis Master is down during a time window when cron would normally have fired, that cron trigger time window is lost. Mantis does not check for this upon restart. The next cron trigger will resume normally. Get Information about a Cluster /api/v1/jobClusters/ clusterName ( GET ) To retrieve a JSON object that includes details about a Job Cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. Example Response Format { name : jschmoe_validation1 , jars : [ { url : https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , uploadedAt : 1553040262171, version : 0.2.9 2019-03-19 17:01:36 , schedulingInfo : { stages : { 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 1, memoryMB : 1024, networkMbps : 128, diskMB : 1024, numPorts : 1 }, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } } } } ], sla : { min : 0, max : 0, cronSpec : null, cronPolicy : null }, parameters : [], owner : { name : Joe Schmoe , teamName : , description : , contactEmail : jschmoe@netflix.com , repo : }, lastJobCount : 0, disabled : false, isReadyForJobMaster : false, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : mantis-examples-sine-function }, { name : _mantis.artifact.version , value : 0.2.9 } ], cronActive : false, latestVersion : 0.2.9 2019-03-19 17:01:36 } Possible Response Codes Response Code Reason 200 normal response 404 no cluster with that cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Change Information about a Cluster /api/v1/jobClusters/ clusterName ( PUT ) To update an existing Job Cluster , send a PUT command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName with the same sort of body described above in the case of a Job Cluster create operation. Increment the version number so as to differentiate your new Cluster from the previous Job artifacts . If you try to update an existing Job Cluster by reusing the version number of an existing one, the operation will fail. Example Request Body { jobDefinition : { name : Validation_jschmoe , user : validator , jobJarFileLocation : https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , parameters : [ { name : useRandom , value : false } ], schedulingInfo : { stages : { 0 : { numberOfInstances : 1, machineDefinition : { cpuCores : 2, memoryMB : 4096, diskMB : 10, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : false }, 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 2, memoryMB : 4096, diskMB : 10, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : false } } }, slaMin : 0, slaMax : 0, cronSpec : null, cronPolicy : KEEP_EXISTING , migrationConfig : { configString : {\\ percentToMove\\ :60, \\ intervalMs\\ :30000} , strategy : PERCENTAGE } }, owner : { name : validator , teamName : Mantis , description : integration validator , contactEmail : mantisteam@netflix.com } } Example Response Format sine-function Job cluster updated Possible Response Codes Response Code Reason 200 normal response 400 client failure 404 no existing cluster with that cluster name was found 405 incorrect HTTP verb (use PUT instead) 500 unknown server error Delete a Cluster /api/v1/jobClusters/ clusterName ( DELETE ) To permanently delete an existing Job Cluster , send a DELETE command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose user (required) Must match the original user in the cluster payload. Example Response Format TBD Possible Response Codes Response Code Reason 202 normal response: asynchronous delete has been scheduled 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error Update a Cluster\u2019s Artifacts /api/v1/jobClusters/ clusterName /actions/updateArtifact ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. Mantis copies the scheduling information, Job parameters , and so forth, from the last Job submitted. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateArtifact with a body that matches the format of the following example: Example Request Body { name : ValidatorDemo , version : 0.0.1 2019-02-06 11:30:49 , url : mantis-artifacts/demo-0.0.1-dev201901231434.zip , skipsubmit : false, user : jschmoe } Example Response Format sine-function Job cluster updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error You will receive in the response the Job ID of the newly submitted Job (unless you set skipsubmit to true in the request body, in which case no such Job will be created). Update a Cluster\u2019s SLA /api/v1/jobClusters/ clusterName /actions/updateSla ( POST ) To update the SLA of a Job Cluster without having to submit a new version, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateSla with a body that matches the format of the following example: Example Request Body { user : YourUserName , name : Foo , min : 0, max : 2, cronspec : 5 * * * * ? , cronpolicy : KEEP_EXISTING , forceenable : true } The fields of this body are as follows: SLA Field Purpose user (required) name of the user calling this endpoint name (required) name of the Job Cluster min minimum number of Jobs to keep running max maximum number of Jobs to allow running simultaneously cronspec cron specification, see below for format and examples cronpolicy either KEEP_EXISTING or KEEP_NEW (see above for details) forceenable either true or false ; reenable the Job Cluster if it is in the disabled state Note While min , max , cronspec , cronpolicy , and forceenable are all optional fields, you should provide at minimum either cronspec or the combination of min max . The cron specification string is defined by Quartz CronTrigger . Here are some examples: Examples of cronspec Values example cronspec value resulting job trigger time \"0 0 12 * * ?\" Fire at 12 p.m. (noon) every day. \"0 15 10 ? * *\" Fire at 10:15 a.m. every day. \"0 15 10 * * ?\" Fire at 10:15 a.m. every day. \"0 0-5 14 * * ?\" Fire every minute starting at 2 p.m. and ending at 2:05 p.m., every day. Scheduling information for Jobs launched by means of cron triggers is inherited from the scheduling information for the Job Cluster. Warning If a Job takes required parameters , the Job will not launch successfully if the Job Cluster does not establish defaults for those parameters. A Job launched by means of a cron trigger always uses these default parameters to launch the Job. If you provide an invalid cron specification, this will disable the Job Cluster. To fix this, when you reformulate your cron specification, also set forceenable to \"true\" in the body that you send via POST to /api/v2/jobClusters/ clusterName /actions/updateSla . Example Response Format sine-function SLA updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Example: Creating a Job Cluster with a cron Specification The following POST body to /api/v2/jobClusters/ clusterName /actions/updateSla would specify Jobs that are launched based on a timed schedule: { jobDefinition : { name : Foo , user : YourUserName , version : 1.0 , parameters : [ { name : param1 , value : value1 }, { name : param2 , value : value2 } ], schedulingInfo : { stages : { 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 2, memoryMB : 4096, diskMB : 10 }, hardConstraints : null, softConstraints : null, scalable : false } } }, slaMin : 0, slaMax : 0, cronSpec : 2 * * * * ? , cronPolicy : KEEP_EXISTING , jobJarFileLocation : http://www.jobjars.com/foo }, owner : { name : MyName , teamName : myTeam , description : description , contactEmail : email@company.com , repo : http://repos.com/myproject.git } } Update a Cluster\u2019s Migration Strategy /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy ( POST ) You can quickly update the migration strategy of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy with a body that matches the format of the following example: Example Request Body { name : NameOfJobCluster , migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :10, \\ intervalMs\\ :1000} }, user : YourUserName } You will receive in the response the migration strategy config that you have updated the Job Cluster to. Example Response Format sine-function worker migration config updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Update a Cluster\u2019s Labels /api/v1/jobClusters/ clusterName /actions/updateLabel ( POST ) You can quickly update the labels of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateLabel with a body that matches the format of the following example: Example Request Body { name : SPaaSBackpressureDemp , labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : backpressure-demo-aggregator-0.0.1 }, { name : _mantis.artifact.version , value : dev201901231434 }, { name : _mantis.jobType , value : aggregator }, { name : _mantis.criticality , value : medium }, { name : myTestLabel , value : bingo } ], user : jschmoe } Example Response Format sine-function labels updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Enable a Cluster /api/v1/jobClusters/ clusterName /actions/enableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=true without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/enableCluster with a body that matches the format of the following example: Example Request Body { name : SPaaSBackpressureDemp , user : jschmoe } Example Response Format sine-function enabled Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Disable a Cluster /api/v1/jobClusters/ clusterName /actions/disableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=false without having to update the entirety of the Cluster definition. When you disable a Job Cluster Mantis will not allow new Job submissions under that Cluster and it will terminate any Jobs from that Cluster that are currently running. Mantis will also stop enforcing the SLA requirements for the Cluster, including any cron setup that would otherwise launch new Jobs. This is useful when a Job Cluster must be temporarily made inactive, for instance if you have determined that there is a problem with it. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/disableCluster with a body that matches the format of the following example: Example Request Body { name : SPaaSBackpressureDemp , user : jschmoe } Example Response Format sine-function disabled Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Get a Map of Mantis Publish Push-Based Streams to Clusters /api/v1/mantis/publish/streamJobClusterMap ( GET ) describe Example Response Format { version : 1 , timestamp : 2, mappings : { __default__ : { requestEventStream : SharedMantisPublishEventSource , __default__ : SharedMantisPublishEventSource } } } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Job Tasks TBD Get a List of Jobs /api/v1/jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs , issue a GET command to the Mantis REST API endpoint /api/v1/jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . matching (optional) The cluster name. Set this to a regex string. You can also use the /api/v1/jobClusters/ clusterName /jobs endpoint if you mean to match a specific cluster name and do not need the flexibility of a regex filter. stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) describe Example Response Format { list : [ { jobMetadata : { jobId : sine-test-4 , name : sine-test , user : someuser , submittedAt : 1574188324276, startedAt : 1574188354708, jarUrl : http://mantis-examples-sine-function-0.2.9.zip , numStages : 1, sla : { runtimeLimitSecs : 0, minRuntimeSecs : 0, slaType : Lossy , durationType : Perpetual , userProvidedType : }, state : Launched , subscriptionTimeoutSecs : 0, parameters : [ { name : useRandom , value : True } ], nextWorkerNumberToUse : 40, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.user , value : zxu }, { name : _mantis.ownerEmail , value : zxu@netflix.com }, { name : _mantis.artifact.version , value : 0.2.9 }, { name : _mantis.artifact , value : mantis-examples-sine-function-0.2.9.zip }, { name : _mantis.version , value : 0.2.9 2019-03-19 17:01:36 } ] }, stageMetadataList : [ { jobId : sine-test-4 , stageNum : 1, numStages : 1, machineDefinition : { cpuCores : 1.0, memoryMB : 1024.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } ], workerMetadataList : [ { workerIndex : 0, workerNumber : 31, jobId : sine-test-4 , stageNum : 1, numberOfPorts : 5, metricsPort : 7150, consolePort : 7152, debugPort : 7151, customPort : 7153, ports : [ 7154 ], state : Started , slave : 100.82.168.140 , slaveID : f39108b0-da43-45df-8b12-c132d85de7c0-S1 , cluster : mantisagent-staging-m5.2xlarge-1 , acceptedAt : 1575675900626, launchedAt : 1575675996506, startingAt : 1575676025493, startedAt : 1575676026661, completedAt : -1, reason : Normal , resubmitOf : 22, totalResubmitCount : 4 } ], version : null } ], prev : null, next : null, total : 1 } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Information About a Particular Job /api/v1/jobs/ jobID ( GET ) /api/v1/jobClusters/ clusterName /jobs/ jobID ( GET ) To retrieve a JSON record of a particular Job , issue a GET command to the Mantis REST API endpoint /api/v1/jobs/ jobID or /api/v1/jobClusters/ clusterName /jobs/ jobID . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. archived (optional) By default only information about an active job will be returned. Set this to true if you want information about the job returned even if it is an archived inactive job. describe Example Response Format { jobMetadata : { jobId : sine-function-7531 , name : sine-function , user : someuser , submittedAt : 1576002266997, startedAt : 0, jarUrl : http://mantis-examples-sine-function-0.2.9.zip , numStages : 2, sla : { runtimeLimitSecs : 0, minRuntimeSecs : 0, slaType : Lossy , durationType : Perpetual , userProvidedType : }, state : Accepted , subscriptionTimeoutSecs : 0, parameters : [ { name : useRandom , value : False } ], nextWorkerNumberToUse : 10, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.artifact , value : mantis-examples-sine-function-0.2.9.zip }, { name : _mantis.version , value : 0.2.9 2018-04-23 13:22:02 } ] }, stageMetadataList : [ { jobId : sine-function-7531 , stageNum : 0, numStages : 2, machineDefinition : { cpuCores : 2.0, memoryMB : 4096.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false }, { jobId : sine-function-7531 , stageNum : 1, numStages : 2, machineDefinition : { cpuCores : 1.0, memoryMB : 1024.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } ], workerMetadataList : [ { workerIndex : 0, workerNumber : 1, jobId : sine-function-7531 , stageNum : 0, numberOfPorts : 5, metricsPort : -1, consolePort : -1, debugPort : -1, customPort : -1, ports : [], state : Accepted , slave : null, slaveID : null, cluster : null, acceptedAt : 1576002267005, launchedAt : -1, startingAt : -1, startedAt : -1, completedAt : -1, reason : Normal , resubmitOf : 0, totalResubmitCount : 0 }, { workerIndex : 0, workerNumber : 2, jobId : sine-function-7531 , stageNum : 1, numberOfPorts : 5, metricsPort : -1, consolePort : -1, debugPort : -1, customPort : -1, ports : [], state : Accepted , slave : null, slaveID : null, cluster : null, acceptedAt : 1576002267007, launchedAt : -1, startingAt : -1, startedAt : -1, completedAt : -1, reason : Normal , resubmitOf : 0, totalResubmitCount : 0 } ], version : 0.2.9 2018-04-23 13:22:02 } Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Kill a Job /api/v1/jobs/ jobID ( DELETE ) To permanently kill a particular Job , issue a DELETE command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID . Query Parameters Query Parameter Purpose reason (required) Specify why you are killing this job. user (required) Specify which user is initiating this request. Example Response Format empty No response Possible Response Codes Response Code Reason 202 the kill request has been accepted and is being processed asynchronously 404 no job with that ID was found 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error List the Archived Workers for a Job /api/v1/jobs/ jobID /archivedWorkers ( GET ) To list all of the archived workers for a particular Job , issue a GET command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID /archivedWorkers . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. describe Example Response Format { list : [ { workerIndex : 0, workerNumber : 2, jobId : sine-function-7532 , stageNum : 1, numberOfPorts : 5, metricsPort : 7155, consolePort : 7157, debugPort : 7156, customPort : 7158, ports : [ 7159 ], state : Failed , slave : 100.85.130.224 , slaveID : 079f4fa6-f910-4247-b5d0-f5574f36cace-S5274 , cluster : mantisagent-main-m5.2xlarge-1 , acceptedAt : 1576003739758, launchedAt : 1576003739861, startingAt : 1576003748544, startedAt : 1576003750069, completedAt : 1576003959366, reason : Relaunched , resubmitOf : 0, totalResubmitCount : 0 } ], prev : null, next : null, total : 1 } Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get a List of Jobs for a Particular Cluster /api/v1/jobClusters/ clusterName /jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs in a particular cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) describe Example Response Format { list : [ { jobMetadata : { jobId : sine-function-7532 , name : sine-function , user : someuser , submittedAt : 1576003739750, startedAt : 1576003750076, jarUrl : http://mantis-examples-sine-function-0.2.9.zip , numStages : 2, sla : { runtimeLimitSecs : 0, minRuntimeSecs : 0, slaType : Lossy , durationType : Perpetual , userProvidedType : }, state : Launched , subscriptionTimeoutSecs : 0, parameters : [ { name : useRandom , value : False } ], nextWorkerNumberToUse : 10, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.isResubmit , value : true }, { name : _mantis.artifact , value : mantis-examples-sine-function-0.2.9.zip }, { name : _mantis.version , value : 0.2.9 2018-04-23 13:22:02 } ] }, stageMetadataList : [ { jobId : sine-function-7532 , stageNum : 0, numStages : 2, machineDefinition : { cpuCores : 2.0, memoryMB : 4096.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false }, { jobId : sine-function-7532 , stageNum : 1, numStages : 2, machineDefinition : { cpuCores : 1.0, memoryMB : 1024.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } ], workerMetadataList : [ { workerIndex : 0, workerNumber : 1, jobId : sine-function-7532 , stageNum : 0, numberOfPorts : 5, metricsPort : 7150, consolePort : 7152, debugPort : 7151, customPort : 7153, ports : [ 7154 ], state : Started , slave : 100.85.130.224 , slaveID : 079f4fa6-f910-4247-b5d0-f5574f36cace-S5274 , cluster : mantisagent-main-m5.2xlarge-1 , acceptedAt : 1576003739756, launchedAt : 1576003739861, startingAt : 1576003748558, startedAt : 1576003749967, completedAt : -1, reason : Normal , resubmitOf : 0, totalResubmitCount : 0 }, { workerIndex : 0, workerNumber : 3, jobId : sine-function-7532 , stageNum : 1, numberOfPorts : 5, metricsPort : 7165, consolePort : 7167, debugPort : 7166, customPort : 7168, ports : [ 7169 ], state : Started , slave : 100.85.130.224 , slaveID : 079f4fa6-f910-4247-b5d0-f5574f36cace-S5274 , cluster : mantisagent-main-m5.2xlarge-1 , acceptedAt : 1576003959366, launchedAt : 1576003959407, startingAt : 1576003961542, startedAt : 1576003962822, completedAt : -1, reason : Normal , resubmitOf : 2, totalResubmitCount : 1 } ], version : 0.2.9 2018-04-23 13:22:02 } ], prev : null, next : null, total : 1 } Possible Response Codes Response Code Reason 200 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Submit a New Job for a Particular Cluster /api/v1/jobClusters/ clusterName /jobs ( POST ) To submit a new Job based on a Job Cluster , issue a POST command to the /api/v1/jobClusters/ clusterName /jobs endpoint with a request body like the following: Example Request Body { name : myClusterName , user : jschmoe , jobJarFileLocation : null, version : 0.0.1 2019-02-06 13:30:07 , subscriptionTimeoutSecs : 0, jobSla : { runtimeLimitSecs : 0 , slaType : Lossy , durationType : Perpetual , userProvidedType : }, schedulingInfo : { stages : { 0 : { numberOfInstances : 1, machineDefinition : { cpuCores : 0.35, memoryMB : 600, networkMbps : 30, diskMB : 100, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : false }, 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 0.35, memoryMB : 600, networkMbps : 30, diskMB : 100, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : true } } }, parameters : [ { name : criterion , value : mock }, { name : sourceJobName , value : RequestSource }, { name : spaasJobId , value : spaasjschmoe-clsessionizer_backpressuretest } ], isReadyForJobMaster : false } Example Response Format (Same as \"Get Information about a Job\" below) (Same as \"Get Information about a Job\" above) Possible Response Codes Response Code Reason 201 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Update a Job Cluster and Submit a New Job at the Same Time /api/v1/jobs/actions/quickSubmit ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobs/actions/quickSubmit with a body that matches the format of the following example: Example Request Body { name : NameOfJobCluster , user : myusername , jobSla : { durationType : Perpetual , runtimeLimitSecs : 0 , minRuntimeSecs : 0 , userProvidedType : } } Example Response Format You will receive in the response the Job ID of the newly submitted Job. E.g sine-test-5 sample response body? Possible Response Codes Response Code Reason 201 normal response 404 no cluster was found with a cluster name matching the value of name from the request body 405 incorrect HTTP verb (use POST instead) 500 unknown server error Post Job Heartbeat Status /api/v1/jobs/actions/postJobStatus ( POST ) Query Parameters TBD Example Request Body { jobId : sine-function-1 , status : { jobId : sine-function-1 , stageNum : 1, workerIndex : 0, workerNumber : 2, type : HEARTBEAT , message : heartbeat , state : Noop , hostname : null, timestamp : 1525813363585, reason : Normal , payloads : [ { type : SubscriptionState , data : false }, { type : IncomingDataDrop , data : {\\ onNextCount\\ :0,\\ droppedCount\\ :0} } ] } } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 405 incorrect HTTP verb (use POST instead) 500 unknown server error Horizontally Scale a Stage /api/v1/jobs/ jobID /actions/scaleStage ( POST ) To manually scale a Job Processing Stage , that is, to alter the number of workers assigned to that stage, send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/scaleStage with a request body in the following format: Note You can only manually scale a Processing Stage if the Job was submitted with the scalable flag turned on. Example Request Body { JobId : ValidatorDemo-33 , StageNumber : 1, NumWorkers : 3 } NumWorkers here is the number of workers you want to be assigned to the stage after the scaling action takes place (that is, it is not the delta by which you want to change the number of workers in the stage). Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Resubmit a Worker /api/v1/jobs/ jobID /actions/resubmitWorker ( POST ) To resubmit a particular worker for a particular Job , send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/resubmitWorker with a request body resembling the following: Example Request Body { user : jschmoe , workerNumber : 5, reason : test worker resubmit } Note workerNumber is the worker number not the worker index . Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Administrative Tasks TBD Return Job Master Information /api/v1/masterInfo ( GET ) TBD Example Response Body { hostname : 100.86.121.198 , hostIP : 100.86.121.198 , apiPort : 7101, schedInfoPort : 7101, apiPortV2 : 7075, apiStatusUri : api/v1/jobs/actions/postJobStatus , consolePort : 7101, createTime : 1548803881867, fullApiStatusUri : http://100.86.121.198:7101/api/v1/jobs/actions/postJobStatus } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Return Job Master Configs /api/v1/masterConfigs ( GET ) TBD Example Response Body [ { name : JobConstraints , value : [\\ UniqueHost\\ ,\\ ExclusiveHost\\ ,\\ ZoneBalance\\ ,\\ M4Cluster\\ ,\\ M3Cluster\\ ,\\ M5Cluster\\ ] }, { name : ScalingReason , value : [\\ CPU\\ ,\\ Memory\\ ,\\ Network\\ ,\\ DataDrop\\ ,\\ KafkaLag\\ ,\\ UserDefined\\ ,\\ KafkaProcessed\\ ] }, { name : MigrationStrategyEnum , value : [\\ ONE_WORKER\\ ,\\ PERCENTAGE\\ ] }, { name : WorkerResourceLimits , value : {\\ maxCpuCores\\ :8,\\ maxMemoryMB\\ :28000,\\ maxNetworkMbps\\ :1024} } ] Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Information about Agent Clusters /api/v1/agentClusters/ ( GET ) Returns information about active agent clusters (agent clusters are physical AWS resources that Mantis connects to). Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Activate or Deactivate an Agent Cluster /api/v1/agentClusters/ ( POST ) TBD Example Request Body [ mantisagent-staging-cl1-m5.2xlarge-1-v00 ] (an array of active clusters) Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 400 client failure 405 incorrect HTTP verb (use POST instead) 500 unknown server error Retrieve the Agent Cluster Scaling Policy /api/v1/agentClusters/autoScalePolicy ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Jobs and Host Information for an Agent Cluster /api/v1/agentClusters/jobs ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Streaming WebSocket/SSE Tasks TBD: introduction Stream Job Status Changes ws:// masterHost :7101/api/v1/jobStatusStream/ jobID TBD: description See also: mantisapi/websocket/ Example Response Stream Excerpt { status : { jobId : SPaaSBackpressureDemp-26 , stageNum : 1, workerIndex : 0, workerNumber : 7, type : INFO , message : SPaaSBackpressureDemp-26-worker-0-7 worker status update , state : StartInitiated , hostname : null, timestamp : 1549404217065, reason : Normal , payloads : [] } } Stream Scheduling Information for a Job /api/v1/jobDiscoveryStream/ jobID ( GET ) /api/v1/jobs/schedulingInfo/ jobID ( GET ) To retrieve an SSE stream of scheduling information for a particular job, send an HTTP GET command to either the Mantis REST API endpoint /api/v1/jobDiscoveryStream/ jobID or /api/v1/jobs/schedulingInfo/ jobID . Query Parameters jobDiscoveryStream Query Parameter Purpose sendHB (optional) Indicate whether or not to send heartbeats (default= false ). jobs/schedulingInfo Query Parameter Purpose jobId (required) The job ID of the job for which scheduling information is to be streamed. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Streaming (SSE) Discovery Info for a Cluster /api/v1/jobClusters/discoveryInfoStream/ clusterName ( GET ) describe Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Stream Job Information for a Cluster /api/v1/lastSubmittedJobIdStream/ clusterName ( GET ) To retrieve an SSE stream of job information for a particular cluster, send an HTTP GET command to the Mantis REST API endpoint /api/v1/lastSubmittedJobIdStream/ clusterName . Query Parameters Query Parameter Purpose sendHB Indicate whether or not to send heartbeats. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Cluster with that Cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Stream Job Sink Messages /api/v1/jobConnectbyid/ jobID (SSE) /api/v1/jobConnectbyname/ jobName (SSE) Collect messages from the job sinks and merge them into a single websocket or SSE stream. Example Response Stream Excerpt data: { x : 928400.000000, y : 3.139934} data: { x : 928402.000000, y : -9.939772} data: { x : 928404.000000, y : 5.132876} data: { x : 928406.000000, y : 5.667712} Submit Job and Stream Job Sink Messages /api/v1/jobsubmitandconnect ( POST ) To submit a job and then collect messages from the job sinks and merge them into a single websocket or SSE stream, send a POST request to /api/v1/jobsubmitandconnect with a request body that describes the job. Example Request Body { name : ValidatorDemo , user : jschmoe , jobSla : { durationType : Perpetual , runtimeLimitSecs : 0 , minRuntimeSecs : 0 , userProvidedType : } } Example Response Stream Excerpt data: { x : 928400.000000, y : 3.139934} data: { x : 928402.000000, y : -9.939772} data: { x : 928404.000000, y : 5.132876} data: { x : 928406.000000, y : 5.667712} Pagination You can configure some endpoints to return their responses in pages . This is to say that rather than returning all of the data responsive to a request all at once, the endpoint can return a specific subset of the data at a time. An endpoint that supports pagination will typically do so by means of two query parameters: Query Parameter Purpose pageSize the maximum number of records to return in this request (default = 0, which means all records) offset the record number to begin with in the set of records to return in this request So, for example, you might make consecutive requests for\u2026 endpoint ?pageSize=10 offset=0 endpoint ?pageSize=10 offset=10 endpoint ?pageSize=10 offset=20 \u2026and so forth, until you reach the final page of records. When you request records in a paginated fashion like this, the Mantis API will enclose them in a JSON structure like the following: { \"list\": [ { \"Id\": originalOffset , \u22ee }, { \"Id\": originalOffset+1 , \u22ee }, \u22ee ], \"prev\": \"/api/v1/ endpoint ?pageSize= pageSize = originalOffset+pageSize \", \"next\": \"/api/v1/ endpoint ?pageSize= pageSize = originalOffset\u2212pageSize \" } For example: { list : [ { Id : 101 , \u22ee }, { Id : 102 , \u22ee } ], next : /api/v1/someResource?pageSize=20 offset=120 , prev : /api/v1/someResource?pageSize=20 offset=80 } prev and/or next will be null if there is no previous or next page, that is, if you are at the first and/or last page in the pagination. WebSocket Some desktops have trouble dealing with SSE . In such a case you can use WebSocket to connect to a Job (other API features are only available via the Mantis REST API). The WebSocket API runs from the same servers as the Mantis REST API, and you can reach it by using the ws:// protocol on port 7102 or the wss:// protocol on port 7103. Connecting to Job Output (Sink) Append this to the WebSocket URI to connect to a Job by name: /jobconnectbyname/ JobName Append this to the WebSocket URI to connect to a specific running Job ID: /jobconnectbyid/ JobID Upon connecting, the server starts writing messages that are coming in from the corresponding Jobs. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d Sink parameter which limits the rate at which the Sink output is sampled by the server. Submitting and Connecting to Job Output Append this to the WebSocket URI to submit and connect to the submitted Job: /jobsubmitandconnect Note that you will have to send one message on the WebSocket, which is the same JSON payload that you would send as a POST body for the equivalent REST API . The server then submits the Job and then starts writing messages into the WebSocket as it gets output from the Job. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d sink parameter which limits the rate at which the Sink output is sampled by the server. Example: Javascript to Connect a Job !DOCTYPE html meta charset = utf-8 / title WebSocket Test / title script language = javascript type = text/javascript var wsUri = ws://your-domain.com/jobconnectbyname/YourJobName?sampleMSec=2000 ; var output ; function init () { output = document . getElementById ( output ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( CONNECTED ); } function onClose ( evt ) { writeToScreen ( DISCONNECTED ); } function onMessage ( evt ) { writeToScreen ( span style= color: blue; RESPONSE: + evt . data + /span ); } function onError ( evt ) { writeToScreen ( span style= color: red; ERROR: /span + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( p ); pre . style . wordWrap = break-word ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( load , init , true ); / script h2 WebSocket Test / h2 div id = output / div Example: Javascript to Submit a Job and Connect to It !DOCTYPE html meta charset = utf-8 / title WebSocket Test / title script language = javascript type = text/javascript var wsUri = ws://your-domain.com/jobsubmitandconnect/ ; var output ; function init () { output = document . getElementById ( output ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( CONNECTED ); // Change this to your job s submit json content. // See job submit REST API above for another example. doSend ( {\\n + \\ name\\ :\\ Outliers-mock3\\ ,\\n + \\ version\\ :\\ \\ ,\\n + \\ parameters\\ :[],\\n + \\ jobSla\\ :{\\ runtimeLimitSecs\\ :0,\\ durationType\\ :\\ Transient\\ ,\\ userProvidedType\\ :\\ {\\\\\\ unique\\\\\\ :\\\\\\ foobar\\\\\\ }\\ },\\n + \\ subscriptionTimeoutSecs\\ :\\ 90\\ ,\\n + \\ jobJarFileLocation\\ :null,\\n + \\ schedulingInfo\\ :{\\ stages\\ :{\\ 1\\ :{\\ numberOfInstances\\ :1,\\ machineDefinition\\ :{\\ cpuCores\\ :1.0,\\ memoryMB\\ :2048.0,\\ diskMB\\ :1.0,\\ scalable\\ :\\ true\\ }}}\\n + } ); } function onClose ( evt ) { writeToScreen ( DISCONNECTED ); } function onMessage ( evt ) { writeToScreen ( span style= color: blue; RESPONSE: + evt . data + /span ); } function onError ( evt ) { writeToScreen ( span style= color: red; ERROR: /span + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( p ); pre . style . wordWrap = break-word ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( load , init , true ); / script h2 WebSocket Test / h2 div id = output / div","title":"API"},{"location":"reference/api/#summary-of-rest-api","text":"","title":"Summary of REST API"},{"location":"reference/api/#cluster-apis","text":"endpoint verb purpose /api/v1/jobClusters GET Return a list of Mantis clusters . /api/v1/jobClusters POST Create a new cluster. /api/v1/jobClusters/ clusterName GET Return information about a single cluster by name. /api/v1/jobClusters/ clusterName PUT Update the information about a particular cluster. /api/v1/jobClusters/ clusterName DELETE Permanently delete a paticular cluster. /api/v1/jobClusters/ clusterName /actions/updateArtifact POST Update the job cluster artifact and optionally resubmit the job . /api/v1/jobClusters/ clusterName /actions/updateSla POST Update cluster SLA information. /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy POST Update the cluster migration strategy . /api/v1/jobClusters/ clusterName /actions/updateLabel POST Update cluster labels . /api/v1/jobClusters/ clusterName /actions/enableCluster POST Enable a disabled cluster. /api/v1/jobClusters/ clusterName /actions/disableCluster POST Disable a cluster. /api/v1/mantis/publish/streamJobClusterMap GET Return a mapping of Mantis Publish push-based streams to clusters.","title":"Cluster APIs"},{"location":"reference/api/#job-apis","text":"endpoint verb purpose /api/v1/jobs GET Return a list of jobs. /api/v1/jobClusters/ clusterName /jobs GET Return a list of jobs for a particular cluster. /api/v1/jobs/ jobID GET Return information about a particular job. /api/v1/jobClusters/ clusterName /jobs/ jobID GET Return information about a particular job. /api/v1/jobs/ jobID DELETE Permanently kill a particular job. /api/v1/jobClusters/ clusterName /jobs POST Submit a new job. /api/v1/jobs/actions/quickSubmit POST Update a job cluster and submit a new job at the same time. /api/v1/jobs/ jobID /actions/postJobStatus POST Post job heartbeat status. /api/v1/jobs/ jobID /actions/scaleStage POST Horizontally scale a stage. /api/v1/jobs/ jobID /actions/resubmitWorker POST Resubmit a worker .","title":"Job APIs"},{"location":"reference/api/#admin-apis","text":"endpoint verb purpose /api/v1/masterInfo GET Return Job Master information. /api/v1/masterConfigs GET Return Job Master configs. /api/v1/agentClusters/ GET Get information about active agent clusters. /api/v1/agentClusters/ POST Activate or deactivate an agent cluster. /api/v1/agentClusters/jobs GET Get jobs and host information for an agent cluster. /api/v1/agentClusters/autoScalePolicy GET Retrieve the Agent Cluster Scaling Policy.","title":"Admin APIs"},{"location":"reference/api/#streaming-websocketsse-apis","text":"endpoint verb purpose ws:// masterHost :7101/api/v1/jobStatusStream/ jobID n/a Stream Job Status Changes. /api/v1/jobDiscoveryStream/ jobID ( SSE ) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobs/schedulingInfo/ jobID (SSE) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobClusters/discoveryInfoStream/ clusterName (SSE) GET Return streaming (SSE) discovery info for the given job cluster. /api/v1/lastSubmittedJobIdStream/ clusterName (SSE) GET Return streaming (SSE) job information for a particular cluster. /api/v1/jobConnectbyid/ jobID (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobConnectbyname/ jobName (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobsubmitandconnect (SSE) POST Submit a job, collect messages from the job sinks, and merge them into a single websocket or SSE stream.","title":"Streaming WebSocket/SSE APIs"},{"location":"reference/api/#cluster-tasks","text":"TBD","title":"Cluster Tasks"},{"location":"reference/api/#get-a-list-of-clusters","text":"/api/v1/jobClusters ( GET ) To retrieve a list of JSON objects that include details about the available Job Clusters, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. For example ?fields=name . offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. matching (optional) You can set this to a regular expression, and Mantis will filter the list of Job Clusters on the server side, and will return only those that match this expression. Example Response Format GET /api/v1/jobClusters?pageSize=5 fields=name sortBy=name ascending=false { list : [{ name : jschmoeSLATest }, { name : sinefn }, { name : Validation_ZV93BAWR2 }, { name : Validation_Z8NB06L1A }, { name : Validation_Y828QAI01 }], prev :null, next : /api/v1/jobClusters?pageSize=5 fields=name sortBy=name ascending=false offset=5 } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a List of Clusters"},{"location":"reference/api/#create-a-new-cluster","text":"/api/v1/jobClusters ( POST ) Before you submit a Mantis Job , you must first have set up a Job Cluster . A Job Cluster contains a unique name for the Job, a URL of the Job\u2019s .jar or .zip artifact file, resource requirements to run your Job, and other optional information such as SLA values for minimum and maximum Jobs to keep active for this Cluster, or a cron-based schedule to launch a Job for this Cluster. Each new Job can be considered as an instance of the Job Cluster, and is given a unique ID by appending a number suffix to the Cluster name. The Job inherits the resource requirements from the Cluster unless whoever submits the Job overrides these at submit time. A Job Cluster name must match this regular expression: ^[A-Za-z]+[A-Za-z0-9+-_=:;]* To create a new Job Cluster, issue a POST command to the Mantis REST API endpoint /api/v1/jobClusters with a request body that matches format of the following example: Example Request Body { jobDefinition : { name : jschmoe_validation , user : jschmoe , jobJarFileLocation : https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , version : 0.2.9 2019-03-19 17:01:36 , schedulingInfo : { stages : { 1 : { numberOfInstances : 1 , machineDefinition : { cpuCores : 1 , memoryMB : 1024 , diskMB : 1024 , networkMbps : 128 , numPorts : 1 }, scalable : false, softConstraints : [], hardConstraints : [] } } }, parameters : [], labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : mantis-examples-sine-function }, { name : _mantis.artifact.version , value : 0.2.9 } ], migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, slaMin : 0 , slaMax : 0 , cronSpec : null, cronPolicy : KEEP_EXISTING }, owner : { contactEmail : jschmoe@netflix.com , description : , name : Joe Schmoe , repo : , teamName : } } Example Response Format { name : jschmoe_validation1 , jars : [ { url : https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , uploadedAt : 1553040262171, version : 0.2.9 2019-03-19 17:01:36 , schedulingInfo : { stages : { 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 1, memoryMB : 1024, networkMbps : 128, diskMB : 1024, numPorts : 1 }, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } } } } ], sla : { min : 0, max : 0, cronSpec : null, cronPolicy : null }, parameters : [], owner : { name : Joe Schmoe , teamName : , description : , contactEmail : jschmoe@netflix.com , repo : }, lastJobCount : 0, disabled : false, isReadyForJobMaster : false, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : mantis-examples-sine-function }, { name : _mantis.artifact.version , value : 0.2.9 } ], cronActive : false, latestVersion : 0.2.9 2019-03-19 17:01:36 } Possible Response Codes Response Code Reason 201 normal response 405 incorrect HTTP verb (use POST instead) 409 cluster name already exists 500 unknown server error","title":"Create a New Cluster"},{"location":"reference/api/#setting-jobs-to-launch-at-particular-times","text":"You can use the cronSpec field in the body of this request to specify when to launch the Jobs in the Cluster. By default this is blank ( \"\" ). If you set cronSpec to a non-blank value, this also sets the min and max values for the Job Cluster to 0 and 1 respectively. That is to say, you can have no more than one Job running at any one time for that Cluster. Optionally, you can provide a policy ( cronpolicy ) to use when a cron trigger fires while a previosuly submitted Job for the Job Cluster is still running. The possible policy values are KEEP_EXISTING (do not replace the current Job) and KEEP_NEW (replace the current Job with a new one). The default policy is KEEP_EXISTING . Note If the Mantis Master is down during a time window when cron would normally have fired, that cron trigger time window is lost. Mantis does not check for this upon restart. The next cron trigger will resume normally.","title":"Setting Jobs to Launch at Particular Times"},{"location":"reference/api/#get-information-about-a-cluster","text":"/api/v1/jobClusters/ clusterName ( GET ) To retrieve a JSON object that includes details about a Job Cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. Example Response Format { name : jschmoe_validation1 , jars : [ { url : https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , uploadedAt : 1553040262171, version : 0.2.9 2019-03-19 17:01:36 , schedulingInfo : { stages : { 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 1, memoryMB : 1024, networkMbps : 128, diskMB : 1024, numPorts : 1 }, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } } } } ], sla : { min : 0, max : 0, cronSpec : null, cronPolicy : null }, parameters : [], owner : { name : Joe Schmoe , teamName : , description : , contactEmail : jschmoe@netflix.com , repo : }, lastJobCount : 0, disabled : false, isReadyForJobMaster : false, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : mantis-examples-sine-function }, { name : _mantis.artifact.version , value : 0.2.9 } ], cronActive : false, latestVersion : 0.2.9 2019-03-19 17:01:36 } Possible Response Codes Response Code Reason 200 normal response 404 no cluster with that cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Information about a Cluster"},{"location":"reference/api/#change-information-about-a-cluster","text":"/api/v1/jobClusters/ clusterName ( PUT ) To update an existing Job Cluster , send a PUT command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName with the same sort of body described above in the case of a Job Cluster create operation. Increment the version number so as to differentiate your new Cluster from the previous Job artifacts . If you try to update an existing Job Cluster by reusing the version number of an existing one, the operation will fail. Example Request Body { jobDefinition : { name : Validation_jschmoe , user : validator , jobJarFileLocation : https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip , parameters : [ { name : useRandom , value : false } ], schedulingInfo : { stages : { 0 : { numberOfInstances : 1, machineDefinition : { cpuCores : 2, memoryMB : 4096, diskMB : 10, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : false }, 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 2, memoryMB : 4096, diskMB : 10, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : false } } }, slaMin : 0, slaMax : 0, cronSpec : null, cronPolicy : KEEP_EXISTING , migrationConfig : { configString : {\\ percentToMove\\ :60, \\ intervalMs\\ :30000} , strategy : PERCENTAGE } }, owner : { name : validator , teamName : Mantis , description : integration validator , contactEmail : mantisteam@netflix.com } } Example Response Format sine-function Job cluster updated Possible Response Codes Response Code Reason 200 normal response 400 client failure 404 no existing cluster with that cluster name was found 405 incorrect HTTP verb (use PUT instead) 500 unknown server error","title":"Change Information about a Cluster"},{"location":"reference/api/#delete-a-cluster","text":"/api/v1/jobClusters/ clusterName ( DELETE ) To permanently delete an existing Job Cluster , send a DELETE command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose user (required) Must match the original user in the cluster payload. Example Response Format TBD Possible Response Codes Response Code Reason 202 normal response: asynchronous delete has been scheduled 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error","title":"Delete a Cluster"},{"location":"reference/api/#update-a-clusters-artifacts","text":"/api/v1/jobClusters/ clusterName /actions/updateArtifact ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. Mantis copies the scheduling information, Job parameters , and so forth, from the last Job submitted. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateArtifact with a body that matches the format of the following example: Example Request Body { name : ValidatorDemo , version : 0.0.1 2019-02-06 11:30:49 , url : mantis-artifacts/demo-0.0.1-dev201901231434.zip , skipsubmit : false, user : jschmoe } Example Response Format sine-function Job cluster updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error You will receive in the response the Job ID of the newly submitted Job (unless you set skipsubmit to true in the request body, in which case no such Job will be created).","title":"Update a Cluster\u2019s Artifacts"},{"location":"reference/api/#update-a-clusters-sla","text":"/api/v1/jobClusters/ clusterName /actions/updateSla ( POST ) To update the SLA of a Job Cluster without having to submit a new version, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateSla with a body that matches the format of the following example: Example Request Body { user : YourUserName , name : Foo , min : 0, max : 2, cronspec : 5 * * * * ? , cronpolicy : KEEP_EXISTING , forceenable : true } The fields of this body are as follows: SLA Field Purpose user (required) name of the user calling this endpoint name (required) name of the Job Cluster min minimum number of Jobs to keep running max maximum number of Jobs to allow running simultaneously cronspec cron specification, see below for format and examples cronpolicy either KEEP_EXISTING or KEEP_NEW (see above for details) forceenable either true or false ; reenable the Job Cluster if it is in the disabled state Note While min , max , cronspec , cronpolicy , and forceenable are all optional fields, you should provide at minimum either cronspec or the combination of min max . The cron specification string is defined by Quartz CronTrigger . Here are some examples: Examples of cronspec Values example cronspec value resulting job trigger time \"0 0 12 * * ?\" Fire at 12 p.m. (noon) every day. \"0 15 10 ? * *\" Fire at 10:15 a.m. every day. \"0 15 10 * * ?\" Fire at 10:15 a.m. every day. \"0 0-5 14 * * ?\" Fire every minute starting at 2 p.m. and ending at 2:05 p.m., every day. Scheduling information for Jobs launched by means of cron triggers is inherited from the scheduling information for the Job Cluster. Warning If a Job takes required parameters , the Job will not launch successfully if the Job Cluster does not establish defaults for those parameters. A Job launched by means of a cron trigger always uses these default parameters to launch the Job. If you provide an invalid cron specification, this will disable the Job Cluster. To fix this, when you reformulate your cron specification, also set forceenable to \"true\" in the body that you send via POST to /api/v2/jobClusters/ clusterName /actions/updateSla . Example Response Format sine-function SLA updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Example: Creating a Job Cluster with a cron Specification The following POST body to /api/v2/jobClusters/ clusterName /actions/updateSla would specify Jobs that are launched based on a timed schedule: { jobDefinition : { name : Foo , user : YourUserName , version : 1.0 , parameters : [ { name : param1 , value : value1 }, { name : param2 , value : value2 } ], schedulingInfo : { stages : { 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 2, memoryMB : 4096, diskMB : 10 }, hardConstraints : null, softConstraints : null, scalable : false } } }, slaMin : 0, slaMax : 0, cronSpec : 2 * * * * ? , cronPolicy : KEEP_EXISTING , jobJarFileLocation : http://www.jobjars.com/foo }, owner : { name : MyName , teamName : myTeam , description : description , contactEmail : email@company.com , repo : http://repos.com/myproject.git } }","title":"Update a Cluster\u2019s SLA"},{"location":"reference/api/#update-a-clusters-migration-strategy","text":"/api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy ( POST ) You can quickly update the migration strategy of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy with a body that matches the format of the following example: Example Request Body { name : NameOfJobCluster , migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :10, \\ intervalMs\\ :1000} }, user : YourUserName } You will receive in the response the migration strategy config that you have updated the Job Cluster to. Example Response Format sine-function worker migration config updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Update a Cluster\u2019s Migration Strategy"},{"location":"reference/api/#update-a-clusters-labels","text":"/api/v1/jobClusters/ clusterName /actions/updateLabel ( POST ) You can quickly update the labels of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateLabel with a body that matches the format of the following example: Example Request Body { name : SPaaSBackpressureDemp , labels : [ { name : _mantis.user , value : jschmoe }, { name : _mantis.ownerEmail , value : jschmoe@netflix.com }, { name : _mantis.artifact , value : backpressure-demo-aggregator-0.0.1 }, { name : _mantis.artifact.version , value : dev201901231434 }, { name : _mantis.jobType , value : aggregator }, { name : _mantis.criticality , value : medium }, { name : myTestLabel , value : bingo } ], user : jschmoe } Example Response Format sine-function labels updated Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Update a Cluster\u2019s Labels"},{"location":"reference/api/#enable-a-cluster","text":"/api/v1/jobClusters/ clusterName /actions/enableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=true without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/enableCluster with a body that matches the format of the following example: Example Request Body { name : SPaaSBackpressureDemp , user : jschmoe } Example Response Format sine-function enabled Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Enable a Cluster"},{"location":"reference/api/#disable-a-cluster","text":"/api/v1/jobClusters/ clusterName /actions/disableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=false without having to update the entirety of the Cluster definition. When you disable a Job Cluster Mantis will not allow new Job submissions under that Cluster and it will terminate any Jobs from that Cluster that are currently running. Mantis will also stop enforcing the SLA requirements for the Cluster, including any cron setup that would otherwise launch new Jobs. This is useful when a Job Cluster must be temporarily made inactive, for instance if you have determined that there is a problem with it. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/disableCluster with a body that matches the format of the following example: Example Request Body { name : SPaaSBackpressureDemp , user : jschmoe } Example Response Format sine-function disabled Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Disable a Cluster"},{"location":"reference/api/#get-a-map-of-mantis-publish-push-based-streams-to-clusters","text":"/api/v1/mantis/publish/streamJobClusterMap ( GET ) describe Example Response Format { version : 1 , timestamp : 2, mappings : { __default__ : { requestEventStream : SharedMantisPublishEventSource , __default__ : SharedMantisPublishEventSource } } } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a Map of Mantis Publish Push-Based Streams to Clusters"},{"location":"reference/api/#job-tasks","text":"TBD","title":"Job Tasks"},{"location":"reference/api/#get-a-list-of-jobs","text":"/api/v1/jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs , issue a GET command to the Mantis REST API endpoint /api/v1/jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . matching (optional) The cluster name. Set this to a regex string. You can also use the /api/v1/jobClusters/ clusterName /jobs endpoint if you mean to match a specific cluster name and do not need the flexibility of a regex filter. stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) describe Example Response Format { list : [ { jobMetadata : { jobId : sine-test-4 , name : sine-test , user : someuser , submittedAt : 1574188324276, startedAt : 1574188354708, jarUrl : http://mantis-examples-sine-function-0.2.9.zip , numStages : 1, sla : { runtimeLimitSecs : 0, minRuntimeSecs : 0, slaType : Lossy , durationType : Perpetual , userProvidedType : }, state : Launched , subscriptionTimeoutSecs : 0, parameters : [ { name : useRandom , value : True } ], nextWorkerNumberToUse : 40, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.user , value : zxu }, { name : _mantis.ownerEmail , value : zxu@netflix.com }, { name : _mantis.artifact.version , value : 0.2.9 }, { name : _mantis.artifact , value : mantis-examples-sine-function-0.2.9.zip }, { name : _mantis.version , value : 0.2.9 2019-03-19 17:01:36 } ] }, stageMetadataList : [ { jobId : sine-test-4 , stageNum : 1, numStages : 1, machineDefinition : { cpuCores : 1.0, memoryMB : 1024.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } ], workerMetadataList : [ { workerIndex : 0, workerNumber : 31, jobId : sine-test-4 , stageNum : 1, numberOfPorts : 5, metricsPort : 7150, consolePort : 7152, debugPort : 7151, customPort : 7153, ports : [ 7154 ], state : Started , slave : 100.82.168.140 , slaveID : f39108b0-da43-45df-8b12-c132d85de7c0-S1 , cluster : mantisagent-staging-m5.2xlarge-1 , acceptedAt : 1575675900626, launchedAt : 1575675996506, startingAt : 1575676025493, startedAt : 1575676026661, completedAt : -1, reason : Normal , resubmitOf : 22, totalResubmitCount : 4 } ], version : null } ], prev : null, next : null, total : 1 } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a List of Jobs"},{"location":"reference/api/#get-information-about-a-particular-job","text":"/api/v1/jobs/ jobID ( GET ) /api/v1/jobClusters/ clusterName /jobs/ jobID ( GET ) To retrieve a JSON record of a particular Job , issue a GET command to the Mantis REST API endpoint /api/v1/jobs/ jobID or /api/v1/jobClusters/ clusterName /jobs/ jobID . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. archived (optional) By default only information about an active job will be returned. Set this to true if you want information about the job returned even if it is an archived inactive job. describe Example Response Format { jobMetadata : { jobId : sine-function-7531 , name : sine-function , user : someuser , submittedAt : 1576002266997, startedAt : 0, jarUrl : http://mantis-examples-sine-function-0.2.9.zip , numStages : 2, sla : { runtimeLimitSecs : 0, minRuntimeSecs : 0, slaType : Lossy , durationType : Perpetual , userProvidedType : }, state : Accepted , subscriptionTimeoutSecs : 0, parameters : [ { name : useRandom , value : False } ], nextWorkerNumberToUse : 10, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.artifact , value : mantis-examples-sine-function-0.2.9.zip }, { name : _mantis.version , value : 0.2.9 2018-04-23 13:22:02 } ] }, stageMetadataList : [ { jobId : sine-function-7531 , stageNum : 0, numStages : 2, machineDefinition : { cpuCores : 2.0, memoryMB : 4096.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false }, { jobId : sine-function-7531 , stageNum : 1, numStages : 2, machineDefinition : { cpuCores : 1.0, memoryMB : 1024.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } ], workerMetadataList : [ { workerIndex : 0, workerNumber : 1, jobId : sine-function-7531 , stageNum : 0, numberOfPorts : 5, metricsPort : -1, consolePort : -1, debugPort : -1, customPort : -1, ports : [], state : Accepted , slave : null, slaveID : null, cluster : null, acceptedAt : 1576002267005, launchedAt : -1, startingAt : -1, startedAt : -1, completedAt : -1, reason : Normal , resubmitOf : 0, totalResubmitCount : 0 }, { workerIndex : 0, workerNumber : 2, jobId : sine-function-7531 , stageNum : 1, numberOfPorts : 5, metricsPort : -1, consolePort : -1, debugPort : -1, customPort : -1, ports : [], state : Accepted , slave : null, slaveID : null, cluster : null, acceptedAt : 1576002267007, launchedAt : -1, startingAt : -1, startedAt : -1, completedAt : -1, reason : Normal , resubmitOf : 0, totalResubmitCount : 0 } ], version : 0.2.9 2018-04-23 13:22:02 } Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Information About a Particular Job"},{"location":"reference/api/#kill-a-job","text":"/api/v1/jobs/ jobID ( DELETE ) To permanently kill a particular Job , issue a DELETE command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID . Query Parameters Query Parameter Purpose reason (required) Specify why you are killing this job. user (required) Specify which user is initiating this request. Example Response Format empty No response Possible Response Codes Response Code Reason 202 the kill request has been accepted and is being processed asynchronously 404 no job with that ID was found 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error","title":"Kill a Job"},{"location":"reference/api/#list-the-archived-workers-for-a-job","text":"/api/v1/jobs/ jobID /archivedWorkers ( GET ) To list all of the archived workers for a particular Job , issue a GET command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID /archivedWorkers . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. describe Example Response Format { list : [ { workerIndex : 0, workerNumber : 2, jobId : sine-function-7532 , stageNum : 1, numberOfPorts : 5, metricsPort : 7155, consolePort : 7157, debugPort : 7156, customPort : 7158, ports : [ 7159 ], state : Failed , slave : 100.85.130.224 , slaveID : 079f4fa6-f910-4247-b5d0-f5574f36cace-S5274 , cluster : mantisagent-main-m5.2xlarge-1 , acceptedAt : 1576003739758, launchedAt : 1576003739861, startingAt : 1576003748544, startedAt : 1576003750069, completedAt : 1576003959366, reason : Relaunched , resubmitOf : 0, totalResubmitCount : 0 } ], prev : null, next : null, total : 1 } Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"List the Archived Workers for a Job"},{"location":"reference/api/#get-a-list-of-jobs-for-a-particular-cluster","text":"/api/v1/jobClusters/ clusterName /jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs in a particular cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) describe Example Response Format { list : [ { jobMetadata : { jobId : sine-function-7532 , name : sine-function , user : someuser , submittedAt : 1576003739750, startedAt : 1576003750076, jarUrl : http://mantis-examples-sine-function-0.2.9.zip , numStages : 2, sla : { runtimeLimitSecs : 0, minRuntimeSecs : 0, slaType : Lossy , durationType : Perpetual , userProvidedType : }, state : Launched , subscriptionTimeoutSecs : 0, parameters : [ { name : useRandom , value : False } ], nextWorkerNumberToUse : 10, migrationConfig : { strategy : PERCENTAGE , configString : {\\ percentToMove\\ :25,\\ intervalMs\\ :60000} }, labels : [ { name : _mantis.isResubmit , value : true }, { name : _mantis.artifact , value : mantis-examples-sine-function-0.2.9.zip }, { name : _mantis.version , value : 0.2.9 2018-04-23 13:22:02 } ] }, stageMetadataList : [ { jobId : sine-function-7532 , stageNum : 0, numStages : 2, machineDefinition : { cpuCores : 2.0, memoryMB : 4096.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false }, { jobId : sine-function-7532 , stageNum : 1, numStages : 2, machineDefinition : { cpuCores : 1.0, memoryMB : 1024.0, networkMbps : 128.0, diskMB : 1024.0, numPorts : 1 }, numWorkers : 1, hardConstraints : [], softConstraints : [], scalingPolicy : null, scalable : false } ], workerMetadataList : [ { workerIndex : 0, workerNumber : 1, jobId : sine-function-7532 , stageNum : 0, numberOfPorts : 5, metricsPort : 7150, consolePort : 7152, debugPort : 7151, customPort : 7153, ports : [ 7154 ], state : Started , slave : 100.85.130.224 , slaveID : 079f4fa6-f910-4247-b5d0-f5574f36cace-S5274 , cluster : mantisagent-main-m5.2xlarge-1 , acceptedAt : 1576003739756, launchedAt : 1576003739861, startingAt : 1576003748558, startedAt : 1576003749967, completedAt : -1, reason : Normal , resubmitOf : 0, totalResubmitCount : 0 }, { workerIndex : 0, workerNumber : 3, jobId : sine-function-7532 , stageNum : 1, numberOfPorts : 5, metricsPort : 7165, consolePort : 7167, debugPort : 7166, customPort : 7168, ports : [ 7169 ], state : Started , slave : 100.85.130.224 , slaveID : 079f4fa6-f910-4247-b5d0-f5574f36cace-S5274 , cluster : mantisagent-main-m5.2xlarge-1 , acceptedAt : 1576003959366, launchedAt : 1576003959407, startingAt : 1576003961542, startedAt : 1576003962822, completedAt : -1, reason : Normal , resubmitOf : 2, totalResubmitCount : 1 } ], version : 0.2.9 2018-04-23 13:22:02 } ], prev : null, next : null, total : 1 } Possible Response Codes Response Code Reason 200 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a List of Jobs for a Particular Cluster"},{"location":"reference/api/#submit-a-new-job-for-a-particular-cluster","text":"/api/v1/jobClusters/ clusterName /jobs ( POST ) To submit a new Job based on a Job Cluster , issue a POST command to the /api/v1/jobClusters/ clusterName /jobs endpoint with a request body like the following: Example Request Body { name : myClusterName , user : jschmoe , jobJarFileLocation : null, version : 0.0.1 2019-02-06 13:30:07 , subscriptionTimeoutSecs : 0, jobSla : { runtimeLimitSecs : 0 , slaType : Lossy , durationType : Perpetual , userProvidedType : }, schedulingInfo : { stages : { 0 : { numberOfInstances : 1, machineDefinition : { cpuCores : 0.35, memoryMB : 600, networkMbps : 30, diskMB : 100, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : false }, 1 : { numberOfInstances : 1, machineDefinition : { cpuCores : 0.35, memoryMB : 600, networkMbps : 30, diskMB : 100, numPorts : 1 }, hardConstraints : null, softConstraints : null, scalable : true } } }, parameters : [ { name : criterion , value : mock }, { name : sourceJobName , value : RequestSource }, { name : spaasJobId , value : spaasjschmoe-clsessionizer_backpressuretest } ], isReadyForJobMaster : false } Example Response Format (Same as \"Get Information about a Job\" below) (Same as \"Get Information about a Job\" above) Possible Response Codes Response Code Reason 201 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Submit a New Job for a Particular Cluster"},{"location":"reference/api/#update-a-job-cluster-and-submit-a-new-job-at-the-same-time","text":"/api/v1/jobs/actions/quickSubmit ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobs/actions/quickSubmit with a body that matches the format of the following example: Example Request Body { name : NameOfJobCluster , user : myusername , jobSla : { durationType : Perpetual , runtimeLimitSecs : 0 , minRuntimeSecs : 0 , userProvidedType : } } Example Response Format You will receive in the response the Job ID of the newly submitted Job. E.g sine-test-5 sample response body? Possible Response Codes Response Code Reason 201 normal response 404 no cluster was found with a cluster name matching the value of name from the request body 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Update a Job Cluster and Submit a New Job at the Same Time"},{"location":"reference/api/#post-job-heartbeat-status","text":"/api/v1/jobs/actions/postJobStatus ( POST ) Query Parameters TBD Example Request Body { jobId : sine-function-1 , status : { jobId : sine-function-1 , stageNum : 1, workerIndex : 0, workerNumber : 2, type : HEARTBEAT , message : heartbeat , state : Noop , hostname : null, timestamp : 1525813363585, reason : Normal , payloads : [ { type : SubscriptionState , data : false }, { type : IncomingDataDrop , data : {\\ onNextCount\\ :0,\\ droppedCount\\ :0} } ] } } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Post Job Heartbeat Status"},{"location":"reference/api/#horizontally-scale-a-stage","text":"/api/v1/jobs/ jobID /actions/scaleStage ( POST ) To manually scale a Job Processing Stage , that is, to alter the number of workers assigned to that stage, send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/scaleStage with a request body in the following format: Note You can only manually scale a Processing Stage if the Job was submitted with the scalable flag turned on. Example Request Body { JobId : ValidatorDemo-33 , StageNumber : 1, NumWorkers : 3 } NumWorkers here is the number of workers you want to be assigned to the stage after the scaling action takes place (that is, it is not the delta by which you want to change the number of workers in the stage). Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Horizontally Scale a Stage"},{"location":"reference/api/#resubmit-a-worker","text":"/api/v1/jobs/ jobID /actions/resubmitWorker ( POST ) To resubmit a particular worker for a particular Job , send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/resubmitWorker with a request body resembling the following: Example Request Body { user : jschmoe , workerNumber : 5, reason : test worker resubmit } Note workerNumber is the worker number not the worker index . Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Resubmit a Worker"},{"location":"reference/api/#administrative-tasks","text":"TBD","title":"Administrative Tasks"},{"location":"reference/api/#return-job-master-information","text":"/api/v1/masterInfo ( GET ) TBD Example Response Body { hostname : 100.86.121.198 , hostIP : 100.86.121.198 , apiPort : 7101, schedInfoPort : 7101, apiPortV2 : 7075, apiStatusUri : api/v1/jobs/actions/postJobStatus , consolePort : 7101, createTime : 1548803881867, fullApiStatusUri : http://100.86.121.198:7101/api/v1/jobs/actions/postJobStatus } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Return Job Master Information"},{"location":"reference/api/#return-job-master-configs","text":"/api/v1/masterConfigs ( GET ) TBD Example Response Body [ { name : JobConstraints , value : [\\ UniqueHost\\ ,\\ ExclusiveHost\\ ,\\ ZoneBalance\\ ,\\ M4Cluster\\ ,\\ M3Cluster\\ ,\\ M5Cluster\\ ] }, { name : ScalingReason , value : [\\ CPU\\ ,\\ Memory\\ ,\\ Network\\ ,\\ DataDrop\\ ,\\ KafkaLag\\ ,\\ UserDefined\\ ,\\ KafkaProcessed\\ ] }, { name : MigrationStrategyEnum , value : [\\ ONE_WORKER\\ ,\\ PERCENTAGE\\ ] }, { name : WorkerResourceLimits , value : {\\ maxCpuCores\\ :8,\\ maxMemoryMB\\ :28000,\\ maxNetworkMbps\\ :1024} } ] Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Return Job Master Configs"},{"location":"reference/api/#get-information-about-agent-clusters","text":"/api/v1/agentClusters/ ( GET ) Returns information about active agent clusters (agent clusters are physical AWS resources that Mantis connects to). Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Information about Agent Clusters"},{"location":"reference/api/#activate-or-deactivate-an-agent-cluster","text":"/api/v1/agentClusters/ ( POST ) TBD Example Request Body [ mantisagent-staging-cl1-m5.2xlarge-1-v00 ] (an array of active clusters) Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 400 client failure 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Activate or Deactivate an Agent Cluster"},{"location":"reference/api/#retrieve-the-agent-cluster-scaling-policy","text":"/api/v1/agentClusters/autoScalePolicy ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Retrieve the Agent Cluster Scaling Policy"},{"location":"reference/api/#get-jobs-and-host-information-for-an-agent-cluster","text":"/api/v1/agentClusters/jobs ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Jobs and Host Information for an Agent Cluster"},{"location":"reference/api/#streaming-websocketsse-tasks","text":"TBD: introduction","title":"Streaming WebSocket/SSE Tasks"},{"location":"reference/api/#stream-job-status-changes","text":"ws:// masterHost :7101/api/v1/jobStatusStream/ jobID TBD: description See also: mantisapi/websocket/ Example Response Stream Excerpt { status : { jobId : SPaaSBackpressureDemp-26 , stageNum : 1, workerIndex : 0, workerNumber : 7, type : INFO , message : SPaaSBackpressureDemp-26-worker-0-7 worker status update , state : StartInitiated , hostname : null, timestamp : 1549404217065, reason : Normal , payloads : [] } }","title":"Stream Job Status Changes"},{"location":"reference/api/#stream-scheduling-information-for-a-job","text":"/api/v1/jobDiscoveryStream/ jobID ( GET ) /api/v1/jobs/schedulingInfo/ jobID ( GET ) To retrieve an SSE stream of scheduling information for a particular job, send an HTTP GET command to either the Mantis REST API endpoint /api/v1/jobDiscoveryStream/ jobID or /api/v1/jobs/schedulingInfo/ jobID . Query Parameters jobDiscoveryStream Query Parameter Purpose sendHB (optional) Indicate whether or not to send heartbeats (default= false ). jobs/schedulingInfo Query Parameter Purpose jobId (required) The job ID of the job for which scheduling information is to be streamed. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Stream Scheduling Information for a Job"},{"location":"reference/api/#get-streaming-sse-discovery-info-for-a-cluster","text":"/api/v1/jobClusters/discoveryInfoStream/ clusterName ( GET ) describe Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Streaming (SSE) Discovery Info for a Cluster"},{"location":"reference/api/#stream-job-information-for-a-cluster","text":"/api/v1/lastSubmittedJobIdStream/ clusterName ( GET ) To retrieve an SSE stream of job information for a particular cluster, send an HTTP GET command to the Mantis REST API endpoint /api/v1/lastSubmittedJobIdStream/ clusterName . Query Parameters Query Parameter Purpose sendHB Indicate whether or not to send heartbeats. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Cluster with that Cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Stream Job Information for a Cluster"},{"location":"reference/api/#stream-job-sink-messages","text":"/api/v1/jobConnectbyid/ jobID (SSE) /api/v1/jobConnectbyname/ jobName (SSE) Collect messages from the job sinks and merge them into a single websocket or SSE stream. Example Response Stream Excerpt data: { x : 928400.000000, y : 3.139934} data: { x : 928402.000000, y : -9.939772} data: { x : 928404.000000, y : 5.132876} data: { x : 928406.000000, y : 5.667712}","title":"Stream Job Sink Messages"},{"location":"reference/api/#submit-job-and-stream-job-sink-messages","text":"/api/v1/jobsubmitandconnect ( POST ) To submit a job and then collect messages from the job sinks and merge them into a single websocket or SSE stream, send a POST request to /api/v1/jobsubmitandconnect with a request body that describes the job. Example Request Body { name : ValidatorDemo , user : jschmoe , jobSla : { durationType : Perpetual , runtimeLimitSecs : 0 , minRuntimeSecs : 0 , userProvidedType : } } Example Response Stream Excerpt data: { x : 928400.000000, y : 3.139934} data: { x : 928402.000000, y : -9.939772} data: { x : 928404.000000, y : 5.132876} data: { x : 928406.000000, y : 5.667712}","title":"Submit Job and Stream Job Sink Messages"},{"location":"reference/api/#pagination","text":"You can configure some endpoints to return their responses in pages . This is to say that rather than returning all of the data responsive to a request all at once, the endpoint can return a specific subset of the data at a time. An endpoint that supports pagination will typically do so by means of two query parameters: Query Parameter Purpose pageSize the maximum number of records to return in this request (default = 0, which means all records) offset the record number to begin with in the set of records to return in this request So, for example, you might make consecutive requests for\u2026 endpoint ?pageSize=10 offset=0 endpoint ?pageSize=10 offset=10 endpoint ?pageSize=10 offset=20 \u2026and so forth, until you reach the final page of records. When you request records in a paginated fashion like this, the Mantis API will enclose them in a JSON structure like the following: { \"list\": [ { \"Id\": originalOffset , \u22ee }, { \"Id\": originalOffset+1 , \u22ee }, \u22ee ], \"prev\": \"/api/v1/ endpoint ?pageSize= pageSize = originalOffset+pageSize \", \"next\": \"/api/v1/ endpoint ?pageSize= pageSize = originalOffset\u2212pageSize \" } For example: { list : [ { Id : 101 , \u22ee }, { Id : 102 , \u22ee } ], next : /api/v1/someResource?pageSize=20 offset=120 , prev : /api/v1/someResource?pageSize=20 offset=80 } prev and/or next will be null if there is no previous or next page, that is, if you are at the first and/or last page in the pagination.","title":"Pagination"},{"location":"reference/api/#websocket","text":"Some desktops have trouble dealing with SSE . In such a case you can use WebSocket to connect to a Job (other API features are only available via the Mantis REST API). The WebSocket API runs from the same servers as the Mantis REST API, and you can reach it by using the ws:// protocol on port 7102 or the wss:// protocol on port 7103.","title":"WebSocket"},{"location":"reference/api/#connecting-to-job-output-sink","text":"Append this to the WebSocket URI to connect to a Job by name: /jobconnectbyname/ JobName Append this to the WebSocket URI to connect to a specific running Job ID: /jobconnectbyid/ JobID Upon connecting, the server starts writing messages that are coming in from the corresponding Jobs. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d Sink parameter which limits the rate at which the Sink output is sampled by the server.","title":"Connecting to Job Output (Sink)"},{"location":"reference/api/#submitting-and-connecting-to-job-output","text":"Append this to the WebSocket URI to submit and connect to the submitted Job: /jobsubmitandconnect Note that you will have to send one message on the WebSocket, which is the same JSON payload that you would send as a POST body for the equivalent REST API . The server then submits the Job and then starts writing messages into the WebSocket as it gets output from the Job. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d sink parameter which limits the rate at which the Sink output is sampled by the server.","title":"Submitting and Connecting to Job Output"},{"location":"reference/api/#example-javascript-to-connect-a-job","text":"!DOCTYPE html meta charset = utf-8 / title WebSocket Test / title script language = javascript type = text/javascript var wsUri = ws://your-domain.com/jobconnectbyname/YourJobName?sampleMSec=2000 ; var output ; function init () { output = document . getElementById ( output ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( CONNECTED ); } function onClose ( evt ) { writeToScreen ( DISCONNECTED ); } function onMessage ( evt ) { writeToScreen ( span style= color: blue; RESPONSE: + evt . data + /span ); } function onError ( evt ) { writeToScreen ( span style= color: red; ERROR: /span + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( p ); pre . style . wordWrap = break-word ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( load , init , true ); / script h2 WebSocket Test / h2 div id = output / div","title":"Example: Javascript to Connect a Job"},{"location":"reference/api/#example-javascript-to-submit-a-job-and-connect-to-it","text":"!DOCTYPE html meta charset = utf-8 / title WebSocket Test / title script language = javascript type = text/javascript var wsUri = ws://your-domain.com/jobsubmitandconnect/ ; var output ; function init () { output = document . getElementById ( output ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( CONNECTED ); // Change this to your job s submit json content. // See job submit REST API above for another example. doSend ( {\\n + \\ name\\ :\\ Outliers-mock3\\ ,\\n + \\ version\\ :\\ \\ ,\\n + \\ parameters\\ :[],\\n + \\ jobSla\\ :{\\ runtimeLimitSecs\\ :0,\\ durationType\\ :\\ Transient\\ ,\\ userProvidedType\\ :\\ {\\\\\\ unique\\\\\\ :\\\\\\ foobar\\\\\\ }\\ },\\n + \\ subscriptionTimeoutSecs\\ :\\ 90\\ ,\\n + \\ jobJarFileLocation\\ :null,\\n + \\ schedulingInfo\\ :{\\ stages\\ :{\\ 1\\ :{\\ numberOfInstances\\ :1,\\ machineDefinition\\ :{\\ cpuCores\\ :1.0,\\ memoryMB\\ :2048.0,\\ diskMB\\ :1.0,\\ scalable\\ :\\ true\\ }}}\\n + } ); } function onClose ( evt ) { writeToScreen ( DISCONNECTED ); } function onMessage ( evt ) { writeToScreen ( span style= color: blue; RESPONSE: + evt . data + /span ); } function onError ( evt ) { writeToScreen ( span style= color: red; ERROR: /span + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( p ); pre . style . wordWrap = break-word ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( load , init , true ); / script h2 WebSocket Test / h2 div id = output / div","title":"Example: Javascript to Submit a Job and Connect to It"},{"location":"reference/cli/","text":"Introduction You can interact with Mantis Job Clusters by means of a command-line interface (CLI). This page explains how to install and use this interface. Installation You can install the Mantis CLI locally in your development environment by following the instructions on the Mantis CLI Readme . Additionally, you can install the Mantis CLI onto your system as a package by following instructions on the Mantis CLI Releasing section of the readme. Mantis Commands A Mantis command takes the following form: mantis command You can also issue the following command to learn about the various Mantis commands available to you from the command line: mantis --help Or you can issue the following command to learn about the use of a specific Mantis command: mantis command --help Setting Up Your AWS Credentials Mantis can operate natively on different clouds. By default, Mantis provides CLI facilities for operating Mantis in the Amazon AWS environment. Before you can use the Mantis CLI to interact with Mantis Clusters in AWS, you must first establish your AWS credentials with Mantis. To do this, first obtain your AWS access key and secret access key, then issue the following command: mantis aws:configure [ -k | --key= access-key ] [ -s | --secret= secret-access-key ] This command will store your AWS credentials to the same location and with the same file format as the Amazon AWS CLI . Bootstrapping a Mantis Cluster in AWS You can use the Mantis CLI to bootstrap a Mantis Cluster in AWS. This will create the following: AWS key pair Default VPC Security groups Single Zookeeper EC2 instance backed by EBS volume Single Mesos Master EC2 instance backed by EBS volume Single Mesos Agent EC2 instance backed by EBS volume Single Mantis Master EC2 instance backed by EBS volume Single Mantis API EC2 instance backed by EBS volume To do this issue the following command: mantis aws:bootstrap Note Before you issue the mantis aws:bootstrap command you must have first set up your AWS credentials. You can do that with the mantis aws:configure command (see above). By default, the Mantis CLI bootstrap command is interactive and will prompt you for provisioning choices. Additionally, you can pass the following command-line parameters to skip some of the prompts from the mantis aws:bootstrap command: prompt parameter valid inputs select a region [ -r | --region ] region us-east-1 , us-east-2 , us-west-2 confirm -y n/a After you have finished executing the mantis aws:bootstrap command, you can submit Jobs into the Mantis cluster that it creates.","title":"CLI"},{"location":"reference/cli/#introduction","text":"You can interact with Mantis Job Clusters by means of a command-line interface (CLI). This page explains how to install and use this interface.","title":"Introduction"},{"location":"reference/cli/#installation","text":"You can install the Mantis CLI locally in your development environment by following the instructions on the Mantis CLI Readme . Additionally, you can install the Mantis CLI onto your system as a package by following instructions on the Mantis CLI Releasing section of the readme.","title":"Installation"},{"location":"reference/cli/#mantis-commands","text":"A Mantis command takes the following form: mantis command You can also issue the following command to learn about the various Mantis commands available to you from the command line: mantis --help Or you can issue the following command to learn about the use of a specific Mantis command: mantis command --help","title":"Mantis Commands"},{"location":"reference/cli/#setting-up-your-aws-credentials","text":"Mantis can operate natively on different clouds. By default, Mantis provides CLI facilities for operating Mantis in the Amazon AWS environment. Before you can use the Mantis CLI to interact with Mantis Clusters in AWS, you must first establish your AWS credentials with Mantis. To do this, first obtain your AWS access key and secret access key, then issue the following command: mantis aws:configure [ -k | --key= access-key ] [ -s | --secret= secret-access-key ] This command will store your AWS credentials to the same location and with the same file format as the Amazon AWS CLI .","title":"Setting Up Your AWS Credentials"},{"location":"reference/cli/#bootstrapping-a-mantis-cluster-in-aws","text":"You can use the Mantis CLI to bootstrap a Mantis Cluster in AWS. This will create the following: AWS key pair Default VPC Security groups Single Zookeeper EC2 instance backed by EBS volume Single Mesos Master EC2 instance backed by EBS volume Single Mesos Agent EC2 instance backed by EBS volume Single Mantis Master EC2 instance backed by EBS volume Single Mantis API EC2 instance backed by EBS volume To do this issue the following command: mantis aws:bootstrap Note Before you issue the mantis aws:bootstrap command you must have first set up your AWS credentials. You can do that with the mantis aws:configure command (see above). By default, the Mantis CLI bootstrap command is interactive and will prompt you for provisioning choices. Additionally, you can pass the following command-line parameters to skip some of the prompts from the mantis aws:bootstrap command: prompt parameter valid inputs select a region [ -r | --region ] region us-east-1 , us-east-2 , us-west-2 confirm -y n/a After you have finished executing the mantis aws:bootstrap command, you can submit Jobs into the Mantis cluster that it creates.","title":"Bootstrapping a Mantis Cluster in AWS"},{"location":"reference/dsl/","text":"Mantis leverages Reactive Streams for its Job DSL, specifically RxJava 1.x. Refer to the RxJava 1.x Operator Guide for more details.","title":"Job DSL"},{"location":"reference/mql/","text":"This page introduces the MQL operators, including a syntax summary, usage examples, and a brief description of each operator. For the most part these operations use their RxJava analogs: MQL operator RxJava operator select map window window where filter group by groupBy order by n/a limit take For this reason you may find the ReactiveX documentation useful as well, though it is a goal of MQL to provide you with a layer of abstraction such that you will not need to use or think about ReactiveX when writing queries. MQL attempts to stay true to the SQL syntax in order to reduce friction for new users writing queries and to allow them to leverage their experiences with SQL. An essential concept in MQL is the \u201c property ,\u201d which can take on one of several forms: property.name.here e[\"this\"][\"accesses\"][\"nested\"][\"properties\"] 15 \"string literal\" Most of the MQL operators use these forms to refer to properties within the event stream, as well as string and numeric literals. The last detail of note concerns unbounded vs. discrete streams. ReactiveX Observables that are not expected to close are an unbounded stream and must be bounded/discretized in order for certain operators to function with them. The window operator is useful for partitioning unbounded streams into discrete bounded streams for operators such as aggregate, group by, or order by. select Syntax: select property from source Examples: \"select * from servo\" \"select nf.node from servo\" \"select e[\"tags\"][\"nf.cluster\"] from servo\" The select operator allows you to project data into a new object by specifying which properties should be carried forward into the output. Properties will bear the same name as was used to select them. In the case of numbers and string literals their value will also be their name. In the case of nested properties the result will be a top-level object joined with dots. For example, the following select example\u2026 \"select \"string literal\", 45, e[\"tags\"][\"cluster\"], nf.node from servo\" \u2026would result in an object like: { string literal : string literal , 45: 45, tags.cluster : mantisagent , nf.node : i-123456 } Warning Be careful to avoid collisions between the top-level objects with dotted names and the nested objects that result in top-level properties with dotted names. Aggregates Syntax: \"select aggregate ( property ) from source \" Examples: \"select count(e[\"node\"]) from servo window 60\" \"select count(e[\"node\"]) from servo window 60 where e[\"metrics\"][\"latency\"] 350\" \"select average(e[\"metrics\"][\"latency\"]), e[\"node\"] from servo window 10\" Supported Aggregates: Min Max Average Count Sum Aggregates add analysis capabilities to MQL in order to allow you to answer interesting questions about data in real-time. They can be intermixed with regular properties in order to select properties and compute information about those properties, such as the last example above which computes average latency on a per-node basis in 10 second windows. Note Aggregates require that the stream on which they operate be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. You can use the distinct operator on a property to operate only on unique items within the window. This is particularly useful with the count aggregate if you want to count the number of items with some distinct property value in that window. For example: \"select count(distinct esn) from stream window 10 1\" from Syntax: \"select something from source \" Example: \"select * from servo\" The from clause indicates to MQL from which Observable it should draw data. This requires some explanation, as it bears different meaning in different contexts. Directly against queryable sources the from clause refers to the source Observable no matter which name is given. The operator is in fact optional, and the name of the source is arbitrary in this context, and the clause will be inserted for you if you omit it. When you use MQL as a library, the source corresponds with the names in the context map parameter. The second parameter to eval-mql() is a Map String, Observable T and the from clause will attempt to fetch the Observable from this Map . window Syntax: \"WINDOW integer \" Examples: \"select node, latency from servo window 10\" \"select MAX(latency) from servo window 60\" The window clause divides an otherwise unbounded stream of data into discrete time-bounded streams. The integer parameter is the number of seconds over which to perform this bounding. For example \"select * from observable window 10\" will produce 10-second windows of data. This discretization of streams is important for use with aggregate operations as well as group-by and order-by clauses which cannot be executed on, and will hang on, unbounded streams. where Syntax: \"select property from source where predicate \" Examples: \"select * from servo where node == \"i-123456\" AND e[\"metrics\"][\"latency\"] 350\" \"select * from servo where (node == \"i-123456\" AND e[\"metrics\"][\"latency\"] 350) OR node == \"1-abcdef\"\" \"select * from servo where node ==~ /i-123/\" \"select * from servo where e[\"metrics\"][\"latency\"] != null \"select * from servo where e[\"list_of_requests\"][*][\"status\"] == \"success\" The where clause filters any events out of the stream which do not match a given predictate. Predicates support AND and OR operations. Binary operators supported are = , == , , != , , = , , = , ==~ . The first two above are both equality, and either of the next two represent not-equal. You can use the last of those operators, ==~ , with a regular expression as in: \"where property ==~ / regex /\" (any Java regular expression will suffice). To take an event with certain attribute, use e[\"{{key}}\"] != null . If the event contains a list field, you can use the [*] operator to match objects inside that list. For example, say the events have a field called list_of_requests and each item in the list has a field called status . Then, the condition e[\"list_of_requests\"][*][\"status\"] == \"success\" will return true if at least 1 item has status equals success . Further, you can combine multiple conditions on the list. For example, e[ list_of_requests ][*][ status ] == success AND e[ list_of_requests ][*][ url ] == abc This condition returns true if at least 1 item has status equals success and url equals abc . group by Syntax: \"GROUP BY property \" Examples: \"select node, latency from servo where latency 300.0 group by node\" \"select MAX(latency), e[\"node\"] from servo group by node\" group by groups values in the output according to some property. This is particularly useful in conjunction with aggregate operators in which one can compute aggregate values over a group. For example, the following query calculates the maximum latency observed for each node in 60-second windows: \"select MAX(latency), e[\"node\"] from servo window 60 group by node\" Note The group by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. order by Syntax: \"ORDER BY property \" Example: \"select node, latency from servo group by node order by latency\" The order by operator orders the results in the inner-most Observable by the specified property. For example, the query \"select * from observable window 5 group by nf.node order by latency\" would produce an Observable of Observables (windows) of Observables (groups). The events within the groups would be ordered by their latency property. Note The order by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. limit Syntax: \"LIMIT integer \" Examples: \"select * from servo limit 10\" \"select AVERAGE(latency) from servo window 60 limit 10\" The limit operator takes as a parameter a single integer and bounds the number of results to \u2264 integer . Note Limit does not discretize the stream for earlier operators such as group by , order by , aggregates.","title":"MQL"},{"location":"reference/mql/#select","text":"Syntax: select property from source Examples: \"select * from servo\" \"select nf.node from servo\" \"select e[\"tags\"][\"nf.cluster\"] from servo\" The select operator allows you to project data into a new object by specifying which properties should be carried forward into the output. Properties will bear the same name as was used to select them. In the case of numbers and string literals their value will also be their name. In the case of nested properties the result will be a top-level object joined with dots. For example, the following select example\u2026 \"select \"string literal\", 45, e[\"tags\"][\"cluster\"], nf.node from servo\" \u2026would result in an object like: { string literal : string literal , 45: 45, tags.cluster : mantisagent , nf.node : i-123456 } Warning Be careful to avoid collisions between the top-level objects with dotted names and the nested objects that result in top-level properties with dotted names.","title":"select"},{"location":"reference/mql/#aggregates","text":"Syntax: \"select aggregate ( property ) from source \" Examples: \"select count(e[\"node\"]) from servo window 60\" \"select count(e[\"node\"]) from servo window 60 where e[\"metrics\"][\"latency\"] 350\" \"select average(e[\"metrics\"][\"latency\"]), e[\"node\"] from servo window 10\" Supported Aggregates: Min Max Average Count Sum Aggregates add analysis capabilities to MQL in order to allow you to answer interesting questions about data in real-time. They can be intermixed with regular properties in order to select properties and compute information about those properties, such as the last example above which computes average latency on a per-node basis in 10 second windows. Note Aggregates require that the stream on which they operate be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. You can use the distinct operator on a property to operate only on unique items within the window. This is particularly useful with the count aggregate if you want to count the number of items with some distinct property value in that window. For example: \"select count(distinct esn) from stream window 10 1\"","title":"Aggregates"},{"location":"reference/mql/#from","text":"Syntax: \"select something from source \" Example: \"select * from servo\" The from clause indicates to MQL from which Observable it should draw data. This requires some explanation, as it bears different meaning in different contexts. Directly against queryable sources the from clause refers to the source Observable no matter which name is given. The operator is in fact optional, and the name of the source is arbitrary in this context, and the clause will be inserted for you if you omit it. When you use MQL as a library, the source corresponds with the names in the context map parameter. The second parameter to eval-mql() is a Map String, Observable T and the from clause will attempt to fetch the Observable from this Map .","title":"from"},{"location":"reference/mql/#window","text":"Syntax: \"WINDOW integer \" Examples: \"select node, latency from servo window 10\" \"select MAX(latency) from servo window 60\" The window clause divides an otherwise unbounded stream of data into discrete time-bounded streams. The integer parameter is the number of seconds over which to perform this bounding. For example \"select * from observable window 10\" will produce 10-second windows of data. This discretization of streams is important for use with aggregate operations as well as group-by and order-by clauses which cannot be executed on, and will hang on, unbounded streams.","title":"window"},{"location":"reference/mql/#where","text":"Syntax: \"select property from source where predicate \" Examples: \"select * from servo where node == \"i-123456\" AND e[\"metrics\"][\"latency\"] 350\" \"select * from servo where (node == \"i-123456\" AND e[\"metrics\"][\"latency\"] 350) OR node == \"1-abcdef\"\" \"select * from servo where node ==~ /i-123/\" \"select * from servo where e[\"metrics\"][\"latency\"] != null \"select * from servo where e[\"list_of_requests\"][*][\"status\"] == \"success\" The where clause filters any events out of the stream which do not match a given predictate. Predicates support AND and OR operations. Binary operators supported are = , == , , != , , = , , = , ==~ . The first two above are both equality, and either of the next two represent not-equal. You can use the last of those operators, ==~ , with a regular expression as in: \"where property ==~ / regex /\" (any Java regular expression will suffice). To take an event with certain attribute, use e[\"{{key}}\"] != null . If the event contains a list field, you can use the [*] operator to match objects inside that list. For example, say the events have a field called list_of_requests and each item in the list has a field called status . Then, the condition e[\"list_of_requests\"][*][\"status\"] == \"success\" will return true if at least 1 item has status equals success . Further, you can combine multiple conditions on the list. For example, e[ list_of_requests ][*][ status ] == success AND e[ list_of_requests ][*][ url ] == abc This condition returns true if at least 1 item has status equals success and url equals abc .","title":"where"},{"location":"reference/mql/#group-by","text":"Syntax: \"GROUP BY property \" Examples: \"select node, latency from servo where latency 300.0 group by node\" \"select MAX(latency), e[\"node\"] from servo group by node\" group by groups values in the output according to some property. This is particularly useful in conjunction with aggregate operators in which one can compute aggregate values over a group. For example, the following query calculates the maximum latency observed for each node in 60-second windows: \"select MAX(latency), e[\"node\"] from servo window 60 group by node\" Note The group by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream.","title":"group by"},{"location":"reference/mql/#order-by","text":"Syntax: \"ORDER BY property \" Example: \"select node, latency from servo group by node order by latency\" The order by operator orders the results in the inner-most Observable by the specified property. For example, the query \"select * from observable window 5 group by nf.node order by latency\" would produce an Observable of Observables (windows) of Observables (groups). The events within the groups would be ordered by their latency property. Note The order by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream.","title":"order by"},{"location":"reference/mql/#limit","text":"Syntax: \"LIMIT integer \" Examples: \"select * from servo limit 10\" \"select AVERAGE(latency) from servo window 60 limit 10\" The limit operator takes as a parameter a single integer and bounds the number of results to \u2264 integer . Note Limit does not discretize the stream for earlier operators such as group by , order by , aggregates.","title":"limit"}]}