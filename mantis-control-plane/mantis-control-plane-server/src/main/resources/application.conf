####################################
# Akka Actor Reference Config File #
####################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

# Akka version, checked against the runtime version of Akka. Loaded from generated conf file.
include "version"
include "akka-http-version"

akka {
  # Home directory of Akka, modules in the deploy directory will be loaded
  home = ""

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream. It can perform
  # fine grained filtering based on the log source. The default
  # implementation filters on the `loglevel`.
  # FQCN of the LoggingFilter. The Class of the FQCN must implement
  # akka.event.LoggingFilter and have a public constructor with
  # (akka.actor.ActorSystem.Settings, akka.event.EventStream) parameters.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  # Specifies the default loggers dispatcher
  loggers-dispatcher = "akka.actor.default-dispatcher"

  # Loggers are created and registered synchronously during ActorSystem
  # start-up, and since they are actors, this timeout is used to bound the
  # waiting time
  logger-startup-timeout = 5s

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "INFO"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "INFO"

  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  # Log at info level when messages are sent to dead letters.
  # Possible values:
  # on: all dead letters are logged
  # off: no logging of dead letters
  # n: positive integer, number of dead letters that will be logged
  log-dead-letters = 10

  # Possibility to turn off logging of dead letters while the actor system
  # is shutting down. Logging is only done when enabled by 'log-dead-letters'
  # setting.
  log-dead-letters-during-shutdown = on

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Library extensions are regular extensions that are loaded at startup and are
  # available for third party library authors to enable auto-loading of extensions when
  # present on the classpath. This is done by appending entries:
  # 'library-extensions += "Extension"' in the library `reference.conf`.
  #
  # Should not be set by end user applications in 'application.conf', use the extensions property for that
  #
  library-extensions = ${?akka.library-extensions} ["akka.serialization.SerializationExtension"]

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Should be on the format: 'extensions = ["foo", "bar"]' etc.
  # See the Akka Documentation for more info about Extensions
  extensions = []

  # Toggles whether threads created by this ActorSystem should be daemons or not
  daemonic = off

  # JVM shutdown, System.exit(-1), in case of a fatal error,
  # such as OutOfMemoryError
  jvm-exit-on-fatal-error = on

  # Akka installs JVM shutdown hooks by default, e.g. in CoordinatedShutdown and Artery. This property will
  # not disable user-provided hooks registered using `CoordinatedShutdown#addCancellableJvmShutdownHook`.
  # This property is related to `akka.coordinated-shutdown.run-by-jvm-shutdown-hook` below.
  # This property makes it possible to disable all such hooks if the application itself
  # or a higher level framework such as Play prefers to install the JVM shutdown hook and
  # terminate the ActorSystem itself, with or without using CoordinatedShutdown.
  jvm-shutdown-hooks = on

  actor {

    # Either one of "local", "remote" or "cluster" or the
    # FQCN of the ActorRefProvider to be used; the below is the built-in default,
    # note that "remote" and "cluster" requires the akka-remote and akka-cluster
    # artifacts to be on the classpath.
    provider = "local"

    # The guardian "/user" will use this class to obtain its supervisorStrategy.
    # It needs to be a subclass of akka.actor.SupervisorStrategyConfigurator.
    # In addition to the default there is akka.actor.StoppingSupervisorStrategy.
    guardian-supervisor-strategy = "io.mantisrx.master.akka.MantisActorSupervisorStrategy"

    # Timeout for ActorSystem.actorOf
    creation-timeout = 20s

    # Serializes and deserializes (non-primitive) messages to ensure immutability,
    # this is only intended for testing.
    serialize-messages = off

    # Additional serialization bindings which are enabled automatically when allow-java-serialization is disabled.
    # settings are provided
    java-serialization-disabled-additional-serialization-bindings = {}

    # Serializes and deserializes creators (in Props) to ensure that they can be
    # sent over the network, this is only intended for testing. Purely local deployments
    # as marked with deploy.scope == LocalScope are exempt from verification.
    serialize-creators = off

    # Timeout for send operations to top-level actors which are in the process
    # of being started. This is only relevant if using a bounded mailbox or the
    # CallingThreadDispatcher for a top-level actor.
    unstarted-push-timeout = 10s

    typed {
      # Default timeout for typed actor methods with non-void return type
      timeout = 5s
    }

    # Mapping between ´deployment.router' short names to fully qualified class names
    router.type-mapping {
      from-code = "akka.routing.NoRouter"
      round-robin-pool = "akka.routing.RoundRobinPool"
      round-robin-group = "akka.routing.RoundRobinGroup"
      random-pool = "akka.routing.RandomPool"
      random-group = "akka.routing.RandomGroup"
      balancing-pool = "akka.routing.BalancingPool"
      smallest-mailbox-pool = "akka.routing.SmallestMailboxPool"
      broadcast-pool = "akka.routing.BroadcastPool"
      broadcast-group = "akka.routing.BroadcastGroup"
      scatter-gather-pool = "akka.routing.ScatterGatherFirstCompletedPool"
      scatter-gather-group = "akka.routing.ScatterGatherFirstCompletedGroup"
      tail-chopping-pool = "akka.routing.TailChoppingPool"
      tail-chopping-group = "akka.routing.TailChoppingGroup"
      consistent-hashing-pool = "akka.routing.ConsistentHashingPool"
      consistent-hashing-group = "akka.routing.ConsistentHashingGroup"
    }

    deployment {

      # deployment id pattern - on the format: /parent/child etc.
      default {

        # The id of the dispatcher to use for this actor.
        # If undefined or empty the dispatcher specified in code
        # (Props.withDispatcher) is used, or default-dispatcher if not
        # specified at all.
        dispatcher = ""

        # The id of the mailbox to use for this actor.
        # If undefined or empty the default mailbox of the configured dispatcher
        # is used or if there is no mailbox configuration the mailbox specified
        # in code (Props.withMailbox) is used.
        # If there is a mailbox defined in the configured dispatcher then that
        # overrides this setting.
        mailbox = ""

        # routing (load-balance) scheme to use
        # - available: "from-code", "round-robin", "random", "smallest-mailbox",
        #              "scatter-gather", "broadcast"
        # - or:        Fully qualified class name of the router class.
        #              The class must extend akka.routing.CustomRouterConfig and
        #              have a public constructor with com.typesafe.config.Config
        #              and optional akka.actor.DynamicAccess parameter.
        # - default is "from-code";
        # Whether or not an actor is transformed to a Router is decided in code
        # only (Props.withRouter). The type of router can be overridden in the
        # configuration; specifying "from-code" means that the values specified
        # in the code shall be used.
        # In case of routing, the actors to be routed to can be specified
        # in several ways:
        # - nr-of-instances: will create that many children
        # - routees.paths: will route messages to these paths using ActorSelection,
        #   i.e. will not create children
        # - resizer: dynamically resizable number of routees as specified in
        #   resizer below
        router = "from-code"

        # number of children to create in case of a router;
        # this setting is ignored if routees.paths is given
        nr-of-instances = 1

        # within is the timeout used for routers containing future calls
        within = 5 seconds

        # number of virtual nodes per node for consistent-hashing router
        virtual-nodes-factor = 10

        tail-chopping-router {
          # interval is duration between sending message to next routee
          interval = 10 milliseconds
        }

        routees {
          # Alternatively to giving nr-of-instances you can specify the full
          # paths of those actors which should be routed to. This setting takes
          # precedence over nr-of-instances
          paths = []
        }

        # To use a dedicated dispatcher for the routees of the pool you can
        # define the dispatcher configuration inline with the property name
        # 'pool-dispatcher' in the deployment section of the router.
        # For example:
        # pool-dispatcher {
        #   fork-join-executor.parallelism-min = 5
        #   fork-join-executor.parallelism-max = 5
        # }

        # Routers with dynamically resizable number of routees; this feature is
        # enabled by including (parts of) this section in the deployment
        resizer {

          enabled = off

          # The fewest number of routees the router should ever have.
          lower-bound = 1

          # The most number of routees the router should ever have.
          # Must be greater than or equal to lower-bound.
          upper-bound = 10

          # Threshold used to evaluate if a routee is considered to be busy
          # (under pressure). Implementation depends on this value (default is 1).
          # 0:   number of routees currently processing a message.
          # 1:   number of routees currently processing a message has
          #      some messages in mailbox.
          # > 1: number of routees with at least the configured pressure-threshold
          #      messages in their mailbox. Note that estimating mailbox size of
          #      default UnboundedMailbox is O(N) operation.
          pressure-threshold = 1

          # Percentage to increase capacity whenever all routees are busy.
          # For example, 0.2 would increase 20% (rounded up), i.e. if current
          # capacity is 6 it will request an increase of 2 more routees.
          rampup-rate = 0.2

          # Minimum fraction of busy routees before backing off.
          # For example, if this is 0.3, then we'll remove some routees only when
          # less than 30% of routees are busy, i.e. if current capacity is 10 and
          # 3 are busy then the capacity is unchanged, but if 2 or less are busy
          # the capacity is decreased.
          # Use 0.0 or negative to avoid removal of routees.
          backoff-threshold = 0.3

          # Fraction of routees to be removed when the resizer reaches the
          # backoffThreshold.
          # For example, 0.1 would decrease 10% (rounded up), i.e. if current
          # capacity is 9 it will request an decrease of 1 routee.
          backoff-rate = 0.1

          # Number of messages between resize operation.
          # Use 1 to resize before each message.
          messages-per-resize = 10
        }

        # Routers with dynamically resizable number of routees based on
        # performance metrics.
        # This feature is enabled by including (parts of) this section in
        # the deployment, cannot be enabled together with default resizer.
        optimal-size-exploring-resizer {

          enabled = off

          # The fewest number of routees the router should ever have.
          lower-bound = 1

          # The most number of routees the router should ever have.
          # Must be greater than or equal to lower-bound.
          upper-bound = 10

          # probability of doing a ramping down when all routees are busy
          # during exploration.
          chance-of-ramping-down-when-full = 0.2

          # Interval between each resize attempt
          action-interval = 5s

          # If the routees have not been fully utilized (i.e. all routees busy)
          # for such length, the resizer will downsize the pool.
          downsize-after-underutilized-for = 72h

          # Duration exploration, the ratio between the largest step size and
          # current pool size. E.g. if the current pool size is 50, and the
          # explore-step-size is 0.1, the maximum pool size change during
          # exploration will be +- 5
          explore-step-size = 0.1

          # Probabily of doing an exploration v.s. optmization.
          chance-of-exploration = 0.4

          # When downsizing after a long streak of underutilization, the resizer
          # will downsize the pool to the highest utiliziation multiplied by a
          # a downsize ratio. This downsize ratio determines the new pools size
          # in comparison to the highest utilization.
          # E.g. if the highest utilization is 10, and the down size ratio
          # is 0.8, the pool will be downsized to 8
          downsize-ratio = 0.8

          # When optimizing, the resizer only considers the sizes adjacent to the
          # current size. This number indicates how many adjacent sizes to consider.
          optimization-range = 16

          # The weight of the latest metric over old metrics when collecting
          # performance metrics.
          # E.g. if the last processing speed is 10 millis per message at pool
          # size 5, and if the new processing speed collected is 6 millis per
          # message at pool size 5. Given a weight of 0.3, the metrics
          # representing pool size 5 will be 6 * 0.3 + 10 * 0.7, i.e. 8.8 millis
          # Obviously, this number should be between 0 and 1.
          weight-of-latest-metric = 0.5
        }
      }

      "/IO-DNS/inet-address" {
        mailbox = "unbounded"
        router = "consistent-hashing-pool"
        nr-of-instances = 4
      }

      "/IO-DNS/inet-address/*" {
        dispatcher = "akka.actor.default-blocking-io-dispatcher"
      }

      "/IO-DNS/async-dns" {
        mailbox = "unbounded"
        router = "round-robin-pool"
        nr-of-instances = 1
      }

      # Mantis actors config
      "/JobClustersManager" {
        # isolate job management dispatcher for bulkheading
        dispatcher = "akka.actor.jobmgmt-dispatcher"
      }

      "/JobClustersManager/**" {
        # all children of JobClustersManagerActor, and their children recursively
        dispatcher = "akka.actor.jobmgmt-dispatcher"
      }

      "StatusEventBroker" {
        # separate dispatcher to isoloate status event handling
        dispatcher = "akka.actor.statusevent-dispatcher"
      }

      "AuditEventBroker" {
        # bulkhead audit events publishing as Chronos client makes blocking API calls
        dispatcher = "akka.actor.blocking-auditevent-dispatcher"
      }
    }

    default-dispatcher {
      # Must be one of the following
      # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting
      # MessageDispatcherConfigurator with a public constructor with
      # both com.typesafe.config.Config parameter and
      # akka.dispatch.DispatcherPrerequisites parameters.
      # PinnedDispatcher must be used together with executor=thread-pool-executor.
      type = "Dispatcher"

      # Which kind of ExecutorService to use for this dispatcher
      # Valid options:
      #  - "default-executor" requires a "default-executor" section
      #  - "fork-join-executor" requires a "fork-join-executor" section
      #  - "thread-pool-executor" requires a "thread-pool-executor" section
      #  - "affinity-pool-executor" requires an "affinity-pool-executor" section
      #  - A FQCN of a class extending ExecutorServiceConfigurator
      executor = "default-executor"

      # This will be used if you have set "executor = "default-executor"".
      # If an ActorSystem is created with a given ExecutionContext, this
      # ExecutionContext will be used as the default executor for all
      # dispatchers in the ActorSystem configured with
      # executor = "default-executor". Note that "default-executor"
      # is the default value for executor, and therefore used if not
      # specified otherwise. If no ExecutionContext is given,
      # the executor configured in "fallback" will be used.
      default-executor {
        fallback = "fork-join-executor"
      }

      # This will be used if you have set "executor = "affinity-pool-executor""
      # Underlying thread pool implementation is akka.dispatch.affinity.AffinityPool.
      # This executor is classified as "ApiMayChange".
      affinity-pool-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 0.8

        # Max number of threads to cap factor-based parallelism number to.
        parallelism-max = 64

        # Each worker in the pool uses a separate bounded MPSC queue. This value
        # indicates the upper bound of the queue. Whenever an attempt to enqueue
        # a task is made and the queue does not have capacity to accomodate
        # the task, the rejection handler created by the rejection handler specified
        # in "rejection-handler" is invoked.
        task-queue-size = 512

        # FQCN of the Rejection handler used in the pool.
        # Must have an empty public constructor and must
        # implement akka.actor.affinity.RejectionHandlerFactory.
        rejection-handler = "akka.dispatch.affinity.ThrowOnOverflowRejectionHandler"

        # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
        # The tradeoff is that to have low latency more CPU time must be used to be
        # able to react quickly on incoming messages or send as fast as possible after
        # backoff backpressure.
        # Level 1 strongly prefer low CPU consumption over low latency.
        # Level 10 strongly prefer low latency over low CPU consumption.
        idle-cpu-level = 5

        # FQCN of the akka.dispatch.affinity.QueueSelectorFactory.
        # The Class of the FQCN must have a public constructor with a
        # (com.typesafe.config.Config) parameter.
        # A QueueSelectorFactory create instances of akka.dispatch.affinity.QueueSelector,
        # that is responsible for determining which task queue a Runnable should be enqueued in.
        queue-selector = "akka.dispatch.affinity.FairDistributionHashCache"

        # When using the "akka.dispatch.affinity.FairDistributionHashCache" queue selector
        # internally the AffinityPool uses two methods to determine which task
        # queue to allocate a Runnable to:
        # - map based - maintains a round robin counter and a map of Runnable
        # hashcodes to queues that they have been associated with. This ensures
        # maximum fairness in terms of work distribution, meaning that each worker
        # will get approximately equal amount of mailboxes to execute. This is suitable
        # in cases where we have a small number of actors that will be scheduled on
        # the pool and we want to ensure the maximum possible utilization of the
        # available threads.
        # - hash based - the task - queue in which the runnable should go is determined
        # by using an uniformly distributed int to int hash function which uses the
        # hash code of the Runnable as an input. This is preferred in situations where we
        # have enough number of distinct actors to ensure statistically uniform
        # distribution of work across threads or we are ready to sacrifice the
        # former for the added benefit of avoiding map look-ups.
        fair-work-distribution {
          # The value serves as a threshold which determines the point at which the
          # pool switches from the first to the second work distribution schemes.
          # For example, if the value is set to 128, the pool can observe up to
          # 128 unique actors and schedule their mailboxes using the map based
          # approach. Once this number is reached the pool switches to hash based
          # task distribution mode. If the value is set to 0, the map based
          # work distribution approach is disabled and only the hash based is
          # used irrespective of the number of unique actors. Valid range is
          # 0 to 2048 (inclusive)
          threshold = 128
        }
      }

      # This will be used if you have set "executor = "fork-join-executor""
      # Underlying thread pool implementation is akka.dispatch.forkjoin.ForkJoinPool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 3.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 32

        # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
        # like peeking mode which "pop".
        task-peeking-mode = "FIFO"
      }

      # This will be used if you have set "executor = "thread-pool-executor""
      # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor
      thread-pool-executor {
        # Keep alive time for threads
        keep-alive-time = 60s

        # Define a fixed thread pool size with this property. The corePoolSize
        # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
        # value, if it is defined. Then the other pool-size properties will not
        # be used.
        #
        # Valid values are: `off` or a positive integer.
        fixed-pool-size = off

        # Min number of threads to cap factor-based corePoolSize number to
        core-pool-size-min = 8

        # The core-pool-size-factor is used to determine corePoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor).
        # Resulting size is then bounded by the core-pool-size-min and
        # core-pool-size-max values.
        core-pool-size-factor = 3.0

        # Max number of threads to cap factor-based corePoolSize number to
        core-pool-size-max = 64

        # Minimum number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-min = 8

        # The max-pool-size-factor is used to determine maximumPoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor)
        # The maximumPoolSize will not be less than corePoolSize.
        # It is only used if using a bounded task queue.
        max-pool-size-factor  = 3.0

        # Max number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-max = 64

        # Specifies the bounded capacity of the task queue (< 1 == unbounded)
        task-queue-size = -1

        # Specifies which type of task queue will be used, can be "array" or
        # "linked" (default)
        task-queue-type = "linked"

        # Allow core threads to time out
        allow-core-timeout = on
      }

      # How long time the dispatcher will wait for new actors until it shuts down
      shutdown-timeout = 1s

      # Throughput defines the number of messages that are processed in a batch
      # before the thread is returned to the pool. Set to 1 for as fair as possible.
      throughput = 5

      # Throughput deadline for Dispatcher, set to 0 or negative for no deadline
      throughput-deadline-time = 0ms

      # For BalancingDispatcher: If the balancing dispatcher should attempt to
      # schedule idle actors using the same dispatcher when a message comes in,
      # and the dispatchers ExecutorService is not fully busy already.
      attempt-teamwork = on

      # If this dispatcher requires a specific type of mailbox, specify the
      # fully-qualified class name here; the actually created mailbox will
      # be a subtype of this type. The empty string signifies no requirement.
      mailbox-requirement = ""
    }

    default-blocking-io-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      throughput = 1

      thread-pool-executor {
        fixed-pool-size = 16
      }
    }

    blocking-auditevent-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 4
      }
      throughput = 1
    }

    jobmgmt-dispatcher {
      # Dispatcher is the name of the event-based dispatcher
      type = Dispatcher
      # What kind of ExecutionService to use
      executor = "fork-join-executor"
      # Configuration for the fork join pool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4
        # Parallelism (threads) ... ceil(available processors * factor)
        parallelism-factor = 3.0
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 32
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }

    statusevent-dispatcher {
      # Dispatcher is the name of the event-based dispatcher
      type = Dispatcher
      # What kind of ExecutionService to use
      executor = "fork-join-executor"
      # Configuration for the fork join pool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 1
        # Parallelism (threads) ... ceil(available processors * factor)
        parallelism-factor = 2.0
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 4
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }

    bounded-mailbox {
      # use a NonBlockingBoundedMailbox for actors with bounded queues, the overflow messages are discarded as deadletters
      mailbox-type = "akka.dispatch.NonBlockingBoundedMailbox"
      mailbox-capacity = 5000
    }

    default-mailbox {
      # FQCN of the MailboxType. The Class of the FQCN must have a public
      # constructor with
      # (akka.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.
      mailbox-type = "akka.dispatch.UnboundedMailbox"
      # mailbox-type = "io.mantisrx.master.akka.UnboundedMeteredMailbox"

      # If the mailbox is bounded then it uses this setting to determine its
      # capacity. The provided value must be positive.
      # NOTICE:
      # Up to version 2.1 the mailbox type was determined based on this setting;
      # this is no longer the case, the type must explicitly be a bounded mailbox.
      mailbox-capacity = 1000

      # If the mailbox is bounded then this is the timeout for enqueueing
      # in case the mailbox is full. Negative values signify infinite
      # timeout, which should be avoided as it bears the risk of dead-lock.
      mailbox-push-timeout-time = 10s

      # For Actor with Stash: The default capacity of the stash.
      # If negative (or zero) then an unbounded stash is used (default)
      # If positive then a bounded stash is used and the capacity is set using
      # the property
      stash-capacity = -1
    }

    metered-mailbox {
      # FQCN of the MailboxType. The Class of the FQCN must have a public
      # constructor with
      # (akka.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.
      # mailbox-type = "akka.dispatch.UnboundedMailbox"
      mailbox-type = "io.mantisrx.master.akka.UnboundedMeteredMailbox"

      # If the mailbox is bounded then it uses this setting to determine its
      # capacity. The provided value must be positive.
      # NOTICE:
      # Up to version 2.1 the mailbox type was determined based on this setting;
      # this is no longer the case, the type must explicitly be a bounded mailbox.
      mailbox-capacity = 1000

      # If the mailbox is bounded then this is the timeout for enqueueing
      # in case the mailbox is full. Negative values signify infinite
      # timeout, which should be avoided as it bears the risk of dead-lock.
      mailbox-push-timeout-time = 10s

      # For Actor with Stash: The default capacity of the stash.
      # If negative (or zero) then an unbounded stash is used (default)
      # If positive then a bounded stash is used and the capacity is set using
      # the property
      stash-capacity = -1
    }

    mailbox {
      # Mapping between message queue semantics and mailbox configurations.
      # Used by akka.dispatch.RequiresMessageQueue[T] to enforce different
      # mailbox types on actors.
      # If your Actor implements RequiresMessageQueue[T], then when you create
      # an instance of that actor its mailbox type will be decided by looking
      # up a mailbox configuration via T in this mapping
      requirements {
        "akka.dispatch.UnboundedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-queue-based
        "akka.dispatch.BoundedMessageQueueSemantics" =
          akka.actor.bounded-mailbox
        "akka.dispatch.DequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-deque-based
        "akka.dispatch.UnboundedDequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-deque-based
        "akka.dispatch.BoundedDequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.bounded-deque-based
        "akka.dispatch.MultipleConsumerSemantics" =
          akka.actor.mailbox.unbounded-queue-based
        "akka.dispatch.ControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-control-aware-queue-based
        "akka.dispatch.UnboundedControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-control-aware-queue-based
        "akka.dispatch.BoundedControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.bounded-control-aware-queue-based
        "akka.event.LoggerMessageQueueSemantics" =
          akka.actor.mailbox.logger-queue
      }

      unbounded-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedMailbox"
      }

      bounded-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedMailbox"
      }

      unbounded-deque-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedDequeBasedMailbox"
      }

      bounded-deque-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedDequeBasedMailbox"
      }

      unbounded-control-aware-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
      }

      bounded-control-aware-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedControlAwareMailbox"
      }

      # The LoggerMailbox will drain all messages in the mailbox
      # when the system is shutdown and deliver them to the StandardOutLogger.
      # Do not change this unless you know what you are doing.
      logger-queue {
        mailbox-type = "akka.event.LoggerMailboxType"
      }
    }

    debug {
      # enable function of Actor.loggable(), which is to log any received message
      # at DEBUG level, see the “Testing Actor Systems” section of the Akka
      # Documentation at http://akka.io/docs
      receive = off

      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill etc.)
      autoreceive = off

      # enable DEBUG logging of actor lifecycle changes
      lifecycle = off

      # enable DEBUG logging of all LoggingFSMs for events, transitions and timers
      fsm = off

      # enable DEBUG logging of subscription changes on the eventStream
      event-stream = off

      # enable DEBUG logging of unhandled messages
      unhandled = on

      # enable WARN logging of misconfigured routers
      router-misconfiguration = off
    }

    # SECURITY BEST-PRACTICE is to disable java serialization for its multiple
    # known attack surfaces.
    #
    # This setting is a short-cut to
    # - using DisabledJavaSerializer instead of JavaSerializer
    # - enable-additional-serialization-bindings = on
    #
    # Completely disable the use of `akka.serialization.JavaSerialization` by the
    # Akka Serialization extension, instead DisabledJavaSerializer will
    # be inserted which will fail explicitly if attempts to use java serialization are made.
    #
    # The log messages emitted by such serializer SHOULD be treated as potential
    # attacks which the serializer prevented, as they MAY indicate an external operator
    # attempting to send malicious messages intending to use java serialization as attack vector.
    # The attempts are logged with the SECURITY marker.
    #
    # Please note that this option does not stop you from manually invoking java serialization
    #
    # The default value for this might be changed to off in future versions of Akka.
    allow-java-serialization = on

    # Entries for pluggable serializers and their bindings.
    serializers {
      java = "akka.serialization.JavaSerializer"
      bytes = "akka.serialization.ByteArraySerializer"
    }

    # Class to Serializer binding. You only need to specify the name of an
    # interface or abstract base class of the messages. In case of ambiguity it
    # is using the most specific configured class, or giving a warning and
    # choosing the “first” one.
    #
    # To disable one of the default serializers, assign its class to "none", like
    # "java.io.Serializable" = none
    serialization-bindings {
      "[B" = bytes
      "java.io.Serializable" = java
    }

    # Additional serialization-bindings that are replacing Java serialization are
    # defined in this section for backwards compatibility reasons. They are included
    # by default but can be excluded for backwards compatibility with Akka 2.4.x.
    # They can be disabled with enable-additional-serialization-bindings=off.
    #
    # This should only be needed for backwards compatibility reasons.
    enable-additional-serialization-bindings = on

    # Additional serialization-bindings that are replacing Java serialization are
    # defined in this section for backwards compatibility reasons. They are included
    # by default but can be excluded for backwards compatibility with Akka 2.4.x.
    # They can be disabled with enable-additional-serialization-bindings=off.
    additional-serialization-bindings {
    }

    # Log warnings when the default Java serialization is used to serialize messages.
    # The default serializer uses Java serialization which is not very performant and should not
    # be used in production environments unless you don't care about performance. In that case
    # you can turn this off.
    warn-about-java-serializer-usage = on

    # To be used with the above warn-about-java-serializer-usage
    # When warn-about-java-serializer-usage = on, and this warn-on-no-serialization-verification = off,
    # warnings are suppressed for classes extending NoSerializationVerificationNeeded
    # to reduce noize.
    warn-on-no-serialization-verification = on

    # Configuration namespace of serialization identifiers.
    # Each serializer implementation must have an entry in the following format:
    # `akka.actor.serialization-identifiers."FQCN" = ID`
    # where `FQCN` is fully qualified class name of the serializer implementation
    # and `ID` is globally unique serializer identifier number.
    # Identifier values from 0 to 40 are reserved for Akka internal usage.
    serialization-identifiers {
      "akka.serialization.JavaSerializer" = 1
      "akka.serialization.ByteArraySerializer" = 4
    }

    # Configuration items which are used by the akka.actor.ActorDSL._ methods
    dsl {
      # Maximum queue size of the actor created by newInbox(); this protects
      # against faulty programs which use select() and consistently miss messages
      inbox-size = 1000

      # Default timeout to assume for operations like Inbox.receive et al
      default-timeout = 5s
    }
  }

  http {

    caching {
      lfu-cache {
        # initial size of the cache
        initial-capacity = 10
        # max entries to hold in cache
        max-capacity = 100
        # expire the cache entry after write after this duration
        time-to-live = 1 s
      }
    }

    server {
      # The default value of the `Server` header to produce if no
      # explicit `Server`-header was included in a response.
      # If this value is the empty string and no header was included in
      # the request, no `Server` header will be rendered at all.
      server-header = akka-http/${akka.http.version}

      # "PREVIEW" features that are not yet fully production ready.
      # These flags can can change or be removed between patch releases.
      preview {
        # ONLY WORKS WITH `bindAndHandleAsync` (currently)
        #
        # If this setting is enabled AND the akka-http2-support is found
        # on the classpath the usual Http().bind... method calls will bind
        # using HTTP/2. Please note that you must configure HTTPS while doing so.
        enable-http2 = off
      }

      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle connection timeouts.
      idle-timeout = 30 s

      # Defines the default time period within which the application has to
      # produce an HttpResponse for any given HttpRequest it received.
      # The timeout begins to run when the *end* of the request has been
      # received, so even potentially long uploads can have a short timeout.
      # Set to `infinite` to completely disable request timeout checking.
      #
      # Make sure this timeout is smaller than the idle-timeout, otherwise,
      # the idle-timeout will kick in first and reset the TCP connection
      # without a response.
      #
      # If this setting is not `infinite` the HTTP server layer attaches a
      # `Timeout-Access` header to the request, which enables programmatic
      # customization of the timeout period and timeout response for each
      # request individually.
      request-timeout = 10 s

      # The time period within which the TCP binding process must be completed.
      bind-timeout = 1s

      # Default port to bind HTTP server to when no port was explicitly given.
      default-http-port = 80

      # Default port to bind HTTPS server to when no port was explicitly given.
      default-https-port = 443

      # The time period the HTTP server implementation will keep a connection open after
      # all data has been delivered to the network layer. This setting is similar to the SO_LINGER socket option
      # but does not only include the OS-level socket but also covers the Akka IO / Akka Streams network stack.
      # The setting is an extra precaution that prevents clients from keeping open a connection that is
      # already considered completed from the server side.
      #
      # If the network level buffers (including the Akka Stream / Akka IO networking stack buffers)
      # contains more data than can be transferred to the client in the given time when the server-side considers
      # to be finished with this connection, the client may encounter a connection reset.
      #
      # Set to 'infinite' to disable automatic connection closure (which will risk to leak connections).
      linger-timeout = 1 min

      # The maximum number of concurrently accepted connections when using the
      # `Http().bindAndHandle` methods.
      #
      # This setting doesn't apply to the `Http().bind` method which will still
      # deliver an unlimited backpressured stream of incoming connections.
      #
      # Note, that this setting limits the number of the connections on a best-effort basis.
      # It does *not* strictly guarantee that the number of established TCP connections will never
      # exceed the limit (but it will be approximately correct) because connection termination happens
      # asynchronously. It also does *not* guarantee that the number of concurrently active handler
      # flow materializations will never exceed the limit for the reason that it is impossible to reliably
      # detect when a materialization has ended.
      max-connections = 500000

      # The maximum number of requests that are accepted (and dispatched to
      # the application) on one single connection before the first request
      # has to be completed.
      # Incoming requests that would cause the pipelining limit to be exceeded
      # are not read from the connections socket so as to build up "back-pressure"
      # to the client via TCP flow control.
      # A setting of 1 disables HTTP pipelining, since only one request per
      # connection can be "open" (i.e. being processed by the application) at any
      # time. Set to higher values to enable HTTP pipelining.
      # This value must be > 0 and <= 1024.
      pipelining-limit = 16

      # Enables/disables the addition of a `Remote-Address` header
      # holding the clients (remote) IP address.
      remote-address-header = on

      # Enables/disables the addition of a `Raw-Request-URI` header holding the
      # original raw request URI as the client has sent it.
      raw-request-uri-header = off

      # Enables/disables automatic handling of HEAD requests.
      # If this setting is enabled the server dispatches HEAD requests as GET
      # requests to the application and automatically strips off all message
      # bodies from outgoing responses.
      # Note that, even when this setting is off the server will never send
      # out message bodies on responses to HEAD requests.
      transparent-head-requests = on

      # Enables/disables the returning of more detailed error messages to
      # the client in the error response.
      # Should be disabled for browser-facing APIs due to the risk of XSS attacks
      # and (probably) enabled for internal or non-browser APIs.
      # Note that akka-http will always produce log messages containing the full
      # error details.
      verbose-error-messages = off

      # The initial size of the buffer to render the response headers in.
      # Can be used for fine-tuning response rendering performance but probably
      # doesn't have to be fiddled with in most applications.
      response-header-size-hint = 512

      # The requested maximum length of the queue of incoming connections.
      # If the server is busy and the backlog is full the OS will start dropping
      # SYN-packets and connection attempts may fail. Note, that the backlog
      # size is usually only a maximum size hint for the OS and the OS can
      # restrict the number further based on global limits.
      backlog = 100

      # If this setting is empty the server only accepts requests that carry a
      # non-empty `Host` header. Otherwise it responds with `400 Bad Request`.
      # Set to a non-empty value to be used in lieu of a missing or empty `Host`
      # header to make the server accept such requests.
      # Note that the server will never accept HTTP/1.1 request without a `Host`
      # header, i.e. this setting only affects HTTP/1.1 requests with an empty
      # `Host` header as well as HTTP/1.0 requests.
      # Examples: `www.spray.io` or `example.com:8080`
      default-host-header = ""

      # Socket options to set for the listening socket. If a setting is left
      # undefined, it will use whatever the default on the system is.
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = undefined
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }

      # When graceful termination is enabled and used invoked with a deadline,
      # after the deadline passes pending requests will be replied to with a "terminating" http response,
      # instead of delivering those requests to the user-handler.
      # This response is configurable here using configuration, or via code in case more a sophisticated (e.g. with response entity)
      # response is needed.
      #
      termination-deadline-exceeded-response {
        # Status code of the "terminating" response to be automatically sent to pending requests once the termination deadline is exceeded.
        status = 503 # ServiceUnavailable
      }

      # Modify to tweak parsing settings on the server-side only.
      parsing {
        # no overrides by default, see `akka.http.parsing` for default values
      }

      # Enables/disables the logging of unencrypted HTTP traffic to and from the HTTP
      # server for debugging reasons.
      #
      # Note: Use with care. Logging of unencrypted data traffic may expose secret data.
      #
      # Incoming and outgoing traffic will be logged in hexdump format. To enable logging,
      # specify the number of bytes to log per chunk of data (the actual chunking depends
      # on implementation details and networking conditions and should be treated as
      # arbitrary).
      #
      # For logging on the client side, see akka.http.client.log-unencrypted-network-bytes.
      #
      # `off` : no log messages are produced
      # Int   : determines how many bytes should be logged per data chunk
      log-unencrypted-network-bytes = off

      websocket {
        # periodic keep alive may be implemented using by sending Ping frames
        # upon which the other side is expected to reply with a Pong frame,
        # or by sending a Pong frame, which serves as unidirectional heartbeat.
        # Valid values:
        #   ping - default, for bi-directional ping/pong keep-alive heartbeating
        #   pong - for uni-directional pong keep-alive heartbeating
        #
        # It is also possible to provide a payload for each heartbeat message,
        # this setting can be configured programatically by modifying the websocket settings.
        # See: https://doc.akka.io/docs/akka-http/current/server-side/websocket-support.html
        periodic-keep-alive-mode = ping

        # Interval for sending periodic keep-alives
        # The frame sent will be the one configured in akka.http.server.websocket.periodic-keep-alive-mode
        # `infinite` by default, or a duration that is the max idle interval after which an keep-alive frame should be sent
        # The value `infinite` means that *no* keep-alive heartbeat will be sent, as: "the allowed idle time is infinite"
        periodic-keep-alive-max-idle = infinite
      }
    }

    client {
      # The default value of the `User-Agent` header to produce if no
      # explicit `User-Agent`-header was included in a request.
      # If this value is the empty string and no header was included in
      # the request, no `User-Agent` header will be rendered at all.
      user-agent-header = akka-http/${akka.http.version}

      # The time period within which the TCP connecting process must be completed.
      connecting-timeout = 10s

      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle timeouts.
      idle-timeout = 60 s

      # The initial size of the buffer to render the request headers in.
      # Can be used for fine-tuning request rendering performance but probably
      # doesn't have to be fiddled with in most applications.
      request-header-size-hint = 512

      # Socket options to set for the listening socket. If a setting is left
      # undefined, it will use whatever the default on the system is.
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = undefined
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }

      # Client https proxy options. When using ClientTransport.httpsProxy() with or without credentials,
      # host/port must be either passed explicitly or set here. If a host is not set, the proxy will not be used.
      proxy {
        https {
          host = ""
          port = 443
        }
      }

      # Modify to tweak parsing settings on the client-side only.
      parsing {
        # no overrides by default, see `akka.http.parsing` for default values
      }

      # Enables/disables the logging of unencrypted HTTP traffic to and from the HTTP
      # client for debugging reasons.
      #
      # Note: Use with care. Logging of unencrypted data traffic may expose secret data.
      #
      # Incoming and outgoing traffic will be logged in hexdump format. To enable logging,
      # specify the number of bytes to log per chunk of data (the actual chunking depends
      # on implementation details and networking conditions and should be treated as
      # arbitrary).
      #
      # For logging on the server side, see akka.http.server.log-unencrypted-network-bytes.
      #
      # `off` : no log messages are produced
      # Int   : determines how many bytes should be logged per data chunk
      log-unencrypted-network-bytes = off

      websocket {
        # periodic keep alive may be implemented using by sending Ping frames
        # upon which the other side is expected to reply with a Pong frame,
        # or by sending a Pong frame, which serves as unidirectional heartbeat.
        # Valid values:
        #   ping - default, for bi-directional ping/pong keep-alive heartbeating
        #   pong - for uni-directional pong keep-alive heartbeating
        #
        # See https://tools.ietf.org/html/rfc6455#section-5.5.2
        # and https://tools.ietf.org/html/rfc6455#section-5.5.3 for more information
        periodic-keep-alive-mode = ping

        # Interval for sending periodic keep-alives
        # The frame sent will be the onne configured in akka.http.server.websocket.periodic-keep-alive-mode
        # `infinite` by default, or a duration that is the max idle interval after which an keep-alive frame should be sent
        periodic-keep-alive-max-idle = infinite
      }
    }

    host-connection-pool {
      # The maximum number of parallel connections that a connection pool to a
      # single host endpoint is allowed to establish. Must be greater than zero.
      max-connections = 4

      # The minimum number of parallel connections that a pool should keep alive ("hot").
      # If the number of connections is falling below the given threshold, new ones are being spawned.
      # You can use this setting to build a hot pool of "always on" connections.
      # Default is 0, meaning there might be no active connection at given moment.
      # Keep in mind that `min-connections` should be smaller than `max-connections` or equal
      min-connections = 0

      # The maximum number of times failed requests are attempted again,
      # (if the request can be safely retried) before giving up and returning an error.
      # Set to zero to completely disable request retries.
      max-retries = 5

      # The maximum number of open requests accepted into the pool across all
      # materializations of any of its client flows.
      # Protects against (accidentally) overloading a single pool with too many client flow materializations.
      # Note that with N concurrent materializations the max number of open request in the pool
      # will never exceed N * max-connections * pipelining-limit.
      # Must be a power of 2 and > 0!
      max-open-requests = 32

      # The maximum number of requests that are dispatched to the target host in
      # batch-mode across a single connection (HTTP pipelining).
      # A setting of 1 disables HTTP pipelining, since only one request per
      # connection can be "in flight" at any time.
      # Set to higher values to enable HTTP pipelining.
      # This value must be > 0.
      # (Note that, independently of this setting, pipelining will never be done
      # on a connection that still has a non-idempotent request in flight.
      #
      # Before increasing this value, make sure you understand the effects of head-of-line blocking.
      # Using a connection pool, a request may be issued on a connection where a previous
      # long-running request hasn't finished yet. The response to the pipelined requests may then be stuck
      # behind the response of the long-running previous requests on the server. This may introduce an
      # unwanted "coupling" of run time between otherwise unrelated requests.
      #
      # See http://tools.ietf.org/html/rfc7230#section-6.3.2 for more info.)
      pipelining-limit = 1

      # The time after which an idle connection pool (without pending requests)
      # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
      idle-timeout = 30 s

      # The pool implementation to use. Currently supported are:
      #  - legacy: the original 10.0.x pool implementation
      #  - new: the pool implementation that became the default in 10.1.x and will receive fixes and new features
      pool-implementation = new

      # The "new" pool implementation will fail a connection early and clear the slot if a response entity was not
      # subscribed during the given time period after the response was dispatched. In busy systems the timeout might be
      # too tight if a response is not picked up quick enough after it was dispatched by the pool.
      response-entity-subscription-timeout = 1.second

      # Modify this section to tweak client settings only for host connection pools APIs like `Http().superPool` or
      # `Http().singleRequest`.
      client = {
        # no overrides by default, see `akka.http.client` for default values
      }
    }

    # Modify to tweak default parsing settings.
    #
    # IMPORTANT:
    # Please note that this sections settings can be overridden by the corresponding settings in:
    # `akka.http.server.parsing`, `akka.http.client.parsing` or `akka.http.host-connection-pool.client.parsing`.
    parsing {
      # The limits for the various parts of the HTTP message parser.
      max-uri-length             = 2k
      max-method-length          = 16
      max-response-reason-length = 64
      max-header-name-length     = 64
      max-header-value-length    = 8k
      max-header-count           = 64
      max-chunk-ext-length       = 256
      max-chunk-size             = 1m

      # Default maximum content length which should not be exceeded by incoming request entities.
      # Can be changed at runtime (to a higher or lower value) via the `HttpEntity::withSizeLimit` method.
      # Note that it is not necessarily a problem to set this to a high value as all stream operations
      # are always properly backpressured.
      # Nevertheless you might want to apply some limit in order to prevent a single client from consuming
      # an excessive amount of server resources.
      #
      # Set to `infinite` to completely disable entity length checks. (Even then you can still apply one
      # programmatically via `withSizeLimit`.)
      max-content-length = 8m

      # The maximum number of bytes to allow when reading the entire entity into memory with `toStrict`
      # (which is used by the `toStrictEntity` and `extractStrictEntity` directives)
      max-to-strict-bytes = 8m

      # Sets the strictness mode for parsing request target URIs.
      # The following values are defined:
      #
      # `strict`: RFC3986-compliant URIs are required,
      #     a 400 response is triggered on violations
      #
      # `relaxed`: all visible 7-Bit ASCII chars are allowed
      #
      uri-parsing-mode = strict

      # Sets the parsing mode for parsing cookies.
      # The following value are defined:
      #
      # `rfc6265`: Only RFC6265-compliant cookies are parsed. Surrounding double-quotes are accepted and
      #   automatically removed. Non-compliant cookies are silently discarded.
      # `raw`: Raw parsing allows any non-control character but ';' to appear in a cookie value. There's no further
      #   post-processing applied, so that the resulting value string may contain any number of whitespace, unicode,
      #   double quotes, or '=' characters at any position.
      #   The rules for parsing the cookie name are the same ones from RFC 6265.
      #
      cookie-parsing-mode = rfc6265

      # Enables/disables the logging of warning messages in case an incoming
      # message (request or response) contains an HTTP header which cannot be
      # parsed into its high-level model class due to incompatible syntax.
      # Note that, independently of this settings, akka-http will accept messages
      # with such headers as long as the message as a whole would still be legal
      # under the HTTP specification even without this header.
      # If a header cannot be parsed into a high-level model instance it will be
      # provided as a `RawHeader`.
      # If logging is enabled it is performed with the configured
      # `error-logging-verbosity`.
      illegal-header-warnings = on

      # Sets the list of headers for which illegal values will *not* cause warning logs to be emitted;
      #
      # Adding a header name to this setting list disables the logging of warning messages in case an incoming　message
      # contains an HTTP header which cannot be　parsed into its high-level model class due to incompatible syntax.
      ignore-illegal-header-for = []

      # Parse headers into typed model classes in the Akka Http core layer.
      #
      # If set to `off`, only essential headers will be parsed into their model classes. All other ones will be provided
      # as instances of `RawHeader`. Currently, `Connection`, `Host`, and `Expect` headers will still be provided in their
      # typed model. The full list of headers still provided as modeled instances can be found in the source code of
      # `akka.http.impl.engine.parsing.HttpHeaderParser.alwaysParsedHeaders`. Note that (regardless of this setting)
      # some headers like `Content-Type` are treated specially and will never be provided in the list of headers.
      modeled-header-parsing = on

      # Configures the verbosity with which message (request or response) parsing
      # errors are written to the application log.
      #
      # Supported settings:
      # `off`   : no log messages are produced
      # `simple`: a condensed single-line message is logged
      # `full`  : the full error details (potentially spanning several lines) are logged
      error-logging-verbosity = full

      # Configures the processing mode when encountering illegal characters in
      # header value of response.
      #
      # Supported mode:
      # `error`  : default mode, throw an ParsingException and terminate the processing
      # `warn`   : ignore the illegal characters in response header value and log a warning message
      # `ignore` : just ignore the illegal characters in response header value
      illegal-response-header-value-processing-mode = error

      # limits for the number of different values per header type that the
      # header cache will hold
      header-cache {
        default = 12
        Content-MD5 = 0
        Date = 0
        If-Match = 0
        If-Modified-Since = 0
        If-None-Match = 0
        If-Range = 0
        If-Unmodified-Since = 0
        User-Agent = 32
      }

      # Enables/disables inclusion of an Tls-Session-Info header in parsed
      # messages over Tls transports (i.e., HttpRequest on server side and
      # HttpResponse on client side).
      tls-session-info-header = off
    }
  }

  # Used to set the behavior of the scheduler.
  # Changing the default values may change the system behavior drastically so make
  # sure you know what you're doing! See the Scheduler section of the Akka
  # Documentation for more details.
  scheduler {
    # The LightArrayRevolverScheduler is used as the default scheduler in the
    # system. It does not execute the scheduled tasks on exact time, but on every
    # tick, it will run everything that is (over)due. You can increase or decrease
    # the accuracy of the execution timing by specifying smaller or larger tick
    # duration. If you are scheduling a lot of tasks you should consider increasing
    # the ticks per wheel.
    # Note that it might take up to 1 tick to stop the Timer, so setting the
    # tick-duration to a high value will make shutting down the actor system
    # take longer.
    tick-duration = 10ms

    # The timer uses a circular wheel of buckets to store the timer tasks.
    # This should be set such that the majority of scheduled timeouts (for high
    # scheduling frequency) will be shorter than one rotation of the wheel
    # (ticks-per-wheel * ticks-duration)
    # THIS MUST BE A POWER OF TWO!
    ticks-per-wheel = 512

    # This setting selects the timer implementation which shall be loaded at
    # system start-up.
    # The class given here must implement the akka.actor.Scheduler interface
    # and offer a public constructor which takes three arguments:
    #  1) com.typesafe.config.Config
    #  2) akka.event.LoggingAdapter
    #  3) java.util.concurrent.ThreadFactory
    implementation = akka.actor.LightArrayRevolverScheduler

    # When shutting down the scheduler, there will typically be a thread which
    # needs to be stopped, and this timeout determines how long to wait for
    # that to happen. In case of timeout the shutdown of the actor system will
    # proceed without running possibly still enqueued tasks.
    shutdown-timeout = 5s
  }

  io {

    # By default the select loops run on dedicated threads, hence using a
    # PinnedDispatcher
    pinned-dispatcher {
      type = "PinnedDispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.allow-core-timeout = off
    }

    tcp {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this TCP module; there is
      # no intrinsic general limit, this setting is meant to enable DoS
      # protection by limiting the number of concurrently connected clients.
      # Also note that this is a "soft" limit; in certain cases the implementation
      # will accept a few connections more or a few less than the number configured
      # here. Must be an integer > 0 or "unlimited".
      max-channels = 256000

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of connection that are accepted in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      batch-accept-limit = 10

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # The duration a connection actor waits for a `Register` message from
      # its commander before aborting the connection.
      register-timeout = 5s

      # The maximum number of bytes delivered by a `Received` message. Before
      # more data is read from the network the connection actor will try to
      # do other work.
      # The purpose of this setting is to impose a smaller limit than the
      # configured receive buffer size. When using value 'unlimited' it will
      # try to read all from the receive buffer.
      max-received-message-size = unlimited

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.default-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.default-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # on which file IO tasks are scheduled
      file-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

      # The maximum number of bytes (or "unlimited") to transfer in one batch
      # when using `WriteFile` command which uses `FileChannel.transferTo` to
      # pipe files to a TCP socket. On some OS like Linux `FileChannel.transferTo`
      # may block for a long time when network IO is faster than file IO.
      # Decreasing the value may improve fairness while increasing may improve
      # throughput.
      file-io-transferTo-limit = 512 KiB

      # The number of times to retry the `finishConnect` call after being notified about
      # OP_CONNECT. Retries are needed if the OP_CONNECT notification doesn't imply that
      # `finishConnect` will succeed, which is the case on Android.
      finish-connect-retries = 5

      # On Windows connection aborts are not reliably detected unless an OP_READ is
      # registered on the selector _after_ the connection has been reset. This
      # workaround enables an OP_CONNECT which forces the abort to be visible on Windows.
      # Enabling this setting on other platforms than Windows will cause various failures
      # and undefined behavior.
      # Possible values of this key are on, off and auto where auto will enable the
      # workaround if Windows is detected automatically.
      windows-connection-abort-workaround-enabled = off
    }

    udp {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this UDP module Generally
      # UDP does not require a large number of channels, therefore it is
      # recommended to keep this setting low.
      max-channels = 4096

      # The select loop can be used in two modes:
      # - setting "infinite" will select without a timeout, hogging a thread
      # - setting a positive timeout will do a bounded select call,
      #   enabling sharing of a single thread between multiple selectors
      #   (in this case you will have to use a different configuration for the
      #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
      # - setting it to zero means polling, i.e. calling selectNow()
      select-timeout = infinite

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of datagrams that are read in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      receive-throughput = 3

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.default-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.default-dispatcher"
    }

    udp-connected {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this UDP module Generally
      # UDP does not require a large number of channels, therefore it is
      # recommended to keep this setting low.
      max-channels = 4096

      # The select loop can be used in two modes:
      # - setting "infinite" will select without a timeout, hogging a thread
      # - setting a positive timeout will do a bounded select call,
      #   enabling sharing of a single thread between multiple selectors
      #   (in this case you will have to use a different configuration for the
      #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
      # - setting it to zero means polling, i.e. calling selectNow()
      select-timeout = infinite

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of datagrams that are read in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      receive-throughput = 3

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.default-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.default-dispatcher"
    }

    dns {
      # Fully qualified config path which holds the dispatcher configuration
      # for the manager and resolver router actors.
      # For actual router configuration see akka.actor.deployment./IO-DNS/*
      dispatcher = "akka.actor.default-dispatcher"

      # Name of the subconfig at path akka.io.dns, see inet-address below
      #
      # Change to `async-dns` to use the new "native" DNS resolver,
      # which is also capable of resolving SRV records.
      resolver = "inet-address"

      # To-be-deprecated DNS resolver implementation which uses the Java InetAddress to resolve DNS records.
      # To be replaced by `akka.io.dns.async` which implements the DNS protocol natively and without blocking (which InetAddress does)
      inet-address {
        # Must implement akka.io.DnsProvider
        provider-object = "akka.io.InetAddressDnsProvider"

        # To set the time to cache name resolutions
        # Possible values:
        # default: sun.net.InetAddressCachePolicy.get() and getNegative()
        # forever: cache forever
        # never: no caching
        # n [time unit]: positive timeout with unit, for example "30 s"
        positive-ttl = default
        negative-ttl = default

        # How often to sweep out expired cache entries.
        # Note that this interval has nothing to do with TTLs
        cache-cleanup-interval = 120s
      }

      async-dns {
        provider-object = "akka.io.dns.internal.AsyncDnsProvider"

        # Configures nameservers to query during DNS resolution.
        # Defaults to the nameservers that would be used by the JVM by default.
        # Set to a list of IPs to override the servers, e.g. [ "8.8.8.8", "8.8.4.4" ] for Google's servers
        # If multiple are defined then they are tried in order until one responds
        nameservers = default

        # The time that a request is allowed to live before being discarded
        # given no reply. The lower bound of this should always be the amount
        # of time to reasonably expect a DNS server to reply within.
        # If multiple name servers are provided then each gets this long to response before trying
        # the next one
        resolve-timeout = 5s

        # How often to sweep out expired cache entries.
        # Note that this interval has nothing to do with TTLs
        cache-cleanup-interval = 120s
      }
    }
  }


  # CoordinatedShutdown is an extension that will perform registered
  # tasks in the order that is defined by the phases. It is started
  # by calling CoordinatedShutdown(system).run(). This can be triggered
  # by different things, for example:
  # - JVM shutdown hook will by default run CoordinatedShutdown
  # - Cluster node will automatically run CoordinatedShutdown when it
  #   sees itself as Exiting
  # - A management console or other application specific command can
  #   run CoordinatedShutdown
  coordinated-shutdown {
    # The timeout that will be used for a phase if not specified with
    # 'timeout' in the phase
    default-phase-timeout = 5 s

    # Terminate the ActorSystem in the last phase actor-system-terminate.
    terminate-actor-system = on

    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = off

    # Exit status to use on System.exit(int) when 'exit-jvm' is 'on'.
    exit-code = 0

    # Run the coordinated shutdown when the JVM process exits, e.g.
    # via kill SIGTERM signal (SIGINT ctrl-c doesn't work).
    # This property is related to `akka.jvm-shutdown-hooks` above.
    run-by-jvm-shutdown-hook = on

    # When Coordinated Shutdown is triggered an instance of `Reason` is
    # required. That value can be used to override the default settings.
    # Only 'exit-jvm', 'exit-code' and 'terminate-actor-system' may be
    # overriden depending on the reason.
    reason-overrides {
      # Overrides are applied using the `reason.getClass.getName`. This
      # default overrides the `exit-code` when the `Reason` is a cluster
      # Downing event (identified by the object
      # "akka.actor.CoordinatedShutdown$ClusterDowningReason$").
      "akka.actor.CoordinatedShutdown$ClusterDowningReason$" {
        exit-code = -1
      }
    }

    # CoordinatedShutdown is enabled by default and will run the tasks that
    # are added to these phases by individual Akka modules and user logic.
    #
    # The phases are ordered as a DAG by defining the dependencies between the phases
    # to make sure shutdown tasks are run in the right order.
    #
    # In general user tasks belong in the first few phases, but there may be use
    # cases where you would want to hook in new phases or register tasks later in
    # the DAG.
    #
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # - enabled=off: Skip all tasks registered in this phase. DO NOT use
    #                this to disable phases unless you are absolutely sure what the
    #                consequences are. Many of the built in tasks depend on other tasks
    #                having been executed in earlier phases and may break if those are disabled.
    # depends-on=[]: Run the phase after the given phases
    phases {

      # The first pre-defined phase that applications can add tasks to.
      # Note that more phases can be added in the application's
      # configuration by overriding this phase with an additional
      # depends-on.
      before-service-unbind {
      }

      # Stop accepting new incoming connections.
      # This is where you can register tasks that makes a server stop accepting new connections. Already
      # established connections should be allowed to continue and complete if possible.
      service-unbind {
        depends-on = [before-service-unbind]
      }

      # Wait for requests that are in progress to be completed.
      # This is where you register tasks that will wait for already established connections to complete, potentially
      # also first telling them that it is time to close down.
      service-requests-done {
        depends-on = [service-unbind]
      }

      # Final shutdown of service endpoints.
      # This is where you would add tasks that forcefully kill connections that are still around.
      service-stop {
        depends-on = [service-requests-done]
      }

      # Phase for custom application tasks that are to be run
      # after service shutdown and before cluster shutdown.
      before-cluster-shutdown {
        depends-on = [service-stop]
      }

      # Graceful shutdown of the Cluster Sharding regions.
      # This phase is not meant for users to add tasks to.
      cluster-sharding-shutdown-region {
        timeout = 10 s
        depends-on = [before-cluster-shutdown]
      }

      # Emit the leave command for the node that is shutting down.
      # This phase is not meant for users to add tasks to.
      cluster-leave {
        depends-on = [cluster-sharding-shutdown-region]
      }

      # Shutdown cluster singletons
      # This is done as late as possible to allow the shard region shutdown triggered in
      # the "cluster-sharding-shutdown-region" phase to complete before the shard coordinator is shut down.
      # This phase is not meant for users to add tasks to.
      cluster-exiting {
        timeout = 10 s
        depends-on = [cluster-leave]
      }

      # Wait until exiting has been completed
      # This phase is not meant for users to add tasks to.
      cluster-exiting-done {
        depends-on = [cluster-exiting]
      }

      # Shutdown the cluster extension
      # This phase is not meant for users to add tasks to.
      cluster-shutdown {
        depends-on = [cluster-exiting-done]
      }

      # Phase for custom application tasks that are to be run
      # after cluster shutdown and before ActorSystem termination.
      before-actor-system-terminate {
        depends-on = [cluster-shutdown]
      }

      # Last phase. See terminate-actor-system and exit-jvm above.
      # Don't add phases that depends on this phase because the
      # dispatcher and scheduler of the ActorSystem have been shutdown.
      # This phase is not meant for users to add tasks to.
      actor-system-terminate {
        timeout = 10 s
        depends-on = [before-actor-system-terminate]
      }
    }
  }

}
