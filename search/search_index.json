{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mantis","text":"<p>Mantis is a platform to build an ecosystem of realtime stream processing applications.</p> <p>Similar to micro-services deployed in a cloud, Mantis applications (jobs) are deployed on the Mantis platform. The Mantis platform provides the APIs to manage the life cycle of jobs (like deploy, update, and terminate), manages the underlying resources by containerizing a common pool of servers, and, similar to a traditional micro-service cloud, allows jobs to discover and communicate with other jobs.</p> <p>By providing stream processing As-a-Service, Mantis allows developers to simply focus on their business logic to build powerful and cost-effective streaming applications. </p>"},{"location":"#why-we-built-mantis","title":"Why We Built Mantis","text":"<p>Mantis evolved from the need to get better (faster and in-depth) operational insights in a rapidly growing, complex, micro-service ecosystem at Netflix. </p> <p></p> <p>As complexity of a system increases, our comprehension of the system rapidly decreases. In order to counter this complexity we need new approaches to operational insights.</p> <p>We need to change the way we generate and collect operational data:</p> <ul> <li>We should have access to raw events. Applications should be free to publish every single event. If we reduce the granularity at this stage, such as by pre-aggregating or sampling, then we\u2019re already at a disadvantage when it comes to getting insight since the data in its original form is already lost.</li> <li>We should be able to access this data in realtime. Operational use cases are inherently time-sensitive by nature. This becomes increasingly important with scale as the impact becomes much larger in less time.</li> <li>We should be able to ask new questions of this data without necessarily having to add new instrumentation to our applications. It\u2019s not possible to know ahead of time every single possible failure mode our systems might encounter despite all the rigor built in to make these systems resilient. When these failures do inevitably occur, it\u2019s important that we can derive new insights with this data.</li> </ul> <p>We need a new kind of execution environment:</p> <ul> <li>Can Process high volume data at low latency </li> <li>Has low Operational burden We need a managed platform where most of the operational tasks are handled automatically on behalf of the user. We don\u2019t need the additional overhead of operating our monitoring system.</li> <li>Is Elastic and Resilient We need a highly reliable system that can automatically recover from node failures and be able to scale the resources dynamically based on the data volume.</li> <li>Ecosystem of Streaming services A lot of use-cases often need the same data. By allowing jobs to discover each other and collaborate together by sharing data and results we can build cost-effective jobs that maximise code and data re-use.  </li> </ul> <p>We should be able to do all of the above in a cost-effective way. As our business critical systems scale, we need to make sure the systems in support of these business critical systems don\u2019t end up costing more than the business critical systems themselves.</p> <p>Mantis was built to meet all the above needs. It was designed by Netflix.</p>"},{"location":"#how-can-mantis-be-used","title":"How Can Mantis Be Used?","text":"<p>Mantis provides a robust, scalable platform that is ideally suited for high volume, low latency use cases like  anomaly detection and alerting. </p> <p>Mantis has been in production at Netflix since 2014. It processes trillions of events and peta-bytes of data every day.</p> <p>As a streaming microservices ecosystem, the Mantis platform provides engineers with capabilities to minimize  the costs of observing and operating complex distributed systems without compromising on operational insights.  Engineers can build cost-efficient real-time applications on top of Mantis to quickly identify issues, trigger alerts,  and apply remediations to minimize or completely avoid downtime to the Netflix service.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>To learn more about Mantis, visit the Concepts page or browse through the list of use cases powered by Mantis. To get involved with community, visit the Community page where you can subscribe to one of our mailing lists.</p>"},{"location":"community/","title":"Community","text":"<p>Mantis open source is hosted on Github. Issues are also tracked on Github and we prefer to receive contributions as pull requests.</p>"},{"location":"community/#contributing","title":"Contributing","text":"<p>Mantis components are spread across a few different repos. See the Components section in the README for more information.</p> <p>Before submitting a pull request, make sure to follow the instructions on the repo's <code>CONTRIBUTING.md</code> file.</p>"},{"location":"community/#mailing-lists","title":"Mailing Lists","text":"<ol> <li>Developers: mantis-oss-dev@googlegroups.com - used for discussing Mantis development.</li> <li>Users: mantis-oss-users@googlegroups.com - used for community discussions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-mantis-just-another-stream-processing-engine-like-flink-or-spark","title":"Is Mantis just another stream processing engine like Flink or Spark ?","text":"<p>Mantis can be thought of as a stream processing engine + a container cloud for execution.</p> <p>Mantis jobs do not run in isolation but are part of an ecosystem of jobs. This allows jobs to collaborate with other jobs to form complex streaming microservices system. Allowing jobs to communicate with other jobs promotes code and data reuse thus improving efficiency and productivity as users do not have to <code>re-invent the wheel</code>.</p> <p>Additionally, Mantis jobs can, with the help of the mantis-publish library, source data directly from production systems in an on-demand fashion. This provides a cost effective option to access rich operational data which otherwise would be prohibitive to transport and store (Imagine having to transport and store request and response bodies for every single request)</p> <p>So the answer is a qualified <code>yes</code> but it is much more than a stream processing engine.   </p>"},{"location":"faq/#does-mantis-process-events-in-micro-batches-or-event-a-time","title":"Does Mantis process events in micro batches or event a time ?","text":"<p>By default Mantis Jobs process events one at a time. Users can however create their own micro batches using ReactiveX operators.</p>"},{"location":"faq/#what-kind-of-message-guarantees-does-mantis-support","title":"What kind of message guarantees does Mantis support ?","text":"<p>The default delivery semantic is at-most once. This is intentional as the majority of the use cases on Mantis are latency sensitive (think alerting.) where determining an aggregate trend in subsecond latency is preferred to data completeness albeit with potential delay. </p> <p>In cases where the source of data is Kafka, Mantis supports at-least once semantics.</p> <p>The exactly-once or effectively once semantic support is currently not available in Open Source.</p>"},{"location":"faq/#do-mantis-jobs-operate-in-event-time-or-clock-time","title":"Do Mantis Jobs operate in event time or clock time ?","text":"<p>By default Mantis jobs operate with clock time. </p>"},{"location":"faq/#how-does-mantis-deal-with-backpressure","title":"How does Mantis deal with backpressure ?","text":"<p>Mantis relies on the backpressure support built-in to RxJava. This ensures an upstream operator in the execution DAG will not send more events down until the downstream  consumer is ready. </p> <p>Thus backpressure bubbles up along the DAG to the edge where data is being fetched from across the network. At this point, the strategy depends on whether the external source of data is <code>cold</code> source like a queue or  database or a <code>hot</code> source like data sent via [mantis-publish].</p> <p>For [cold] sources like Kafka no more events are read until the previous batch of events have been acknowledged. For [hot] sources events are first buffered and then as a last resort dropped.</p> <p>One effective way to deal with backpressure in Mantis, is to enable [autoscale] on the job with a strategy to scale up on backpressure. </p>"},{"location":"faq/#is-it-possible-to-use-the-second-stage-of-a-three-stage-mantis-job-as-the-source-job-for-another-mql-job","title":"Is it possible to use the second [stage] of a three-stage [Mantis Job] as the [source job] for another [MQL] job?","text":"<p>Jobs connect to the [sink] stage of other jobs by default, so while it may be technically possible to rig up a solution where you connect to an intermediate stage instead, this is not supported as an out-of-the-box solution. You may instead want to reorganize your original job, perhaps splitting it into two jobs.</p>"},{"location":"glossary/","title":"Glossary","text":"artifact file Each [Mantis Job] has an associated artifact file that contains its source code and JSON configuration. autoscaling You can establish an autoscaling policy for each [component] of your [Mantis Job] that governs how Mantis adjusts the number of [workers] assigned to that component as its workload changes.\u21d2 See Mantis Job Autoscaling backpressure Backpressure refers to a set of possible strategies for coping with [ReactiveX]  [Observables] that produce items more rapidly than their observers consume them.\u21d2 See ReactiveX.io: backpressure operators Binary compression has a particular meaning in the Mantis context... see (Connecting to a Source Job)[/writingjobs/source#connecting-to-a-source-job] Broadcast mode In broadcast mode, each [worker] of your [Mantis Job] gets all the data from all workers of the [Source Job] rather than having that data distributed equally among the workers of your Job.\u21d2 See Source Job Sources: Broadcast Mode Cassandra Apache Cassandra is an open source, distributed database management system.\u21d2 See Apache Cassandra Documentation. Cluster A Job Cluster is a containing entity for [Mantis Jobs]. It defines metadata and certain [service-level agreements]. Job Clusters ease job lifecycle management and job revisioning. A Mantis Cluster is a group of cloud container instances that hold your Mantis resources. cold Observables A cold [ReactiveX]  [Observable] waits until an observer subscribes to it before it begins to emit items. This means the observer is guaranteed to see the whole Observable sequence from the beginning. This is in contrast to a hot Observable, which may begin emitting items as soon as it is created, even before observers have subscribed to it. component A Mantis [Job] is composed of three types of component: a [Source], one or more [Processing Stages], and a [Sink]. Custom source In contrast to a [Source Job], which is a built-in variety of [Source]  [component] designed to pull data from a common sort of data source, a custom source typically accesses data from less-common sources or has unusual delivery guarantee semantics. Executor The stage executor is responsible for loading the bytecode for a [Mantis Job] and then executing its [stages] and [workers] in a coordinated fashion. In the [Mesos] UI, workers are also referred to as executors. Fenzo Fenzo is a Java library that implements a generic task scheduler for [Mesos] frameworks.\u21d2 See the Fenzo documentation. grouped data Grouped (or keyed) data is distinguished from [scalar] data in that each datum is accompanied by a key that indicates what group it belongs to. Grouped data can be processed by a [RxJava]  <code>GroupedObservable</code> or by a <code>MantisGroup</code>. GRPC GRPC is an open-source RPC framework using Protocol Buffers.\u21d2 See GRPC.io hot Observables A hot [ReactiveX]  [Observable] may begin emitting items as soon as it is created, even before observers have subscribed to it. This means the observer may miss items that were emitted before the observer subscribed. This is in contrast to a cold Observable, which waits until an observer subscribes to it before it begins to emit items. JMC Java Mission Control is a tool from Oracle with which developers can monitor and manage Java applications.\u21d2 See Java Components Job A Mantis Job takes in a stream of data, transforms it by using [RxJava] operators, and then outputs the results as another stream. It is composed of a [Source], one or more [Processing Stages], and a [Sink].\u21d2 See Writing Mantis Jobs Job Cluster see [Cluster] Job Master If a job is configured with [autoscaling], Mantis will add a Job Master component to it as its initial component. This component will send metrics back to Mantis to help it govern the autoscaling process. Kafka Apache Kafka is a large-scale, distributed streaming platform.\u21d2 See Apache Kafka. keyed data see [grouped data] Keystone Keystone is Netflix\u2019s data backbone, a stream processing platform that focuses on data analytics.\u21d2 See Keystone Real-time Stream Processing Platform label A label is a text key/value pair that you can add to a [Job Cluster] or to an individual [Job] to make it easier to search for or group. Log4j Log4j is a Java-based logging framework.\u21d2 See Apache Log4j Mantis Master The Mantis Master coordinates the execution of [Mantis Jobs] and starts the services on each [Worker]. Mesos Apache Mesos is an open-source technique for balancing resources across frameworks in clusters.\u21d2 See Apache Mesos Metadata Mantis inserts metadata into its [Job] payload. This may include information about where the data came from, for instance. You can define additional metadata to include in the payload when you establish the [Job Cluster]. meta message A [Source Job] may occasionally inject meta messages into its data stream that indicate things like data drops. migration strategy define Mantis Publish Mantis Publish (internally at Netflix known as Mantis Realtime Events or MRE) is a library that your application can use to stream events into Mantis while respecting [MQL] filters.\u21d2 See MQL. MQL You use Mantis Query Language to define filters and other data processing that Mantis applies to a [Source] data stream at its point of origin, so as to reduce the amount of data going over the wire. Observable In [ReactiveX] an <code>Observable</code> is the method of processing a stream of data in a way that facilitates its [transformation] and consumption by observers. Observables come in [hot] and [cold] varieties. There is also a <code>GroupedObservable</code> that is specialized to [grouped] data.\u21d2 See ReactiveX.io: Observable. Parameter A [Mantis Job] may accept parameters that modify its behavior. You can define these in your [Job Cluster] definition, and set their values on a per-Job basis. Processing Stage A Processing Stage component of a Mantis [Job] transforms the [RxJava]  [Observables] it obtains from the [Source] component. A Job with only one Processing Stage is called a single-stage Job.\u21d2 See The Processing Stage Component property A property is a particular named data value found within events in an event stream. Reactive Streams Reactive Streams is the latest advance of the [ReactiveX] project. It is an API for manipulating streams of asynchronous data in a non-blocking fashion, with [backpressure].\u21d2 See Reactive Streams. ReactiveX ReactiveX is a software technique for transforming, combining, reacting to, and managing streams of data. [RxJava] is an example of a library that implements this technique.\u21d2 See reactivex.io. RxJava RxJava is the Java implementation of [ReactiveX], a software technique for transforming, combining, reacting to, and managing streams of data.\u21d2 See reactivex.io. sampling Sampling is an MQL strategy for mitigating data volume issues. There are two sampling strategies: Random and Sticky. Random sampling uniformly downsamples the source stream to a percentage of its original volume. Sticky sampling selectively samples data from the source stream based on key values.\u21d2 See MQL: Sampling scalar data Scalar data is distinguished from keyed or [grouped] data in that it is not categorized into groups by key. Scalar data can be processed by an ordinary [ReactiveX]  [Observable]. Sink The Sink is the final component of a Mantis [Job]. It takes the [Observables] that has been transformed by the [Processing Stage] and outputs it in the form of a new data stream.\u21d2 See The Sink Component SLA A service-level agreement, in the Mantis context, is defined on a per-[Cluster] basis. You use it to configure how many [Jobs] in the cluster will be in operation at any time, among other things. Source The Source component of a Mantis [Job] fetches data from a source outside of Mantis and makes it available to the [Processing Stage] component in the form of an [RxJava]  [Observable]. There are two varieties of Source: a [Source Job] and a [custom source].\u21d2 See The Source Component Source Job A Source Job is a Mantis [Job] that you can use as a [Source], which wraps a data source external to Mantis and makes it easier for you to create a job that observes its data.\u21d2 See Mantis Source Jobs server-sent events Server-sent events (SSE) are a way for a browser to receive automatic updates from a server through an HTTP connection. Mantis includes an SSE [Sink].\u21d2 See Wikipedia: Server-sent events Transformation A transformation acts on each datum from a stream or [Observables] of data, changing it in some manner before passing it along as a new stream or Observable. Transformations may change data between [scalar] and [grouped] forms. transient jobs A transient (or ephemeral) [Mantis Job] is automatically killed by Mantis after a certain amount of time has passed since the last subscriber to the job disconnects. WebSocket WebSocket is a two-way, interactive communication channel that works over HTTP. In the Mantis context, it is an alternative to [SSE].\u21d2 See WebSocket.org. Worker A worker is the smallest unit of work that is scheduled within a Mantis [component]. You can configure how many resources Mantis allocates to each worker, and Mantis will adjust the number of workers your Mantis component needs based on its [autoscaling] policy. Zookeeper Apache Zookeeper is an open-source server that maintains configuration information and other services required by distributed applications.\u21d2 See Apache ZooKeeper"},{"location":"develop/connectors/iceberg/","title":"Iceberg (Beta)","text":""},{"location":"develop/connectors/iceberg/#sink","title":"Sink","text":"<p>Add this package to your dependencies:</p> <pre><code>implementation \"io.mantisrx:mantis-connector-iceberg:1.2.+\"\n</code></pre> <p>The Iceberg Sink has two components: <code>Writers</code> and a <code>Committer</code>.</p>"},{"location":"develop/connectors/iceberg/#writers","title":"Writers","text":"<p>Writers write Iceberg <code>Record</code>s to files in a specific file format. Writers periodically stage their data by flushing their underlying files to produce metadata in the form of <code>DataFile</code>s which are sent to Committers.</p> <p>Add an Iceberg Writer using one of the following approaches:</p>"},{"location":"develop/connectors/iceberg/#separate-processing-stage","title":"Separate Processing Stage","text":"<p>Use this approach to decouple your application logic from Iceberg writing logic to may make it easier to debug your Mantis Job. This approach incurs extra encode/decode and network cost to move data between workers.</p> <pre><code>public class ExampleIcebergSinkJob extends MantisJobProvider&lt;Map&lt;String, Object&gt;&gt; {\n\n@Override\npublic Job&lt;Map&lt;String, Object&gt;&gt; getJobInstance() {\nreturn MantisJob.source(&lt;source that produces MantisServerSentEvents&gt;)\n.stage(...)                                                           // (0)\n.stage(new IcebergWriterStage(), IcebergWriterStage.config())         // (1)\n.stage(new IcebergCommitterStage(), IcebergCommitterStage.config())   // (2)\n.sink(...)\n.lifecycle(NetflixLifecycles.governator(\n&lt;job file properties name&gt;,\nnew IcebergModule()))                                             // (3)\n.create();\n}\n}\n</code></pre> <p>(0) A series of Processing Stages that you define for your application logic. Produces an Iceberg Record. To emit Iceberg Records out of your separate Processing Stage, add the Iceberg Record Codec to your stage config.</p> <p>(1) A Processing Stage of n parallelizable <code>IcebergWriter</code>s. The Stage Config automatically adds an Iceberg DataFile Codec to encode/decode DataFiles between these workers and the Committer workers from the next stage downstream.</p> <p>(2) A Processing Stage for 1 <code>IcebergCommitter</code>.</p> <p>(3) A module for injecting dependencies and configs required to authenticate/connect/interact with backing Iceberg infrastructure.</p>"},{"location":"develop/connectors/iceberg/#with-an-existing-processing-stage","title":"With an existing Processing Stage","text":"<p>Use this approach to avoid incurring encode/decode and network costs. This approach may make your Mantis Job more difficult to debug.</p> <pre><code>public class ExampleIcebergSinkJob extends MantisJobProvider&lt;Map&lt;String, Object&gt;&gt; {\n\n@Override\npublic Job&lt;Map&lt;String, Object&gt;&gt; getJobInstance() {\nreturn MantisJob.source(&lt;source that produces MantisServerSentEvents&gt;)\n.stage(new ProcessingAndWriterStage(), &lt;config&gt;)                      // (0)\n.stage(new IcebergCommitterStage(), IcebergCommitterStage.config())   // (1)\n.sink(...)\n.lifecycle(NetflixLifecycles.governator(\n&lt;job file properties name&gt;,\nnew IcebergModule()))                                             // (2)\n.create();\n}\n}\n\n/**\n * Example class which takes in MantisServerSentEvents, performs some logic,\n * produces Iceberg Records, writes the Records to files,\n * and produces DataFiles for a downstream Iceberg Committer.\n */\npublic class ProcessingAndWriterStage implements ScalarComputation&lt;MantisServerSentEvent, DataFile&gt; {\n\nprivate Transformer transformer;\npublic static ScalarToScalar.Config&lt;Record, DataFile&gt; config() {\nreturn new ScalarToScalar.Config&lt;Record, DataFile&gt;()\n.description(\"\")\n.codec(IcebergCodecs.dataFile())                                        // (3)\n.withParameters(...);\n}\n\n@Override\npublic void init(Context context) {\ntransformer = IcebergWriterStage.newTransformer(context);                   // (4)\n}\n\n@Override\npublic Observable&lt;DataFile&gt; call(Context context, Observable&lt;Record&gt; recordObservable) {\nreturn recordObservable\n.map(&lt;some application logic&gt;)\n.map(&lt;some more application logic&gt;)\n.compose(transformer);                                                  // (5)\n}\n}\n</code></pre> <p>(0) A series of Processing Stages for your application logic and Iceberg writing logic. You may further reduce network cost by combining your Processing Stage(s) logic into your Source.</p> <p>(1) A Processing Stage for 1 <code>IcebergCommitter</code>.</p> <p>(2) A module for injecting dependencies and configs required to authenticate/connect/interact with backing Iceberg infrastructure.</p> <p>(3) Remember to add an Iceberg DataFile Codec to emit <code>DataFile</code>s to the Committer.</p> <p>(4) Create a new Iceberg Writer Transformer from the static method provided by <code>IcebergWriterStage</code>.</p> <p>(5) Compose the transformer with your application logic Observable.</p> <p>Note</p> <p>Writers are stateless and may be parallelized/autoscaled.</p>"},{"location":"develop/connectors/iceberg/#committer","title":"Committer","text":"<p>The Committer commits <code>DataFile</code>s to Iceberg. <code>Records</code> are queryable from the Iceberg table only after a Committer commits <code>DataFile</code>s. A Committer commits on a configured interval (default: 5 min).</p> <p>If a Committer fails, it will retry until a retry threshold is met, after which it will continue onto the next window of <code>Record</code>s. This avoids backpressure issues originating from downstream consumers which makes it more suitable for operational use cases.</p> <pre><code>public class ExampleIcebergSinkJob extends MantisJobProvider&lt;Map&lt;String, Object&gt;&gt; {\n\n@Override\npublic Job&lt;Map&lt;String, Object&gt;&gt; getJobInstance() {\nreturn MantisJob.source(&lt;source that produces Iceberg Records&gt;)\n.stage(...)\n.stage(new IcebergWriterStage(), IcebergWriterStage.config())\n.stage(new IcebergCommitterStage(), IcebergCommitterStage.config())   // (1)\n.sink(...)\n.lifecycle(NetflixLifecycles.governator(\n&lt;job file properties name&gt;,\nnew IcebergModule()))\n.create();\n}\n}\n</code></pre> <p>(1) A Processing Stage for 1 <code>IcebergCommitter</code>. The Committer outputs a <code>Map</code> which you can subscribe to with a Sink for optional debugging or connecting to another Mantis Job. Otherwise, a Sink not required because the Committer will write directly to Iceberg.</p> <p>Important</p> <p>You should try to have only one Committer per Iceberg Table and try to avoid a high frequency commit intervals (default: <code>5 min</code>). This avoids commit pressure on Iceberg.</p>"},{"location":"develop/connectors/iceberg/#configuration-options","title":"Configuration Options","text":"Name Type Default Description <code>writerRowGroupSize</code> int 100 Number of rows to chunk before checking for file size <code>writerFlushFrequencyBytes</code> String \"134217728\" (128 MiB) Flush frequency by size in Bytes <code>writerFlushFrequencyMsec</code> String \"60000\" (1 min) Flush frequency by time in milliseconds <code>writerFileFormat</code> String parquet File format for writing data files to backing Iceberg store <code>writerMaximumPoolSize</code> int 5 Maximum number of writers that should exist per worker <code>commitFrequencyMs</code> String \"300000\" (5 min) Iceberg Committer frequency by time in milliseconds"},{"location":"develop/connectors/iceberg/#metrics","title":"Metrics","text":"<p>Prefix: <code>io.mantisrx.connector.iceberg.sink.writer.metrics.WriterMetrics</code></p> Name Type Description <code>openSuccessCount</code> Counter Number of times a file was successfully opened <code>openFailureCount</code> Counter Number of times a file failed to open <code>writeSuccessCount</code> Counter Number of times an Iceberg <code>Record</code> was successfully written to a file <code>writeFailureCount</code> Counter Number of times an Iceberg <code>Record</code> failed to be written to a file <code>batchSuccessCount</code> Counter Number of times a file was successfully flushed to produce a <code>DataFile</code> <code>batchFailureCount</code> Counter Number of times a file failed to flush <code>batchSize</code> Gauge Number of Iceberg <code>Records</code> per writer flush as described within a <code>DataFile</code> <code>batchSizeBytes</code> Gauge Cumulative size of Iceberg <code>Records</code> from a writer flush <p>Prefix: <code>io.mantisrx.connector.iceberg.sink.committer.metrics.CommitterMetrics</code></p> Name Type Description <code>invocationCount</code> Counter Number of times a commit was invoked <code>commitSuccessCount</code> Counter Number of times a commit was successful <code>commitFailureCount</code> Counter Number of times a commit failed <code>commitLatencyMsec</code> Gauge Time it took to perform the most recent commit <code>commitBatchSize</code> Gauge Cumulative size of <code>DataFile</code>s from a commit"},{"location":"develop/ingestion/netty/","title":"Netty","text":"<p>You can stream events on-demand from your application to be ingested directly into Mantis using the Mantis Publish library (sometimes casually referred to as \"MRE\").</p> <p>Mantis Publish takes care of filtering the events you send into Mantis, and it will only transmit them over the network if a downstream consumer that is interested in such events is currently subscribed.</p> <p>Mantis Publish contains a subscription registry where each client subscription is represented by its corresponding Mantis Query Language query. Mantis Publish evaluates all MQL queries from its subscription registry against each event as the event is generated by your application. It then tags events with all of the matching MQL queries and enriches the event with a superset of fields from the matched queries. That is, rather than emitting n events for n matching MQL queries, Mantis Publish will instead emit a single event containing all of the fields requested by the matched queries. This happens directly within your application before any events are sent over the network into Mantis. Further, Mantis Publish only dispatches events if a client is subscribed with a matching query. This means that you can freely produce events without incurring the cost until an active subscription exists.</p>"},{"location":"develop/ingestion/netty/#choose-a-package","title":"Choose a Package","text":"<p>MRE works with both Guice-enabled and standalone injectors.</p>"},{"location":"develop/ingestion/netty/#guice-based-injector","title":"Guice-based Injector","text":"<pre><code>implementation 'io.mantisrx:mantis-publish-netty-guice:1.2.+'\n</code></pre>"},{"location":"develop/ingestion/netty/#enable-the-publisher-client","title":"Enable the Publisher Client","text":"<p>Inject the <code>MantisRealtimeEventsPublishModule</code> into your application. In addition to injecting <code>MantisRealtimeEventsPublishModule</code> you will also need to add the  <code>ArchaiusModule</code> and the <code>SpectatorModule</code> if not already injected.</p> <pre><code>Injector injector = Guice.createInjector(\nnew MyBasicModule(),\nnew ArchaiusModule(),\nnew MantisRealtimeEventsPublishModule(),\nnew SpectatorModule());\n</code></pre>"},{"location":"develop/ingestion/netty/#standalone-injector","title":"Standalone Injector","text":"<pre><code>implementation 'io.mantisrx:mantis-publish-netty:1.2.+'\n</code></pre>"},{"location":"develop/ingestion/netty/#enable-the-publisher-client_1","title":"Enable the Publisher Client","text":"<p>Follow the example standalone initializer to manually inject dependencies. Once you have constructed a <code>MrePublishClientInitializer</code>, call the <code>MantisPublishClientInitializer#start</code> method to initialize underlying components.</p>"},{"location":"develop/ingestion/netty/#configure-where-to-send-svents","title":"Configure Where to Send Svents","text":"<p>You will need to configure the location of the Mantis API server for the mantis-publish library to bootstrap.</p> <p>Add the following properties to your <code>application.properties</code>:</p> <pre><code>mantis.publish.discovery.api.hostname=&lt;IP of Mantis API&gt;\n\n# mantis api port\nmantis.publish.discovery.api.port=&lt;port for Mantis API&gt;\n\n# This application's name\nmantis.publish.app.name=JavaApp\n</code></pre>"},{"location":"develop/ingestion/netty/#send-events-into-mantis","title":"Send Events into Mantis","text":"<p>For each event your application wishes to send to Mantis, create a <code>Event</code> object with your desired event fields, and pass that <code>Event</code> to the <code>MantisEventPublisher#publish</code> method.</p> <p>For example:</p> <pre><code>// Create an `Event` for Mantis Publish using your application event.\nfinal Event event = new Event();\nevent.set(\"testKey\", \"testValue\");\n// Send your `Event` into Mantis.\n// Note: This event will only be dispatched over the network\n// if a subscription with a matching MQL query exists.\neventPublisher.publish(event);\n</code></pre>"},{"location":"develop/ingestion/netty/#consuming-a-mantis-stream","title":"Consuming a Mantis Stream","text":"<p>Visit the Querying page for details on how you can consume your application's event stream.</p>"},{"location":"develop/ingestion/netty/#configuration-options","title":"Configuration Options","text":"<p>There are a number of configuration options available to control the behavior of the publishing client.</p> <p>Prefix:</p> <p><code>mantis.publish</code></p> Name Default Description <code>enabled</code> true Determine if event processing is enabled. <code>tee.enabled</code> false Allows events to simultaneously be sent to an external system outside of Mantis <code>tee.stream</code> default_stream Specifies which external stream name tee will write to <code>blacklist</code> param.password Comma separated list of field names where the value will be obfuscated <code>max.num.streams</code> 5 Maximum number of streams this application can create <code>stream.inactive.duration.threshold.sec</code> 86400 (24 hours) Maximum duration in seconds for the stream to be considered inactive if there are no events <code>&lt;stream name&gt;.stream.queue.size</code> 1000 Size of the blocking queue to hold events to be pushed for the specific stream <code>max.subscriptions.per.stream.default</code> 20 Default maximum number of subscriptions per stream. After the limit is reached, further subscriptions on that stream are rejected <code>max.subscriptions.stream.&lt;stream name&gt;</code> 20 Overrides the default maximum number of subscriptions for the specific stream <code>subs.refresh.interval.sec</code> 1 Interval in seconds when subscriptions are fetched. In the default implementation, subscriptions are fetched over http from the workers returned by Discovery API <code>drainer.interval.msec</code> 100 Interval in milliseconds when events are drained from the stream queue and delegated to underlying transmitter for sending <code>subs.expiry.interval.sec</code> 300 Duration in seconds between a subscription is last fetched and when it is removed <code>jobdiscovery.refresh.interval.sec</code> 10 Duration in seconds between workers are refreshed for a job cluster <code>jobcluster.mapping.refresh.interval.sec</code> 60 Duration in seconds between job cluster mapping is refreshed for the current application <code>deepcopy.eventmap.enabled</code> true Determine if event processing should operate on a deep copy of the event. Otherwise the event object is processed directly <code>subs.refresh.max.num.workers</code> 3 Maximum number of mantis workers to fetch subscription from. Workers are randomly chosen from the list returned by Discovery API <code>subs.fetch.query.params.string</code> \"\" Additional query params to pass to the api call to fetch subscription. It should be of the form \"param1=value1&amp;param2=value2\" <code>channel.gzip.enabled</code> true Netty channel configuration for pushing events. Determine if events should be gzip encoded when send over the channel <code>channel.idleTimeout.sec</code> 300 Netty channel configuration for pushing events. Write idle timeout in seconds for the channel <code>channel.writeTimeout.sec</code> 1 Netty channel configuration for pushing events. Write timeout in seconds for the channel <code>channel.httpChunkSize.bytes</code> 32768 (32 KiB) Netty channel configuration for pushing events. Chunked size in bytes of the channel content. It is used by HttpObjectAggregator <code>channel.flushInterval.msec</code> 50 Netty channel configuration for pushing events. Maximum duration in milliseconds between content flushes <code>channel.flushInterval.bytes</code> 524288 Netty channel configuration for pushing events. Content is flushed when aggregated event size is above this threshold <code>channel.lowWriteBufferWatermark.bytes</code> 1572864 Netty channel configuration for pushing events. Used for setting write buffer watermark <code>channel.highWriteBufferWatermark.bytes</code> 2097152 Netty channel configuration for pushing events. Used for setting write buffer watermark <code>channel.ioThreads</code> 1 Netty channel configuration for pushing events. Number of threads in the eventLoopGroup <code>channel.compressionThreads</code> 1 Netty channel configuration for pushing events. Number of threads in the encoderEventLoopGroup when gzip is enabled <code>workerpool.capacity</code> 1000 Size of the pool of Mantis workers to push events to <code>workerpool.refresh.internal.sec</code> 10 Duration in seconds between Mantis workers are refreshed in the pool <code>workerpool.worker.error.quota</code> 60 Number of errors to receive from a Mantis worker before it is blacklisted in the pool <code>workerpool.worker.error.timeout.sec</code> 300 Duration in seconds after which a blacklisted Mantis worker may be reconsidered for selection"},{"location":"develop/querying/mql/","title":"Mantis Query Language","text":"<p>Mantis Query Language (MQL) is a dialect of SQL implemented as an abstraction over RxJava Observables. The purpose of MQL is to make it easy for Mantis users to query, transform, and analyze data flowing through Mantis. MQL maintains high fidelity to SQL syntax while adding capabilities for dealing with JSON structure, as well as a rich set of aggregates for answering analytical queries about the data in question.</p>"},{"location":"develop/querying/mql/#consuming-a-mantis-stream","title":"Consuming a Mantis Stream","text":"<p>There are a few ways to consume a Mantis Stream.</p>"},{"location":"develop/querying/mql/#using-the-low-level-library","title":"Using the Low-Level Library","text":"<p>To use MQL, include the mql-jvm library in your dependencies.</p> <p>Refer to the example below for a minimalist getting started guide. Imagine we have a source of <code>Map&lt;String, Object&gt;</code> representing your data (recall that Mantis events can be easily parsed as such). Executing a query against that Observable is only a matter of putting that <code>Observable</code> into a <code>Map&lt;String, Observable&gt;</code> to represent the context and calling <code>evalMql</code> against said context. The result will be an <code>Observable</code> that represents the results of the query:</p> <pre><code>package my.package;\nimport java.util.HashMap;\n\nclass MqlExample {\npublic static void main(String[] args) {\n// Create a test observable source of x, y coordinates.\nObservable&lt;HashMap&lt;String, Object&gt;&gt; source = Observable.interval(100, TimeUnit.MILLISECONDS).map(x -&gt; {\nHashMap&lt;String, Object&gt; d = new HashMap&lt;&gt;();\nd.put(\"x\", x);\nd.put(\"y\", x);\n});\nHashMap&lt;String, Observable&lt;HashMap&lt;String, Object&gt;&gt; context = new HashMap&lt;&gt;();\ncontext.put(\"observations\", source);\n// You don't have to block, and shouldn't. It is just to keep the example running.\nio.mantisrx.mql.Core.evalMql(\"select y from observations where x &gt; 50 OR y == 10\", context).toBlocking().forEach(System.out::println);\n}\n</code></pre>"},{"location":"develop/querying/sampling/","title":"Sampling","text":"<p>Sampling in MQL mitigates data volume issues. There are two sampling strategies \u2014 Random and Sticky:</p>"},{"location":"develop/querying/sampling/#random-sampling","title":"Random Sampling","text":"<p>Random sampling uniformly downsamples the stream to a percentage of its original volume. You establish random sampling through a sampling clause like the following:</p> <pre><code>select * from stream SAMPLE {'strategy': 'RANDOM', 'threshold': 200, 'factor': 10000}\n</code></pre> <p>For each item in the stream, a numeric hash is generated. That hash is modded by <code>factor</code> to produce a result between 0 (inclusive) and <code>factor</code> (exclusive). If that result is less than <code>threshold</code> the item will be sampled, otherwise it will be ignored.</p> <p>You can determine the sampling percentage by remembering that \\frac{threshold}{factor} values will be sampled \u2014 2% in the example above with a <code>threshold</code> of 200 and <code>factor</code> of 10000.</p> <p>You can also set a <code>salt</code> value, which will change the calculation of the hash. This is helpful if you want to sample over the same set of values but retrieve a different sample of those values. For example:</p> <pre><code>select * from stream SAMPLE {'strategy': 'RANDOM', 'threshold': 200, 'factor': 10000, 'salt': 123}\n</code></pre>"},{"location":"develop/querying/sampling/#sticky-sampling","title":"Sticky Sampling","text":"<p>Sticky sampling \u201csticks\u201d to certain values for the provided keys. That is if you are sampling on \u201c<code>zipcode</code>\u201d (as in the following example) and you observe a specific <code>zipcode</code> in the stream, you will observe all events which contain that specific <code>zipcode</code>. Sticky sampling can be achieved with a query like such:</p> <pre><code>select * from stream SAMPLE {'strategy':'STICKY', 'keys':['zipcode'], 'threshold':200, 'factor':10000, 'salt': 1}\n</code></pre> <p>The query above should retrieve 2% of the total stream (see Random Sampling above for why), predicated on the events being uniformly distributed over the <code>zipcode</code>s.</p>"},{"location":"develop/writing-jobs/group-by/","title":"Writing Your Third Mantis Job: Group By / Aggregate","text":"<p>NOTE: This tutorial is a work in progress.</p> <p>Until now we've run single stage Mantis jobs which a can run in a single process / container. Much of the power provided by Mantis is that we can design and implement a distributed job. Let's take a look at the groupby-sample job definition and then break it down stage by stage. </p> <pre><code>    @Override\npublic Job&lt;String&gt; getJobInstance() {\n\nreturn MantisJob\n// Stream Request Events from our random data generator source\n.source(new RandomRequestSource())\n\n// Groups requests by path\n.stage(new GroupByStage(), GroupByStage.config())\n\n// Computes count per path over a window\n.stage(new AggregationStage(), AggregationStage.config())\n\n// Collects the data and makes it availabe over SSE\n.stage(new CollectStage(), CollectStage.config())\n\n// Reuse built in sink that eagerly subscribes and delivers data over SSE\n.sink(Sinks.eagerSubscribe(\nSinks.sse((String data) -&gt; data)))\n\n.metadata(new Metadata.Builder()\n.name(\"GroupByPath\")\n.description(\"Connects to a random data generator source\"\n+ \" and counts the number of requests for each uri within a window\")\n.build())\n.create();\n\n}\n</code></pre> <p>The job definition above should look relatively familiar with the exception of the fact that our job has three stages. These stages and their configurations are all relatively simple but take advantage of the scalability of Mantis when used in conjunction with each other;</p> <ul> <li>The <code>GroupByStage</code> will group events according to some user specified criteria.</li> <li>The <code>AggregationStage</code> will perform an aggregation on a per-group basis.</li> <li>The <code>CollectStage</code> will collect all aggregations and create a report.</li> </ul> <p>Let's explore each of these stages in sequence.</p>"},{"location":"develop/writing-jobs/group-by/#stage-1-group-by-stage","title":"Stage 1: Group By Stage","text":"<p>The GroupByStage implements <code>ToGroupComputation&lt;RequestEvent, String, RequestEvent&gt;</code> which tells Mantis that the <code>call</code> method will be returning <code>MantisGroup&lt;String, RequestEvent&gt;</code> which represents a group key and the data. You may recall in the previous tutorials we used the reactive <code>groupBy</code> operator to group data. The reactive operator performs an in-memory group by which has some scaling limitations. Grouping data using a Mantis stage does not have the same limitations and allows us to scale the group by operation across multiple containers.</p> <p>The <code>config()</code> method also changes to specify this different stage type. It also allows us to specify <code>concurrentInput()</code> on the config allowing the <code>call</code> method to be run concurrently in this container. Also note the result of <code>getParameters()</code> is added to the config via the <code>withParameters</code> method, it isn't an interface method for stages but we're specifying parameters this way for convenience.</p> <pre><code>public class GroupByStage implements ToGroupComputation&lt;RequestEvent, String, RequestEvent&gt; {\n\nprivate static final String GROUPBY_FIELD_PARAM = \"groupByField\";\nprivate boolean groupByPath = true;\n@Override\npublic Observable&lt;MantisGroup&lt;String, RequestEvent&gt;&gt; call(Context context, Observable&lt;RequestEvent&gt; requestEventO) {\nreturn requestEventO\n.map((requestEvent) -&gt; {\nif(groupByPath) {\nreturn new MantisGroup&lt;&gt;(requestEvent.getRequestPath(), requestEvent);\n} else {\nreturn new MantisGroup&lt;&gt;(requestEvent.getIpAddress(), requestEvent);\n}\n});\n}\n\n@Override\npublic void init(Context context) {\nString groupByField = (String)context.getParameters().get(GROUPBY_FIELD_PARAM,\"path\");\ngroupByPath = groupByField.equalsIgnoreCase(\"path\") ? true : false;\n}\n\n/**\n     * Here we declare stage specific parameters.\n     * @return\n     */\npublic static List&lt;ParameterDefinition&lt;?&gt;&gt; getParameters() {\nList&lt;ParameterDefinition&lt;?&gt;&gt; params = new ArrayList&lt;&gt;();\n\n// Group by field\nparams.add(new StringParameter()\n.name(GROUPBY_FIELD_PARAM)\n.description(\"The key to group events by\")\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"path\")\n.build())   ;\n\nreturn params;\n}\n\npublic static ScalarToGroup.Config&lt;RequestEvent, String, RequestEvent&gt; config(){\nreturn new ScalarToGroup.Config&lt;RequestEvent, String, RequestEvent&gt;()\n.description(\"Group event data by path/ip\")\n.concurrentInput() // signifies events can be processed in parallel\n.withParameters(getParameters())\n.codec(RequestEvent.requestEventCodec());\n}\n}\n</code></pre> <p>We should note that this stage is horizontally scalable. We can run as many of these as necessary to handle the inflow of data which is how Mantis allows us to run a scalable group by operation.</p>"},{"location":"develop/writing-jobs/group-by/#stage-2-aggregation-stage","title":"Stage 2: Aggregation Stage","text":"<p>The previous stage produces <code>MantisGroup&lt;String, RequestEvent&gt;</code> by implementing <code>ToGroupComputation</code>. If we think of <code>ToGroupComputation</code> producing groups of data we can think of <code>GroupToScalarComputation&lt;K, T, R&gt;</code> as the inverse computing a scalar value from a group.</p> <p>The <code>init()</code>, <code>config()</code> and <code>getParameters()</code> methods should be familiar by now but let's take a closer look at the <code>call()</code> method. We're performing the same operation the stream as we did in the first two tutorials. Notice how we still group by <code>MantisGroup::getKeyValue</code>. This is because while Mantis guarantees that all data for an individual group from the previous stage will land on the same container in this stage, it does not invoke the <code>call()</code> method for each individual group and thus we need to handle the groups ourselves.</p> <pre><code>@Slf4j\npublic class AggregationStage implements GroupToScalarComputation&lt;String, RequestEvent, RequestAggregation {\n\npublic static final String AGGREGATION_DURATION_MSEC_PARAM = \"AggregationDurationMsec\";\nint aggregationDurationMsec;\n\n/**\n     * The call method is invoked by the Mantis runtime while executing the job.\n     * @param context Provides metadata information related to the current job.\n     * @param mantisGroupO This is an Observable of {@link MantisGroup} events. Each event is a pair of the Key -&gt; uri Path and\n     *                     the {@link RequestEvent} event itself.\n     * @return\n     */\n@Override\npublic Observable&lt;RequestAggregation&gt; call(Context context, Observable&lt;MantisGroup&lt;String, RequestEvent&gt;&gt; mantisGroupO) {\nreturn mantisGroupO\n.window(aggregationDurationMsec, TimeUnit.MILLISECONDS)\n.flatMap((omg) -&gt; omg.groupBy(MantisGroup::getKeyValue)\n.flatMap((go) -&gt; go.reduce(0, (accumulator, value) -&gt;  accumulator = accumulator + 1)\n.map((count) -&gt; RequestAggregation.builder().count(count).path(go.getKey()).build())\n.doOnNext((aggregate) -&gt; {\nlog.debug(\"Generated aggregate {}\", aggregate);\n})\n));\n}\n\n/**\n     * Invoked only once during job startup. A good place to add one time initialization actions.\n     * @param context\n     */\n@Override\npublic void init(Context context) {\naggregationDurationMsec = (int)context.getParameters().get(AGGREGATION_DURATION_MSEC_PARAM, 1000);\n}\n\n/**\n     * Provides the Mantis runtime configuration information about the type of computation done by this stage.\n     * E.g in this case it specifies this is a GroupToScalar computation and also provides a {@link Codec} on how to\n     * serialize the {@link RequestAggregation} events before sending it to the {@link CollectStage}\n     * @return\n     */\npublic static GroupToScalar.Config&lt;String, RequestEvent, RequestAggregation&gt; config(){\nreturn new GroupToScalar.Config&lt;String, RequestEvent,RequestAggregation&gt;()\n.description(\"sum events for a path\")\n.codec(RequestAggregation.requestAggregationCodec())\n.withParameters(getParameters());\n}\n\n/**\n     * Here we declare stage specific parameters.\n     * @return\n     */\npublic static List&lt;ParameterDefinition&lt;?&gt;&gt; getParameters() {\nList&lt;ParameterDefinition&lt;?&gt;&gt; params = new ArrayList&lt;&gt;();\n\n// Aggregation duration\nparams.add(new IntParameter()\n.name(AGGREGATION_DURATION_MSEC_PARAM)\n.description(\"window size for aggregation\")\n.validator(Validators.range(100, 10000))\n.defaultValue(5000)\n.build())   ;\n\nreturn params;\n}\n\n}\n</code></pre> <p>Much like the previous stage this stage is also horizontally scalable up to the cardinality of our group by. Allowing us to individually (or automatically via autoscaling) scale these two stages ensures this job can be correctly sized for the workload.</p>"},{"location":"develop/writing-jobs/group-by/#stage-3-collect-stage","title":"Stage 3: Collect Stage","text":"<p>The third stage collects data from all of the upstream workers and generates a report. We can see in the code below the stream is windowed for five seconds, then we flatmap a reduce over the window and invoke the <code>RequestAggregationAccumulator#generateReport()</code> method on the reduced value.</p> <pre><code>@Slf4j\npublic class CollectStage implements ScalarComputation&lt;RequestAggregation,String&gt; {\nprivate static final ObjectMapper mapper = new ObjectMapper();\n@Override\npublic Observable&lt;String&gt; call(Context context, Observable&lt;RequestAggregation&gt; requestAggregationO) {\nreturn requestAggregationO\n.window(5, TimeUnit.SECONDS)\n.flatMap((requestAggO) -&gt; requestAggO\n.reduce(new RequestAggregationAccumulator(),(acc, requestAgg) -&gt; acc.addAggregation(requestAgg))\n.map(RequestAggregationAccumulator::generateReport)\n.doOnNext((report) -&gt; {\nlog.debug(\"Generated Collection report {}\", report);\n})\n)\n.map((report) -&gt; {\ntry {\nreturn mapper.writeValueAsString(report);\n} catch (JsonProcessingException e) {\nlog.error(e.getMessage());\nreturn null;\n}\n}).filter(Objects::nonNull);\n}\n\n@Override\npublic void init(Context context) {\n\n}\n\npublic static ScalarToScalar.Config&lt;RequestAggregation,String&gt; config(){\nreturn new ScalarToScalar.Config&lt;RequestAggregation,String&gt;()\n.codec(Codecs.string());\n}\n\n/**\n     * The accumulator class as the name suggests accumulates all aggregates seen during a window and\n     * generates a consolidated report at the end.\n     */\nstatic class RequestAggregationAccumulator {\nprivate final Map&lt;String, Integer&gt; pathToCountMap = new HashMap&lt;&gt;();\n\npublic RequestAggregationAccumulator()  {}\n\npublic RequestAggregationAccumulator addAggregation(RequestAggregation agg) {\npathToCountMap.put(agg.getPath(), agg.getCount());\nreturn this;\n}\n\npublic AggregationReport generateReport() {\nlog.info(\"Generated report from=&gt; {}\", pathToCountMap);\nreturn new AggregationReport(pathToCountMap);\n}\n}\n}\n</code></pre> <p>Unlike the previous stages we only run a single collect stage which gathers all the data in five second batches and generates a report to be output on the sink.</p>"},{"location":"develop/writing-jobs/group-by/#conclusion","title":"Conclusion","text":"<p>We've now explored the concept of a multi-stage Mantis job which allows us to horizontally scale individual stages and express group by and aggregate semantics as a Mantis topology.</p>"},{"location":"develop/writing-jobs/twitter/","title":"Writing Your Second Mantis Job","text":"<p>Our first tutorial primed us for writing and executing a job end-to-end but it wasn't particularly interesting from a data perspective because it just repeatedly looped over the contents of a book. In this example we'll explore writing a more involved source which reads an infinite stream of data from Twitter and performs the same word count in real-time. Mantis jobs can easily subscribe to one another using some built in sources but the technique in this tutorial can be used to pull external data into the Mantis ecosystem.</p> <p>To proceed you'll need to head over to Twitter and grab yourself a pair of API keys.</p>"},{"location":"develop/writing-jobs/twitter/#the-source","title":"The Source","text":"<p>The source is responsible for ingesting data to be processed within the job. Many Mantis jobs will subscribe to other jobs and can simply use a templatized source such as <code>io.mantisrx.connectors.job.source.JobSource</code> which handles all the minutiae of connecting to other jobs for us. If however your job exists on the edge of Mantis it will need to pull data in via a custom source. Since we're reading from the Twitter API we'll need to do this ourselves.</p> <p>Our <code>TwitterSource</code> must implement <code>io.mantisrx.runtime.source.Source</code> which requires us to implement <code>call</code> and optionally <code>init</code>. Mantis provides some guarantees here in that <code>init</code> will be invoked exactly once and before <code>call</code> which will be invoked at least once. This makes <code>init</code> the ideal location to perform one time setup and configuration for the source and <code>call</code> the ideal location for performing work on the incoming stream. The objective of this entire class is to have <code>call</code> return an <code>Observable&lt;Observable&lt;T&gt;&gt;</code> which will be passed as a parameter to the first stage of our job.</p> <p>Let's deconstruct the <code>init</code> method first. Here we will extract our parameters from the <code>Context</code> -- this allows us to write more generic sources which can be templatized and reused across many jobs. This is a very common pattern for writing Mantis jobs and allows you to iterate quickly testing various configurations as jobs can be resubmitted easily with new parameters.</p> <pre><code>/**\n  * Init method is called only once during initialization. It is the ideal place to perform one time\n  * configuration actions.\n  *\n  * @param context Provides access to Mantis system information like JobId, Job parameters etc\n  * @param index   This provides access to the unique workerIndex assigned to this container. It also provides\n  *                the total number of workers of this job.\n  */\n@Override\npublic void init(Context context, Index index) {\n\nString consumerKey = (String) context.getParameters().get(CONSUMER_KEY_PARAM);\nString consumerSecret = (String) context.getParameters().get(CONSUMER_SECRET_PARAM);\nString token = (String) context.getParameters().get(TOKEN_PARAM);\nString tokenSecret = (String) context.getParameters().get(TOKEN_SECRET_PARAM);\nString terms = (String) context.getParameters().get(TERMS_PARAM);\n\nAuthentication auth = new OAuth1(consumerKey,\nconsumerSecret,\ntoken,\ntokenSecret);\n\nStatusesFilterEndpoint endpoint = new StatusesFilterEndpoint();\n\nString[] termArray = terms.split(\",\");\n\nList&lt;String&gt; termsList = Arrays.asList(termArray);\n\nendpoint.trackTerms(termsList);\n\nclient = new ClientBuilder()\n.name(\"twitter-source\")\n.hosts(Constants.STREAM_HOST)\n.endpoint(endpoint)\n.authentication(auth)\n.processor(new StringDelimitedProcessor(twitterObservable))\n.build();\n\n\nclient.connect();\n}\n</code></pre> <p>Our <code>call</code> method is very simple thanks to the fact that our twitter client writes to a custom <code>BlockingQueue</code> adapter that we've written. We simply need to return an <code>Observable&lt;Observable&lt;T&gt;&gt;</code>.</p> <pre><code>@Override\npublic Observable&lt;Observable&lt;String&gt;&gt; call(Context context, Index index) {\nreturn Observable.just(twitterObservable.observe());\n}\n</code></pre> <p>You may have noticed that our <code>init</code> method is pulling a bunch of parameters out of the <code>Context</code>. These are specified in <code>Source#getParameters()</code> and allow us to parameterize this source so that different instances of this job may work with different parameters. This is a very useful concept for designing reusable components for constructing jobs as well as completely reusable jobs.</p> <pre><code>    /**\n     * Define parameters required by this source.\n     *\n     * @return\n     */\n@Override\npublic List&lt;ParameterDefinition&lt;?&gt;&gt; getParameters() {\nList&lt;ParameterDefinition&lt;?&gt;&gt; params = Lists.newArrayList();\n\n// Consumer key\nparams.add(new StringParameter()\n.name(CONSUMER_KEY_PARAM)\n.description(\"twitter consumer key\")\n.validator(Validators.notNullOrEmpty())\n.required()\n.build());\n\nparams.add(new StringParameter()\n.name(CONSUMER_SECRET_PARAM)\n.description(\"twitter consumer secret\")\n.validator(Validators.notNullOrEmpty())\n.required()\n.build());\n\nparams.add(new StringParameter()\n.name(TOKEN_PARAM)\n.description(\"twitter token\")\n.validator(Validators.notNullOrEmpty())\n.required()\n.build());\n\nparams.add(new StringParameter()\n.name(TOKEN_SECRET_PARAM)\n.description(\"twitter token secret\")\n.validator(Validators.notNullOrEmpty())\n.required()\n.build());\n\nparams.add(new StringParameter()\n.name(TERMS_PARAM)\n.description(\"terms to follow\")\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"Netflix,Dark\")\n.build());\n\nreturn params;\n\n}\n</code></pre> <p>Now our primary class <code>TwitterJob</code> which implements <code>MantisJobProvider</code> needs to specify our new source so change the source line to match the following.</p> <pre><code>.source(new TwitterSource())\n</code></pre>"},{"location":"develop/writing-jobs/twitter/#the-stage","title":"The Stage","text":"<p>The stage is nearly equivalent to the previous tutorial. We need to add a few lines to the beginning of the chain of operations to deserialize the string, filter for English Tweets and pluck out the text.</p> <pre><code>            .stage((context, dataO) -&gt; dataO\n\n// Deserialize data\n.map(JsonUtility::jsonToMap)\n\n// Filter for English Tweets\n.filter((eventMap) -&gt; {\nif(eventMap.containsKey(\"lang\") &amp;&amp; eventMap.containsKey(\"text\")) {\nString lang = (String)eventMap.get(\"lang\");\nreturn \"en\".equalsIgnoreCase(lang);\n}\nreturn false;\n})\n\n// Extract Tweet body\n.map((eventMap) -&gt; (String)eventMap.get(\"text\"))\n\n// Same from here...\n</code></pre>"},{"location":"develop/writing-jobs/twitter/#conclusion","title":"Conclusion","text":"<p>We've learned how to create a parameterized source which reads from Twitter and pulls data into the ecosystem. With some slight modifications our previous example's stage deserializes the messages and extracts the data to perform the same word count.</p> <p>If you've checked out the <code>mantis</code> repository, then running following commands should begin running the job and expose a local port for SSE streaming.</p> <pre><code>$ cd mantis-examples/mantis-examples-twitter-sample\n$ ../../gradlew execute --args='consumerKey consumerSecret token tokensecret'\n</code></pre> <p>As an exercise consider how you might begin to scale this work out over multiple machines if the workload were too large to perform on a single host. This will be the topic of the next tutorial.</p>"},{"location":"develop/writing-jobs/word-count/","title":"Writing Your First Mantis Job","text":"<p>We'll be doing the classic word count example for streaming data for the tutorial section. For this example we'll be keeping it simple and focusing on the processing logic and job provider. The tutorials are structured progressively to allow us to incrementally build some experience writing jobs without getting overwhelmed with details. We'll stream text from a Project Gutenberg book, perform some application logic on the stream, and then write the data to a sink for consumption by other Mantis jobs. If you want to follow along, please check the Word Count module in Mantis repository.</p> <p>There are a few things to keep in mind when implementing a Mantis Job;</p> <ul> <li>We're just writing Java and there are a few interfaces necessary for Mantis</li> <li>Mantis jobs are composed of a source, n stages, and a sink.</li> <li>Mantis makes heavy use of Reactive Streams as a DSL for implementing processing logic.</li> </ul>"},{"location":"develop/writing-jobs/word-count/#wordcountjob","title":"WordCountJob","text":"<p>The full source of the WordCountJob class is included below with imports elided. This class implements the <code>io.mantisrx.runtime.MantisJobProvider</code> interface which the Mantis runtime loads. <code>MantisJobProvider#getJobInstance()</code> provides the runtime with an entry point to your job's code.</p> <pre><code>/**\n * This sample demonstrates ingesting data from a text file and counting the number of occurrences of words within a 10\n * sec hopping window.\n * Run the main method of this class and then look for a the SSE port in the output\n * E.g\n * &lt;code&gt; Serving modern HTTP SSE server sink on port: 8650 &lt;/code&gt;\n * You can curl this port &lt;code&gt; curl localhost:8650&lt;/code&gt; to view the output of the job.\n */\n@Slf4j\npublic class WordCountJob extends MantisJobProvider&lt;String&gt; {\n\n@Override\npublic Job&lt;String&gt; getJobInstance() {\nreturn MantisJob\n.source(new IlliadSource()) // Ignore for now, we'll implement one in the next tutorial.\n.stage((context, dataO) -&gt; dataO\n// Tokenize\n.flatMap((text) -&gt; Observable.from(tokenize(text)))\n// On a hopping window of 10 seconds\n.window(10, TimeUnit.SECONDS)\n.flatMap((wordCountPairObservable) -&gt; wordCountPairObservable\n// count how many times a word appears\n.groupBy(WordCountPair::getWord)\n.flatMap((groupO) -&gt; groupO.reduce(0, (cnt, wordCntPair) -&gt; cnt + 1)\n.map((cnt) -&gt; new WordCountPair(groupO.getKey(), cnt))))\n.map(WordCountPair::toString)\n, StageConfigs.scalarToScalarConfig())\n// Reuse built in sink that eagerly subscribes and delivers data over SSE\n.sink(Sinks.eagerSubscribe(Sinks.sse((String data) -&gt; data)))\n.metadata(new Metadata.Builder()\n.name(\"WordCount\")\n.description(\"Reads Homer's The Illiad faster than we can.\")\n.build())\n.create();\n}\n\nprivate List&lt;WordCountPair&gt; tokenize(String text) {\nStringTokenizer tokenizer = new StringTokenizer(text);\nList&lt;WordCountPair&gt; wordCountPairs = new ArrayList&lt;&gt;();\nwhile(tokenizer.hasMoreTokens()) {\nString word = tokenizer.nextToken().replaceAll(\"\\\\s*\", \"\").toLowerCase();\nwordCountPairs.add(new WordCountPair(word,1));\n}\nreturn wordCountPairs;\n}\n\n\npublic static void main(String[] args) {\nLocalJobExecutorNetworked.execute(new WordCountJob().getJobInstance());\n}\n}\n</code></pre> <p>There are several things going on here, let's examine them one at a time...</p>"},{"location":"develop/writing-jobs/word-count/#the-source","title":"The Source","text":"<p>We specify our source in the line <code>.source(new IlliadSource())</code>. The source handles data ingestion and it is very common to use a pre-existing parameterized source when writing jobs. Mantis provides several sources which handle managing connections and queries to other jobs. In the next tutorial we'll learn how to implement our own source which ingests data from Twitter.</p>"},{"location":"develop/writing-jobs/word-count/#the-stage","title":"The Stage","text":"<p>Our stage implements the bulk of the processing logic for the streaming job. Recall that a Mantis job has 1..n stages which can be used to create a topology for data processing. This stage is a <code>ScalarComputation</code> but we'll learn about other stage types in the third tutorial when we make word counting a distributed job.</p> <p>We'll take advantage of Java's lambda syntax to implement this stage inline. The <code>call</code> method receives a <code>Context</code> object and an <code>Observable&lt;String&gt;</code> provided by our source. The stage's responsibility is to produce an <code>Observable&lt;R&gt;</code> for consumption by down stream stages or the sink if this is the last stage.</p> <pre><code>.stage((context, dataO) -&gt; dataO\n// Tokenize the string\n.flatMap((text) -&gt; Observable.from(tokenize(text)))\n\n// Hopping / Tumbling window of 10 seconds\n.window(10, TimeUnit.SECONDS)\n\n// Reduce each window\n.flatMap((wordCountPairObservable) -&gt; wordCountPairObservable\n// count how many times a word appears\n\n.groupBy(WordCountPair::getWord)\n.flatMap((groupO) -&gt; groupO.reduce(0, (cnt, wordCntPair) -&gt; cnt + 1)\n.map((cnt) -&gt; new WordCountPair(groupO.getKey(), cnt))))\n// Convert the result to a string\n.map(WordCountPair::toString)\n</code></pre> <p>If you're familiar with reactive stream processing the above should be fairly easy to comprehend. Unfortunately if you aren't then an introduction to this is outside of the scope of this tutorial. Head over to reactivex.io to learn more about the concept.</p> <p>The stage configuration below specifies a few things; First that this stage is a scalar to scalar stage in that it ingests single events, and produces single events. The type of the input events is String, and the output is also String. Finally the configuration also specifies which <code>Codec</code> to use on the wire for this stage's output. You can use this configuration to specify concurrency for this stage as well, but we've not elected to do so here.</p> <pre><code>public static ScalarToScalar.Config&lt;String, String&gt; scalarToScalarConfig() {\nreturn new ScalarToScalar.Config&lt;String, String&gt;()\n.codec(Codecs.string());\n}\n</code></pre>"},{"location":"develop/writing-jobs/word-count/#the-job-provider","title":"The Job Provider","text":"<p>The <code>MantisJobProvider</code> interface is what the Mantis runtime expects to load. The runtime reads <code>resources/META-INF/services/io.mantisrx.runtime.MantisJobProvider</code> to discover the fully qualified classname of the MantisJobProvider to be used as an entry point for the application.</p>"},{"location":"develop/writing-jobs/word-count/#main-method","title":"Main Method","text":"<p>The main method invokes the <code>LocalJobExecutorNetworked</code> <code>execute</code> method to run our job locally. The first three tutorials will take advantage of the ability to run jobs locally. In the fourth tutorial we will explore uploading and submitting our job on a Mantis cloud deployment for greater scalability. We can and should run this main method by following commands from the Mantis repository:</p> <pre><code>$ cd mantis-examples/mantis-examples-wordcount\n$ ../../gradlew execute\n</code></pre> <pre><code>    public static void main(String[] args) {\nLocalJobExecutorNetworked.execute(new WordCountJob().getJobInstance());\n}\n</code></pre>"},{"location":"develop/writing-jobs/word-count/#conclusions-and-future-work","title":"Conclusions and Future Work","text":"<p>We've implemented a complete end-to-end Mantis job which counts words from The Illiad repeatedly. This leaves much to be desired. If you inspect our source we're really just iterating over the same data set every ten seconds. In the next tutorial we'll explore the task of writing our own custom source to pull external data from Twitter into Mantis and designing this source in a templated fashion so that it can be used with different queries and API keys.</p> <p>As an extra credit task see if you can modify the stage in this job to print the top 10 words instead of the entire list.</p>"},{"location":"getting-started/concepts/","title":"Mantis Concepts","text":"<p>Mantis provides Stream-processing-As-a-Service. It is a self contained platform that manages all tasks associated with running thousands of stream processing jobs. </p> <p>Take a look at Infrastructure Overview to get an understanding of the physical components of the Mantis Platform.</p> <p>Let us walk through some of the key concepts and terminologies used in Mantis.</p>"},{"location":"getting-started/concepts/#mantis-job-cluster","title":"Mantis Job Cluster","text":"<p>A Mantis Job cluster represents metadata (artifact, configuration, resource requirements) associated with a job. A job can be thought of as a running instance of Job Cluster (like a Java Object is an instance of a Java class). A Job cluster can have 0 or more running instances of a job at any given time.  Users can control how many jobs can be running at any given time by specifying the SLA for this cluster. </p> <p>E.g.</p> <p>SLA of Min 1 / Max 1 means there is exactly one instance of job running at any given time. Users can also setup a Cron spec that can be used to submit jobs periodically.</p>"},{"location":"getting-started/concepts/#mantis-jobs","title":"Mantis Jobs","text":"<p>At the core of Mantis is the Mantis Job. Stream processing applications in Mantis are called Mantis Jobs.  Logically a job represents the business logic to transform a stream of events from one or more sources and generate results. A developer writes a job using Mantis primitives and builds an artifact which is then deployed into the Mantis platform for execution.  Each Mantis Job belongs to exactly one Mantis Job Cluster.</p> <p>Lets take a closer look at a Mantis Job.</p>"},{"location":"getting-started/concepts/#worker","title":"Worker","text":"<p>A worker is the smallest unit of execution in Mantis. Physically a worker executes in a resource isolated Mesos container on the Mantis Agent fleet of servers. </p> <p>Each worker is assigned a unique monotonically increasing <code>MantisWorkerIndex</code> and a unique <code>MantisWorkerNumber</code> If a Mantis worker terminates abnormally, Mantis ensures a replacement worker gets launched with the same <code>MantisWorkerIndex</code> and a new <code>MantisWorkerNumber</code>. </p> <p>The resources like CPU, Memory, Network allocated to a worker are configured at Job submit time and cannot be updated later.</p> <p>Each Worker belongs to exactly one Mantis Stage.</p>"},{"location":"getting-started/concepts/#stages-group-of-workers","title":"Stages (Group of workers)","text":"<p>A Mantis job is logically divided into one or more stages. A stage is a collection of homogeneous Mantis Workers that perform the same computation. A typical map-reduce style job would be represented by three stages in Mantis (shuffle, window/aggregate and collect)</p> <p>Workers belonging to a stage establish network connections with all workers of the previous stage (if one exists) Workers of the same stage do not connect to each other.</p> <p>The presence of multiple stages imply network hops which allows users to distribute the job logic across several workers allowing for more scalability.</p> <p>Mantis allows each stage to dynamically scale the number of workers in the stage independently with the help of an autoscaling policy.</p> <p>Info</p> <p>Autoscaling is only recommended on stateless stages. For stateful stages the user would need to implement logic to move state to new workers. In cases where the state can be rebuilt rapidly this is not a concern.  </p>"},{"location":"getting-started/concepts/#mantis-runtime","title":"Mantis Runtime","text":"<p>The Mantis runtime is execution environment for Mantis Jobs. </p> <p>Broadly speaking it covers all aspects of the running a Mantis Job which includes</p> <ul> <li>Operators based Reactive Extensions used by Jobs to implement their business logic. </li> <li>Job topology management which builds and maintains the job execution DAG</li> <li>Exchanging control messages with the Mantis Master including heartbeats among other things. </li> </ul>"},{"location":"getting-started/concepts/#source-job","title":"Source Job","text":"<p>A source job is a type of Mantis Job that makes data available to other Mantis Jobs via an MQL interface. Downstream jobs connect to the Sink (Server Sent Event) of the Source job with an MQL query which denotes what data the job is interested in. Each event flowing through the Source job is evaluated against these MQL queries. Events matching a particular query are then streamed to the corresponding downstream job.</p> <p></p> <p>The source jobs have several advantages</p> <ul> <li> <p>Pluggable source: By abstracting out where the data is coming from, the source jobs allow the downstream jobs to focus on just their processing logic. The same job can then work with data from different sources by simply connecting to a different source jobs. E.g There can be one source job (say A) backed by a Kafka topic and another backed by S3 (say B) and the downstream job can either connect to A or B based on whether they want to process realtime data or historical data.</p> </li> <li> <p>Data re-use: Often a lot of jobs are interested in data from the same source. Just that they maybe interested in different subsets of this data. Instead of each job having to re-fetch the same data again and again from the same external  source, The source jobs fetch the data once and then make it available for any other job to use.</p> </li> <li> <p>Cost Efficiency: A direct consequence of data re-use is fewer resources are required. </p> <p>E.g Let us assume there are 3 jobs interested in processing data from the same Kafka topic. But each is interested in different subsets of this data. Traditionally, each job would have to read the topic in its entirety and then filter out the  data they are not interested in. This means 3x fanout on Kafka, and additionally each job now has to have enough resources to process the entire topic. If this topic is high volume then these jobs would have to be sufficiently  scaled to keep up. All while throwing away large portions of the data. With a source job the Kafka fan out is just 1 and the three downstream jobs need to be scaled just enough to process their anticipated subset of the stream.</p> </li> <li> <p>Less operational load: Source jobs are stateless and thus good candidates for Autoscaling. With autoscaling enabled the Mantis administrators do not have to worry about right sizing the resources, the job will just adapt its size to meet the demands.</p> </li> </ul> <p>Mantis OSS comes with the following source jobs</p> <ol> <li> <p>Kafka Source job:  Reads data from one or more Kafka topics and makes it available for downstream consumers to query via MQL.</p> </li> <li> <p>Publish Source Job: Works the the mantis-publish library to fetch data on-demand from external applications and make it available to downstream consumers. See the On-Demand Sample to see this in action.</p> </li> </ol> <p>Users can also build their own source jobs see the Synthetic Source Job example.</p>"},{"location":"getting-started/concepts/#job-chaining","title":"Job Chaining","text":"<p>One of the unique capabilities of Mantis is the ability for Jobs to communicate with each other to form a kind of streaming microservices architecture. The Source Job -&gt; Downstream job flow is an example of this Job chaining.  In this case the <code>source</code> of the downstream job is the output of the upstream Data source job.  All a job needs to do to connect to the sink of another job is to include the in-built Job Connnector.</p> <p>See the Job Connector Sample to see this in action.</p> <p>These job to job communications happen directly via in memory socket connections with no intermediate disk persistence. If buffering/persistence of results is desired then it is recommended to sink the data into persistence queue like Kafka using the Kafka Connector</p> <p>Job chaining has proven to be extremely useful while operating at scale. It is widely used in the Netflix deployment of Mantis.</p>"},{"location":"getting-started/concepts/#mantis-query-language-mql","title":"Mantis Query Language (MQL)","text":"<p>MQL is a SQL like language that allows users to work with streaming data without having to write Java code.</p> <p>Example MQL query:</p> <pre><code>select * from defaultStream where status==500\n</code></pre> <p>MQL is used in various parts of Mantis platform including the mantis-publish library, Source Jobs and sometimes directly as a library within a Job that can benefit from the query and aggregation features it brings.</p>"},{"location":"getting-started/concepts/#mantis-master","title":"Mantis Master","text":"<p>The Mantis Master is a leader elected control plane for the Mantis platform. It is responsible for managing the life cycle of Job Clusters, Jobs and workers. It also acts as a Resource scheduler to optimally allocate and schedule resources required by the Jobs. The master stores its meta-data into an external source. The OSS version ships with a sample file based store. For production deployments a highly available store is recommended.</p> <p>The Master is built using Akka principles, where each Job Cluster, Job etc are modelled as Actors. For scheduling of resources Mantis relies on the Mesos Framework The Master registers itself as a Mesos Framework. It receives resource offers from Mesos and uses Fenzo to optimally match workers to these offers. </p>"},{"location":"getting-started/concepts/#mantis-api","title":"Mantis API","text":"<p>The Mantis API is almost like a traditional API server which proxies request to the Mantis Master.  But has additional capabilities such as:</p> <ul> <li>Allowing users to stream the output of a job via web sockets (<code>/api/v1/jobConnectbyid/jobID</code> API)</li> <li>Acts as a discovery server for Jobs, allowing consumers to get a stream of scheduling information (like host, port for workers belonging to a job)</li> </ul>"},{"location":"getting-started/use-cases/","title":"Mantis Use Cases","text":"<p>Here is a non-comprehensive list of use-cases powered by Mantis to give you an idea of the type of applications that can be built on Mantis.</p>"},{"location":"getting-started/use-cases/#realtime-monitoring-of-netflix-streaming-health","title":"Realtime monitoring of Netflix streaming health","text":"<p>At Netflix Stream Starts per Second (SPS) is a key metric used to track the health of the Netflix streaming service. Streaming starts per second tracks the number of people successfully hitting <code>play</code> on their streaming devices. Any abnormal change in the trend of this metric signifies a negative impact on user's viewing experience.</p> <p>This Mantis application monitors the SPS trend by processing data sourced directly from thousands of Netflix servers (using the mantis-publish library) in realtime. Using a version of Double Exponential Smoothing (DES) it can detect abnormal deviations in seconds and alerts key teams at Netflix.</p>"},{"location":"getting-started/use-cases/#contextual-alerting","title":"Contextual Alerting","text":"<p>As Netflix has grown over the years so has the number of microservices. Getting alerted for outages for your own service is often not sufficient to root-cause issues. Engineers need to understand what is happening with downstream and upstream services as well to be able to quickly narrow down the root of the issue.</p> <p>The Contextual alerting application analyzes millions of interactions between dozens of Netflix microservices in realtime to  identify anomalies and provide operators with rich and relevant context.  The realtime nature of these Mantis-backed aggregations allows the Mean-Time-To-Detect to be cut down from tens of minutes to a few seconds.  Given the scale of Netflix this makes a huge impact.</p>"},{"location":"getting-started/use-cases/#raven","title":"Raven","text":"<p>In large distributed systems, often there are cases where a user reports a problem but the overall health of the system is green. In such cases there is a need to explore events associated with the user/device/service in realtime to find a smoking gun. With the potential of a user request landing across thousands of servers it is often a laborious task to find the right servers and inspect their logs.</p> <p>The Raven applications makes this task trivial, The raven jobs work with the mantis-publish library to look for events matching a certain criterion (user-id/device-id etc) right at the server and stream matching results in realtime. It provides an intuitive UI that allows SREs to construct and submit simple MQL queries. </p>"},{"location":"getting-started/use-cases/#cassandra-and-elastic-search-health-monitoring","title":"Cassandra and Elastic Search Health Monitoring","text":"<p>Netflix maintains hundreds of Cassandra and Elastic Search clusters. These clusters are critical for the day to day operation of Netflix.  </p> <p>The Cassandra and Elastic Search health application analyzes rich operational events sent by the Priam side car in realtime to generate a holistic picture  of the health of every Cassandra cluster at Netflix. Since this system has gone into operation the number of false pages  has dropped down significantly. </p>"},{"location":"getting-started/use-cases/#alerting-on-log","title":"Alerting on Log","text":"<p>The Alerting on Logs application allows users to create alerts which page when a certain pattern is detected within the application logs. This application analyzes logs from thousands of servers in realtime. </p>"},{"location":"getting-started/use-cases/#chaos-experimentation-monitoring","title":"Chaos Experimentation monitoring","text":"<p>Chaos Testing is one of the pillars of resilience at Netflix. Dozens of chaos experiments are run daily to test the resilience of variety of applications. </p> <p>The Chaos experimentation application tracks user experience by analyzing client and server side events during a Chaos exercise in realtime and triggers an abort of the chaos exercise in case of an adverse impact.</p>"},{"location":"getting-started/use-cases/#realtime-personally-identifiable-information-pii-data-detection","title":"Realtime Personally Identifiable Information (PII) data detection","text":"<p>With trillions of events flowing through Netflix data systems daily it is critical to ensure no sensitive data is accidentally passed along. </p> <p>This application samples data across all streaming sources and applies custom pattern detection algorithms to identify presence of such data.</p>"},{"location":"getting-started/samples/on-demand/","title":"On-Demand","text":"<p>One of the key features of Mantis is the ability to stream filtered events on-demand from external applications.</p> <p>In this example we walk through publishing data to Mantis from a simple Java application using the mantis-publish library. Followed by setting up a Data Source Job that will act as a broker  and a simple Mantis Job that when launched will trigger on-demand streaming of data matching a certain criterion.</p> <p>This end-to-end example highlights two powerful Mantis concepts</p> <ol> <li> <p>On demand streaming of filtered data from external applications directly into Mantis</p> </li> <li> <p>Job Chaining where one Job connects to the output of another job.</p> </li> </ol> <p></p>"},{"location":"getting-started/samples/on-demand/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>SharedMrePublishEventSource</code> Job Cluster exists.</li> <li><code>JobConnectorSample</code> Job cluster exists</li> <li>Java Sample is setup and running.</li> </ul> <p>Note If you are following the Mantis Cluster using Docker instructions all of this will be already set up. </p>"},{"location":"getting-started/samples/on-demand/#publishing-events-into-mantis","title":"Publishing events into Mantis","text":"<p>Note: The local docker setup has already preconfigured a simple Java Sample application to publish events to Mantis..</p>"},{"location":"getting-started/samples/on-demand/#setting-up-a-publish-data-source-job","title":"Setting up a Publish Data Source Job","text":"<p>A Publish Source Job is a special kind of a Mantis Job that interacts with the mantis-publish library on behalf of a downstream job to push <code>subscriptions</code> up to the mantis-publish library and receive events  matching the subscription from the mantis-publish library.</p>"},{"location":"getting-started/samples/on-demand/#submit-the-sharedmrepublisheventsource","title":"Submit the SharedMrePublishEventSource","text":"<p>Part of the docker setup we have preconfigured the <code>SharedMrePublishEventSource</code> cluster. So all we have  to do is submit an instance of it.</p> <ol> <li> <p>Go to the clusters page and click on <code>SharedMrePublishEventSource</code></p> </li> <li> <p>Click <code>submit</code> on the top right corner of the screen</p> <p></p> </li> <li> <p>This will open up the Job Detail page. Now wait for the Job to go into <code>Launched</code> state</p> <p></p> </li> </ol> <p>You are all set. Now the Java application referenced in the previous section should be able to communicate with this source job to exchange subscriptions and data.</p> <p>At this point however there are no active subscriptions so no data is actually being sent out from our Java application. If you look at the shell window where the docker is running you should see output like</p> <pre><code>mantispublish_1     | 2019-10-16 17:55:46 INFO  SampleDataPublisher:56 - Mantis publish JavaApp send event status =&gt; SKIPPED_NO_SUBSCRIPTIONS(PRECONDITION_FAILED)\n</code></pre> <p>Next step. Launch a new job to query for Data generated by the Java application.</p>"},{"location":"getting-started/samples/on-demand/#query-data-generated-by-the-java-application","title":"Query data generated by the Java application.","text":"<p>Our Java application generates a stream of Events representing requests made to it by an external client. Say we want to look at all events that are failing i.e have <code>status=500</code></p>"},{"location":"getting-started/samples/on-demand/#submit-a-query-job","title":"Submit a query job","text":"<p>Now we launch a simple Mantis Job to query data generated by our Java Application.</p> <ol> <li> <p>Go to clusters page and click on <code>JobConnectorSample</code></p> </li> <li> <p>Click the green <code>submit</code> button on the top right corner of the screen.</p> </li> <li> <p>On the Job submit page scroll down and click on <code>Override Defaults</code> to configure our query.</p> </li> </ol> <p>Enter the following as the value for parameter <code>target</code></p> <p><code>json     {\"targets\":[{\"sourceJobName\":\"SharedMrePublishEventSource\",\"criterion\":\"select * from defaultStream where status==500\"}]}</code></p> <p>and hit submit. </p> <p> </p> <p>Two key things to note:</p> <ul> <li>We set the <code>sourceJobName</code> to <code>SharedMrePublishEventSource</code> which is the source job we configured in the    previous step.</li> <li>We set the <code>criterion</code> key in the payload to our MQL query <code>select * from defaultStream where status==500</code></li> </ul> <p>Click on <code>Submit</code>.</p> <p>On the Job Detail page scroll down to the <code>Job Output</code> section and click on Start.    In a few seconds you should see events matching our query flow through.</p> <p> </p> <p>If you go back to the shell that is running the docker images you should now see output like</p> <pre><code>mantispublish_1     | 2019-10-16 17:58:32 INFO  SampleDataPublisher:56 - Mantis publish JavaApp send event status =&gt; ENQUEUED(SENDING)\n</code></pre> <p>If we now terminate our <code>JobConnector</code> Job, then our Java application will again revert to not sending any data.</p>"},{"location":"getting-started/samples/on-demand/#take-aways","title":"Take aways","text":"<p>By integrating the mantis-publish library with their applications, users can get access to rich data generated by their applications in realtime and in a cost-effective manner for analysis into their jobs. </p>"},{"location":"getting-started/samples/sine-function/","title":"Sine Function","text":"<p>The Sine function sample is a very simple job that generates a set of x and y coordinates of a sine wave. </p>"},{"location":"getting-started/samples/sine-function/#prerequisites","title":"Prerequisites","text":"<p>A SineFunction Job Cluster exists.</p> <p>Note If you are following the Mantis Cluster using Docker instructions this will be already set up. </p>"},{"location":"getting-started/samples/sine-function/#running-the-sample","title":"Running the sample","text":"<ol> <li> <p>Go to the clusters page page and  Click on <code>SineFunction</code></p> </li> <li> <p>On the Job Cluster detail page. Click the <code>Submit</code> green button on the top right.</p> <p>This will open up a submit screen that will  allow you to override Resource configurations as well as parameter values.</p> </li> </ol> <p></p> <p>Let us skip all that and scroll directly to the bottom and hit the <code>Submit</code> button on the bottom left.</p> <p></p> <p>View output of the job</p> <p>If all goes well your job would go into <code>Launched</code> state.</p> <p></p> <p>Scroll to the bottom and in the <code>Job Output</code> section click on <code>Start</code></p> <p>You should see output of the Sine function job being streamed below</p> <pre><code>Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585}\n</code></pre> <p></p>"},{"location":"getting-started/samples/sine-function/#terminate-the-job","title":"Terminate the job","text":"<p>To stop the job click on the red <code>Kill Job</code> button on the top right corner.</p>"},{"location":"getting-started/samples/sine-function/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Explore the code</p> </li> <li> <p>Checkout out the other samples</p> </li> </ul>"},{"location":"getting-started/samples/twitter/","title":"Twitter","text":"<p>The Twitter sample demonstrates sourcing data from an external source (twitter in this case) and  calculates the number of occurrences of a word within a hopping window. </p>"},{"location":"getting-started/samples/twitter/#prerequisites","title":"Prerequisites","text":"<p>A <code>TwitterSample</code> Job Cluster exists.</p> <p>Note If you are following the Mantis Cluster using Docker instructions this will be already set up. </p> <p>Twitter credentials to be used to connect to Twitter.</p> <p>If you don't already have a twitter application You can create one here here The Keys and Tokens section should list the credentials needed for this application. </p>"},{"location":"getting-started/samples/twitter/#running-the-sample","title":"Running the sample","text":"<p>Let us try submitting this job.</p> <ol> <li> <p>Click on TwitterSample from the clusters page.</p> </li> <li> <p>Click the <code>Submit</code> green button on the top right.</p> <p>This will open up a submit screen that will  allow you to override Resource configurations as well as parameter values.</p> </li> </ol> <p>Let us scroll down to the parameters section.  </p> <p></p> <p>Here we fill in the required parameters for this job. These include</p> <ul> <li>Twitter consumer key</li> <li>Twitter consumer secret</li> <li>Twitter token </li> <li>Twitter token secret </li> </ul>"},{"location":"getting-started/samples/twitter/#view-output-of-the-job","title":"View output of the job","text":"<p>If all goes well your job would go into <code>Launched</code> state. Scroll to the bottom and click on <code>Start</code> You should see output of the Twitter job being streamed below</p> <p></p>"},{"location":"getting-started/samples/twitter/#terminate-the-job","title":"Terminate the job","text":"<p>To stop the job click on the red <code>Kill Job</code> button on the top right corner.</p> <ul> <li> <p>Explore the code</p> </li> <li> <p>Checkout out the other samples</p> </li> </ul>"},{"location":"getting-started/tutorials/cloud/","title":"Spinning up your first Mantis cluster in the cloud using the Mantis CLI","text":""},{"location":"getting-started/tutorials/cloud/#prerequisites","title":"Prerequisites","text":"<p>The Mantis CLI currently supports AWS by spinning up a minimal cluster using T2 micro instances. This is meant for basic testing and not meant to run production traffic.</p> <p>In order for you to spin up a cluster in AWS, you will need to create or use an existing AWS account. You can follow AWS's account creation instructions for more information.</p> <p>Once your account is created, you will need to create and download your AWS Access Keys. You can follow AWS's instructions on creating access keys for more information.</p> <p>Now that you have your AWS account and access keys on hand, you're ready to bootstrap your first Mantis cluster using the Mantis CLI.</p>"},{"location":"getting-started/tutorials/cloud/#bootstrapping-your-first-mantis-cluster-in-aws","title":"Bootstrapping your first Mantis cluster in AWS","text":""},{"location":"getting-started/tutorials/cloud/#download-and-install-the-mantis-cli","title":"Download and install the Mantis CLI","text":"<p>First, you'll need to download the Mantis CLI app.</p> <p>If you're on Mac OS, it's recommended that you download and install using the Mac package:</p> <ul> <li>mantis-v0.1.1.pkg</li> </ul> <p>If you're on other systems such as Linux or Windows, choose the appropriate package from the Release Assets:</p> <ul> <li>v0.1.1 Release Assets</li> </ul>"},{"location":"getting-started/tutorials/cloud/#configure-aws-credentials","title":"Configure AWS credentials","text":"<p>Now that you have the Mantis CLI installed, you'll need to tell it about your AWS credentials:</p> <pre><code>$ mantis aws:configure\nAWS access key id: &lt;input access key id&gt;\nAWS secret access key: &lt;input secret access key&gt;\nConfiguring AWS credentials... done\n</code></pre> <p>This command stores your AWS credentials in the same exact format and location as the AWS SDK. More on this at the Mantis CLI page.</p>"},{"location":"getting-started/tutorials/cloud/#bootstrap-your-cluster","title":"Bootstrap your cluster","text":"<p>With AWS credentials configured, you can bootstrap your cluster in a single command:</p> <pre><code>$ mantis aws:bootstrap\n? select a region us-east-2\n? Proceed with Mantis cluster creation? (Y/n) Y\n  \u2193 Create key pair [skipped]\n\u2192 Key-pair file already exists (mantis-us-east-2)\n\u2193 Create default VPC [skipped]\n\u2192 Default VPC already exists\n  \u2193 Create zookeeper security group [skipped]\n\u2192 Security group already exists (zookeeper)\n\u2193 Authorize zookeeper security group ssh port ingress [skipped]\n\u2192 Ingress rule already exists\n  \u2193 Authorize zookeeper security group zookeeper port ingress [skipped]\n\u2192 Ingress rule already exists\n  \u2714 Bootstrap Zookeeper node\n  \u2193 Create mesos-master security group [skipped]\n\u2192 Security group already exists (mesos-master)\n\u2193 Authorize mesos-master security group ssh port ingress [skipped]\n\u2192 Ingress rule already exists\n  \u2839 Authorize mesos-master security group mesos-master port 5050 ingress\n    Bootstrap Mesos Master node\n    Create mesos-slave security group\n    Authorize mesos-slave security group ssh port ingress\n    Authorize mesos-slave security group mantis-agent port 7104 ingress\n    Authorize mesos-slave security group mantis-agent port 7150-7400 ingress\n    Authorize mesos-slave security group mesos-slave resource port ingress\n    Bootstrap Mesos Slave node\n    Create mantis-control-plane security group\n    Authorize mantis-control-plane security group ssh port ingress\n    Authorize mantis-control-plane security group remote debug port 5050 ingress\n    Authorize mantis-control-plane security group api port 8100 ingress\n    Authorize mantis-control-plane security group api v2 port 8075 ingress\n    Authorize mantis-control-plane security group scheduling info port 8076 ingress\n    Authorize mantis-control-plane security group metrics port 8082 ingress\n    Authorize mantis-control-plane security group console port 9090 ingress\n    Bootstrap Mantis Control Plane service\n    Create mantis-api security group\n    Authorize mantis-api security group ssh port ingress\n    Authorize mantis-api security group web port 80 ingress\n    Authorize mantis-api security group ssl port 443 ingress\n    Authorize mantis-api security group api port 7101 ingress\n    Authorize mantis-api security group websocket port 7102 ingress\n    Authorize mantis-api security group tunnel port 7001 ingress\n  \u2714 Bootstrap Mantis API service\nMantis API will be provisioned in a few minutes\n    with public DNS available at &lt;ec2 address&gt;:7101\n    Input this URL into your local Mantis UI to connect to the Mantis cluster.\nMesos Master will be provisioned in a few minutes\n    with public DNS available at &lt;ec2 address&gt;:5050\n    Input this URL into your local Mantis UI so it can connect to Mesos logs.\n</code></pre> <p>This will launch and configure 5 AWS EC2 instances. Notice at the end of the bootstrap are 2 EC2 address. You will need these to input into the Mantis UI.</p>"},{"location":"getting-started/tutorials/cloud/#using-the-mantis-ui","title":"Using the Mantis UI","text":"<p>On your browser, navigate to the Mantis UI at:</p> <ul> <li>https://netflix.github.io/mantis-ui</li> </ul> <p>And fill out the Registration form as follows:</p> <p></p> <ol> <li>Name: <code>Example</code></li> <li>Email: <code>example@example.com</code></li> <li>Master Name: <code>Example</code></li> <li>Mantis API URL: <code>&lt;your ec2 Mantis API URL outputted from the Mantis CLI&gt;</code></li> <li>Mesos URL: <code>&lt;your ec2 Mesos URL outputted from the Mantis CLI&gt;</code></li> </ol>"},{"location":"getting-started/tutorials/cloud/#launching-a-mantis-job","title":"Launching a Mantis Job","text":"<p>When you go into the UI, you'll notice that the Mantis CLI has automatically preloaded a Job Cluster for you to try out. Simply click on the <code>SineTest</code> Job Cluster to go into the cluster details page.</p> <p></p> <p>Once in the cluster details page, click on the green <code>Submit latest version</code> button on the top right to bring you to the Job Submit page.</p> <p></p> <p>On the Job Submit page, everything has already been configured for you. All you have to do is hit the green <code>Submit to Mantis</code> button at the bottom of the page to launch your first Mantis Job.</p> <p></p> <p>Now you can view the output of this job.</p> <p>If all goes well your job would go into <code>Launched</code> state.</p> <p></p> <p>Scroll to the bottom and in the <code>Job Output</code> section click on <code>Start</code></p> <p>You should see output of the Sine function job being streamed below</p> <pre><code>Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585}\n</code></pre>"},{"location":"getting-started/tutorials/cloud/#tearing-down-your-cluster","title":"Tearing down your cluster","text":"<p>Once you're done, you can clean up all of your AWS resources by tearing down instances and deleting security groups.</p> <pre><code>$ aws:teardown\n? select a region us-east-2\n? Proceed with Mantis cluster creation? (Y/n) Y\n  \u2714 Terminate instances\n</code></pre>"},{"location":"getting-started/tutorials/cloud/#debugging-your-cluster","title":"Debugging your cluster","text":"<p>You can debug your cluster by looking at the logs. To look at the logs, you'll need to go into your AWS EC2 Console and find instances with the <code>Application: Mantis</code> tag.</p> <p></p> <p>From there, you can look at the instances with the following security groups:</p> <ol> <li>zookeeper</li> <li>mesos-slave</li> <li>mesos-master</li> <li>mantis-control-plane</li> <li>mantis-api</li> </ol> <p>You can connect to your EC2 instances by following instructions from the <code>Connect</code> button at the top.</p> <p>Info</p> <p>The Mantis CLI puts your EC2 <code>.pem</code> keys in the same folder as your AWS credentials, typically located in <code>$HOME/.aws</code>.</p> <p>Tip</p> <p>Application logs, e.g. Mantis-related or Zookeeper, for all instances will be located in <code>/logs</code>.</p> <p>Tip</p> <p>Mesos-related logs for <code>mesos-master</code> and <code>mesos-slave</code> will be located in <code>/var/run/mesos</code>.</p>"},{"location":"getting-started/tutorials/docker/","title":"Spinning up your first Mantis cluster using Docker","text":""},{"location":"getting-started/tutorials/docker/#prerequisites","title":"Prerequisites","text":"<p>Install Docker on your local machine (if you don't already have it) </p> <ol> <li>Mac</li> <li>Windows</li> <li>Linux</li> </ol>"},{"location":"getting-started/tutorials/docker/#bootstraping-your-first-mantis-cluster-in-docker","title":"Bootstraping your first Mantis Cluster in Docker","text":""},{"location":"getting-started/tutorials/docker/#download-the-docker-compose-file","title":"Download the docker-compose file","text":"<p>Download the docker-compose.yml to a local folder mantis</p> <pre><code>$ cd &lt;mantis&gt;\n$ docker-compose -f docker-compose.yml up </code></pre> <p>This starts up the following Docker containers:</p> <ul> <li>Zookeeper</li> <li>Mesos Master</li> <li>Mantis Master</li> <li>Mantis API</li> <li>Mesos Slave and Mantis Worker run on a single container (mantisagent)</li> <li>A simple hello world web application that sends events to Mantis</li> <li>A simple Java application that sends events to Manits</li> </ul>"},{"location":"getting-started/tutorials/docker/#mantis-admin-ui","title":"Mantis Admin UI","text":"<p>The Mantis Admin UI allows you to manage your Mantis Jobs.</p> <p>Open the Mantis UI in a new browser window.</p> <p>Fill out the Registration form as follows</p> <p></p> <ol> <li>Name: <code>Example</code></li> <li>Email: <code>example@example.com</code></li> <li>Master Name: <code>Example</code></li> <li>Mantis API URL: <code>http://localhost:7101</code></li> <li>Mesos URL: <code>http://localhost:5050</code></li> </ol> <p>Click on <code>Create</code></p> <p>The Mantis Admin page should be pre-populated with all the Mantis examples.</p> <p></p>"},{"location":"getting-started/tutorials/docker/#try-out-mantis-jobs","title":"Try out Mantis Jobs","text":"<p>Now that you have setup a Mantis cluster locally try running some of the preconfigured Mantis samples</p> <ol> <li> <p>Sine Function Sample - A simple job that generates x and y coordinates of a sine wave.</p> </li> <li> <p>Twitter Sample - Connects to a twitter stream using consumer and token keys specified and performs a streaming word count.</p> </li> <li> <p>On Demand Sample - Demonstrates how Mantis Jobs can pull events on demand from external applications.</p> </li> </ol>"},{"location":"getting-started/tutorials/docker/#next-steps","title":"Next steps","text":"<ul> <li> <p>Setup Mantis in AWS and run the samples</p> </li> <li> <p>Write your first Mantis Job</p> </li> </ul> <p>To teardown the Mantis cluster, issue the following command</p> <pre><code>$ cd &lt;mantis&gt;\n$ docker-compose -f docker-compose.yml down\n</code></pre>"},{"location":"getting-started/tutorials/local/","title":"Explore a Mantis Job locally","text":""},{"location":"getting-started/tutorials/local/#prerequisites","title":"Prerequisites","text":"<p>JDK 8 or higher</p>"},{"location":"getting-started/tutorials/local/#build-and-run-the-synthetic-sourcejob-sample","title":"Build and run the synthetic-sourcejob sample","text":"<p>This job outputs request events sourced from an imaginary service. The RequestEvent data has information such as uri, status, userId, country etc. Data Source Jobs are mantis jobs that allow consumers to filter the raw stream down to just the events  they are interested in.  This filtering is done by specifying an MQL query while connecting to the sink.</p> <p>Clone the mantis repo:</p> <pre><code>$ git clone https://github.com/Netflix/mantis.git\n</code></pre> <p>Run the synthetic-sourcejob sample via gradle.</p> <pre><code>$ cd mantis/mantis-examples/mantis-examples-synthetic-sourcejob\n$ ../../gradlew execute\n</code></pre> <p>This will launch the job and you would see output like</p> <pre><code>2019-10-06 14:14:07 INFO  StageExecutors:254 main - initializing io.mantisrx.sourcejob.synthetic.stage.TaggingStage\n2019-10-06 14:14:07 INFO  SinkPublisher:82 main - Got sink subscription, onSubscribe=null\n2019-10-06 14:14:07 INFO  ServerSentEventsSink:141 main - Serving modern HTTP SSE server sink on port: 8436\n</code></pre> <p>The default Mantis sink is a ServerSentEvent sink that opens a port allowing anyone to connect to it and stream the results of the job. Look for a line like</p> <pre><code>Serving modern HTTP SSE server sink on port: 8436\n</code></pre> <p>The source job is now up and ready to serve data.</p> <p>Let us query for requests from countries where the status code is 500.  Such an MQL query would like like this. </p> <pre><code>select country from stream where status==500\n</code></pre> <p>In another terminal window curl this port</p> <pre><code>$ curl \"localhost:8436?subscriptionId=nj&amp;criterion=select%20country%20from%20stream%20where%20status%3D%3D500&amp;clientId=nj2\"\n</code></pre> <p>Here the subscriptionId and clientId are any valid strings. They are used to tag events that match the query. The criterion parameter is the URLEncoded MQL query.</p> <p>You should see events matching your query appear in your terminal</p> <pre><code>data: {\"country\":\"Ecuador\",\"mantis.meta.sourceName\":\"SyntheticRequestSource\",\"mantis.meta.timestamp\":1570396602599,\"status\":500}\n\ndata: {\"country\":\"Solomon Islands\",\"mantis.meta.sourceName\":\"SyntheticRequestSource\",\"mantis.meta.timestamp\":1570396603342,\"status\":500}\n\ndata: {\"country\":\"Liberia\",\"mantis.meta.sourceName\":\"SyntheticRequestSource\",\"mantis.meta.timestamp\":1570396603844,\"status\":500}\n</code></pre>"},{"location":"getting-started/tutorials/local/#next-steps","title":"Next Steps","text":"<ul> <li>Import the project into your IDE to explore the code.</li> <li>Try out other samples from the Mantis examples module.</li> <li>Set up Mantis locally using Docker and run the samples.</li> <li>Set up Mantis in AWS and run the samples.</li> <li>Learn to write your first Mantis Job.</li> </ul>"},{"location":"internals/infrastructure-overview/","title":"Infrastructure Overview","text":"<p>At a high level the Mantis Architecture consists of the Control plane (Mantis Master) and the Mantis runtime. The Master is responsible for managing the life cycle of Jobs, including creation, scheduling and deletion of jobs. The Mantis runtime is responsible for the actual execution  of Mantis jobs.</p>"},{"location":"internals/infrastructure-overview/#major-components","title":"Major components","text":"<p>Here is a brief description of the major components of the Mantis Architecture</p>"},{"location":"internals/infrastructure-overview/#mantis-control-plane-master","title":"Mantis control plane (Master)","text":"<p>The Control plane plane is responsible for managing the lifecycle of Mantis Jobs.  Metadata related to the jobs is stored in the Metadata store. The Open source version of Mantis ships with a simple file based store. For production use of a highly available store is recommended.</p> <p>The default resource manager for scheduling Job tasks is Mesos. The control plane registers itself as a framework into Mesos. The control plane uses Fenzo to match the job to the available Mesos offers in an optimal manner.</p>"},{"location":"internals/infrastructure-overview/#mantis-runtime","title":"Mantis Runtime","text":"<p>Mantis jobs are executed on Mesos agent(s) as one or more tasks (Job Workers) based on the job topology. The Mantis runtime is responsible for the actual execution of the job. This includes bootstrapping user code, exchanging control messages with the Master and establishing/maintaining upstream and downstream  network connections per the Job execution DAG.  One of the unique features of Mantis is that it allows Mantis Jobs to discover other Mantis Jobs and  stream results of one job as input to another. The Mantis runtime does the work of discovering,  connecting and maintaining these intra-job network flows.</p>"},{"location":"internals/infrastructure-overview/#mantis-api","title":"Mantis API","text":"<p>Mantis API provides an easy REST interface for interacting  with the Mantis system. It allows users to create/submit/kill Mantis Jobs and Mantis Job Clusters. Additionally, it also allows users to stream the output of any running jobs via WebSocket or SSE.</p>"},{"location":"internals/infrastructure-overview/#zookeeper","title":"Zookeeper","text":"<p>Mantis usage of Zookeeper is fairly minimum. The zookeeper is used primarily for Master leader election both by the Mantis Control Plane and the Mesos Master. </p>"},{"location":"internals/infrastructure-overview/#external-dependencies","title":"External Dependencies","text":"<p>Zookeeper : Used for leader election of Mantis and Mesos Masters.</p> <p>RxJava 1.x : Provides the fluent, functional programming model for Mantis Jobs.</p> <p>Mesos 1.x : Powers the underlying resource management system.</p> <p>RxNetty 0.4 : Used for the intra-task network communication.</p>"},{"location":"internals/mantis-publish/","title":"Mantis Publish","text":"<p>The Mantis Publish runtime flow consists of three phases: connecting, event processing, and event delivery.</p>"},{"location":"internals/mantis-publish/#phase-1-connecting","title":"Phase 1: Connecting","text":"<p>Mantis Publish will only stream an event from your application into Mantis if there is a subscriber to that specific application instance with an MQL query that matches the event. Any [Mantis Job] can connect to these applications. However, it is a best practice to have [Source Jobs] connect to Mantis Publish applications. This is because Source Jobs provide several conveniences over regular Mantis Jobs, such as multiplexing events and connection management. By leveraging Source Jobs as an intermediary, Mantis Jobs are able to consume events from an external source without having to worry about lower-level details of that external source.</p> <p>This is possible through job chaining, which Mantis provides by default. When connecting to a Mantis Publish application, downstream Mantis Jobs will send subscription requests with an MQL query via HTTP to a Source Job. The Source Job will store these subscriptions in memory. These subscriptions are then fetched by upstream applications at the edge running the Mantis Publish library. Once the upstream edge Mantis Publish application is aware of the subscription, it will start pushing events downstream into the Source Job.</p> <p>Tip</p> <p>The Mantis Publish library not only handles subscriptions, but also takes care of discovering Source Job workers so you do not have to worry about Source Job rebalancing/autoscaling. For more information about Source Jobs see Mantis Source Jobs.</p>"},{"location":"internals/mantis-publish/#creating-stream-subscriptions","title":"Creating Stream Subscriptions","text":"<p>Clients such as Mantis Jobs connect to a Mantis Publish application by submitting a subscription represented by an HTTP request. Mantis Publish\u2019s <code>StreamManager</code> maintains these subscriptions in-memory. The <code>StreamManager</code> manages internal resources such as subscriptions, streams, and internal processing queues.</p> <p>Clients can create subscriptions to different event streams. There are two types:</p> <ol> <li><code>default</code> streams contain events emitted by applications that use the Mantis Publish library.</li> <li><code>log</code> streams contain events which may not be core to the application, such as log or general infrastructure events.</li> </ol>"},{"location":"internals/mantis-publish/#phase-2-event-processing","title":"Phase 2: Event Processing","text":"<p>Event processing within Mantis Publish takes place in two steps: event ingestion and event dispatch.</p>"},{"location":"internals/mantis-publish/#event-ingestion","title":"Event Ingestion","text":"<p>Event ingestion begins at the edge, in your application, by invoking <code>EventPublisher#publish</code> which places the event onto an internal queue for dispatching.</p>"},{"location":"internals/mantis-publish/#event-dispatch","title":"Event Dispatch","text":"<p>Events are dispatched by a drainer thread created by the Event Publisher. The drainer will drain events from the internal queue previously populated by <code>EventPublisher#publish</code>, perform some transformations, and finally dispatch events over the network and into Mantis.</p> <p>Events are transformed by an <code>EventProcessor</code> which processes events one at a time. Transformation includes the following steps:</p> <ol> <li>Masks sensitive fields in the event.<ul> <li>Sensitive fields are referenced by a blacklist defined by a configuration.</li> <li>This blacklist is a comma-delimited string of keys you wish to blacklist in your event.</li> </ul> </li> <li>Evaluates the MQL query of each subscription and builds a list of matching subscriptions.</li> <li>For each matching subscription, enriches the event with a superset of fields from the MQL query    from all the other matching subscriptions (see the following diagram).</li> <li>Sends this enriched event to all of the subscribers (see    Event Delivery below for the details).</li> </ol> <p>Info</p> <p>More Mantis Publish configuration options can be found in the mantis-publish configuration class.</p>"},{"location":"internals/mantis-publish/#phase-3-event-delivery","title":"Phase 3: Event Delivery","text":"<p>Mantis Publish delivers events on-demand. When a client subscribes to a Mantis Job that issues an MQL query, the Event Publisher delivers the event using non-blocking I/O.</p>"},{"location":"internals/runtime/","title":"Runtime","text":"<p>The Mantis Runtime is consists of two components:</p> <ol> <li>A single Mantis Master which coordinates the execution of Mantis Jobs.</li> <li>Independent Mantis Jobs which receive streams of events as input, transform events one at a    time, and produce streams of events as output.</li> </ol> <p>This page assumes familiarity with Mantis Job high-level concepts. An introduction can be found in Writing Mantis Jobs. This page presents internal details for Mantis Jobs.</p>"},{"location":"internals/runtime/#mantis-job-components","title":"Mantis Job Components","text":"<p>A Mantis Job consists of three components. Each one is based on a cold Observable that emits events to the next Observer in the Observable chain:</p> <ol> <li> <p>Source</p> <p>The Source component is an <code>RxFunction</code> that consumes data in a streaming, non-blocking, backpressure-aware manner from an external service.</p> </li> <li> <p>Processing Stage</p> <p>A Processing Stage component is based on an <code>RxFunction</code>. This is where event transformations take place. There can be many Processing Stages in a Mantis Job.</p> </li> <li> <p>Sink</p> <p>The Sink component is based on an <code>RxAction</code>. It asynchronously emits results of the final Processing Stage to an external service.</p> </li> </ol> <p>Note</p> <p>Mantis Jobs can consume events from typical external services such as APIs, databases, and Kafka topics. Mantis Jobs can also consume events emitted by other Mantis Jobs. This is referred to in Mantis as job chaining.</p>"},{"location":"internals/runtime/#runtime-lifecycle","title":"Runtime Lifecycle","text":"<p>The entry point for a Mantis Job is the Mantis Worker. The Mantis Master starts three primary services on a Mantis Worker when the Master boots the Worker up:</p> <ol> <li> <p>The virtual machine worker service interacts with the underlying substrate, currently    Mesos. This service subscribes to task updates and registers the Mantis Worker with Mesos    executor callbacks to launch Mantis Jobs.</p> </li> <li> <p>The heartbeat service sends HTTP heartbeat requests to notify the Mantis Master that the    worker is alive and available to process events.</p> </li> <li> <p>The stage executor dynamically loads bytecode for a Mantis Job, creates an in-memory    representation of all the metadata required to execute events for that Mantis Job, and    processes events for the current Processing Stage.</p> </li> </ol>"},{"location":"internals/runtime/#job-master-stage","title":"Job Master Stage","text":"<p>The Job Master autoscales Processing Stages. It can autoscale such stages independently of each other. If the configuration of a Job indicates that any Processing Stage is autoscalable, Mantis will automatically add a Job Master as the initial processing stage of the Job. This is a hidden stage that Job owners do not explicitly manage; instead, Mantis will create and configure a <code>JobMasterService</code>. This service creates a subscription to worker metrics via the <code>WorkerMetricHandler</code> and a <code>MetricsClient</code> which receives metrics over HTTP via SSE and sends them over to the <code>JobAutoScaler</code>.</p>"},{"location":"internals/runtime/#job-autoscaler","title":"Job Autoscaler","text":"<p>The Job autoscaler is based on a PID controller. Within this autoscaler are three controllers for CPU, memory, and network resources which continuously calculate an error value and apply corrections. Once the autoscaler makes a prediction, it delegates an API call to the Mantis Master to perform the scaling action on resources for a Processing stage.</p>"},{"location":"internals/runtime/#single-stage-and-multi-stage-jobs","title":"Single-Stage and Multi-Stage Jobs","text":"<p>A Job with only one Processing Stage is a single-stage Job. In such a case, the entire Job (Source, Processing Stage, and Sink) will execute on the current worker node.</p> <p>A Job with more than one Processing Stage is a multi-stage Job. In such a Job, the stage executor will first inspect the current component. If the current component is a Source, then the executor will execute it as a Source. Otherwise, it will inspect the context again to determine if current component is a Sink. If so, it will acquire a port and create a <code>SinkPublisher</code> to publish events to the next Job. Finally, if the component is a normal Processing Stage, then the executor will execute its transformations.</p>"},{"location":"internals/source-jobs/","title":"Source Jobs","text":"<p>Mantis Source Jobs are Mantis Jobs that fetch data from external sources. There are four types of Source Jobs:</p> <ol> <li> <p>Mantis Publish Source Jobs    read from their sources by using the Mantis Publish client library. As such, they do not apply MQL on events    themselves. Instead, they propagate the MQL queries upstream to Mantis Publish running on the external    source, which then applies the MQL queries to the events it produces and only then pushes those    events downstream to the Request Source Job.</p> </li> <li> <p>Kafka Source Jobs    consume events from Kafka and apply MQL to each incoming event.</p> </li> </ol>"},{"location":"internals/source-jobs/#composition-of-a-source-job","title":"Composition of a Source Job","text":"<p>A Source Job is composed of three components:</p> <ol> <li>the default Source</li> <li>custom Processing Stages including a tagging operator</li> <li>the default Sink consisting of an SSE operator</li> </ol> <p>For example, here is how you might declare a Kafka Source Job:</p> <pre><code>MantisJob\n.source(KafkaSource)\n.stage(\ngetAckableTaggingStage(),\nCustomizedAutoAckTaggingStage.config())\n.sink(new TaggedDataSourceSink(\nnew QueryRequestPreProcessor(),\nnew QueryRequestPostProcessor()))\n.lifecycle(...)\n.create();\n</code></pre>"},{"location":"internals/source-jobs/#source-rxfunction","title":"Source (RxFunction)","text":"<p>The Source in this example contains code that creates and manages connections to Kafka using the 0.10 high level consumer. It creates an Observable with backpressure semantics by leveraging the <code>SyncOnSubscribe</code> class.</p>"},{"location":"internals/source-jobs/#processing-stage-rxfunction","title":"Processing Stage (RxFunction)","text":"<p>The next stage in this Job is the Processing Stage which enriches events with metadata. This stage transforms events in the following way:</p> <ol> <li>Applies a user-defined pre-mapping function.<ul> <li>This is a Groovy function that takes a <code>Map&lt;String, Object&gt;</code> and returns a   <code>Map&lt;String, Object&gt;</code> referenced by a variable named <code>e</code>.</li> </ul> </li> <li>Filters out empty events.</li> <li>Inspects its internal subscription registry and enriches each event with all matching    subscriptions.<ul> <li>Subscriptions are represented by an MQL query and are registered when a consumer (e.g. Mantis   Job) subscribes to the Source Job.</li> <li> <p>Each event is enriched with fields specified by the projections of a subscription\u2019s MQL query,   as in the following illustration:</p> <p></p> </li> </ul> </li> </ol>"},{"location":"internals/source-jobs/#sink-rxaction","title":"Sink (RxAction)","text":"<p>In order for a consumer to consume events from a Source Job, the consumer connects to the Job\u2019s Sink.</p> <p>Consumers subscribe to a Source Job by sending a subscription request over HTTP to the Source Job\u2019s Sink.</p> <p>When a consumer connects to a Sink, the consumer must provide three query parameters:</p> <ol> <li><code>criterion</code> \u2014 An MQL query string</li> <li><code>clientId</code> \u2014 This is automatically generated if you use the Mantis client library; it defaults to    the Mantis Job ID</li> <li><code>subscriptionId</code> \u2014 This is used as a load-balancing mechanism for <code>clientId</code></li> </ol> <p>A consumer (represented as a client through <code>clientId</code>) may have many consumer instances (represented as susbcriptions through <code>subscriptionId</code>). Source Jobs use <code>clientId</code> and <code>subscriptionId</code> to broadcast and/or load balance events to consumers.</p> <p>Source Jobs will broadcast an event to all <code>clientId</code>s. This means that consumer instances with different <code>clientId</code>s will each receive the same event.</p> <p>However, Source Jobs will load balance an event within a <code>clientId</code>. This means that consumer instances with the same <code>clientId</code> but different <code>subscriptionId</code>s are effectively grouped together. Events with the same <code>clientId</code> are load balanced among its <code>subscriptionId</code>s.</p> Example of subscribing to a Source Job\u2019s Sink which outputs the results of a sine wave function: curl \"http://instance-address:port?clientId=myId&amp;subscriptionId=mySubscription&amp;criterion=select%20*%20where%20true\"<pre>\n\ndata: {\"x\": 60.000000, \"y\": -3.048106}\n\ndata: {\"x\": 100.000000, \"y\": -5.063656}\n\ndata: {\"x\": 26.000000, \"y\": 7.625585}\n\n\u22ee</pre> <p>Sinks have a pre-processor (<code>QueryRequestPreProcessor</code>), a post-processor (<code>QueryRequestPostProcessor</code>), and a router:</p> <ul> <li> <p>The pre-processor is an RxFunction that registers the consumer\u2019s query, with their <code>clientId</code>   and <code>subscriptionId</code>, into an in-memory cache called a <code>QueryRefCountMap</code> when a consumer instance   connects to the Sink. This registers queries so that the Source Job can apply them to events as   those events are ingested by the Source Job.</p> </li> <li> <p>The post-processor is an RxFunction that de-registers subscriptions from the   <code>QueryRefCountMap</code> when a consumer instance disconnects from the Sink. The Source Job removes   removes the <code>clientId</code> entirely from the <code>QueryRefCountMap</code> only when all of its <code>subscriptionId</code>s   have been removed.</p> </li> <li> <p>The router routes incoming events for a <code>clientId</code>s to its subscriptions. It does this by   using a drainer called a <code>ChunkProcessor</code> to drain events from an internal queue on an   interval and randomly distribute the events to subscriptions.</p> </li> </ul> <p>Note</p> <p>Typically, subscriptions to a Source Job come from other Mantis Jobs. However, because subscriptions are SSE endpoints, you can subscribe to Source Jobs over that same SSE endpoint to view the Job\u2019s output for debugging purposes.</p>"},{"location":"internals/source-jobs/#caveats","title":"Caveats","text":"<p>Source Jobs are single-stage Mantis Jobs that perform projection and filtering operations. MQL queries containing <code>groupBy</code>, <code>orderBy</code>, and <code>window</code> are ignored. These clauses are interpreted into RxJava operations and run by downstream Mantis Jobs.</p> <p>Mantis Publish-based Source Jobs do not autoscale. Autoscaling Mantis Publish-based Source Jobs requires future work to reshuffle connections among all Source Job instances and their upstream Mantis Publish connections.</p>"},{"location":"internals/mantis-jobs/introduction/","title":"Introduction","text":"<p>A Mantis Job is a JVM-based stream processing application that takes in an Observable stream of data items, transforms this stream by using RxJava operators, and then outputs the results as another Observable stream.</p> <p>RxJava Observables can be visualized by using \u201cmarble diagrams\u201d:</p> <p></p> <p>You can combine multiple RxJava operators to transform an Observable stream of items in many ways:</p> <p></p> <p>The above diagram shows a Mantis Job composed of two operators that process an input stream to compose an output stream. The first operator, <code>map</code>, emits a new Observable for each item emitted by the source Observable. The second operator, <code>merge</code>, emits each item emitted by those Observables as a fresh Observable stream.</p> <p>There is an enormous wealth of ways in which you can transform streams of data by using RxJava Observable operators.</p> <p>If the volume of data to be processed is too large for a single worker to handle, you can \u201cdivide and conquer\u201d by grouping and dividing the operators across various processing stages, as in the following diagram:</p> <p></p>"},{"location":"internals/mantis-jobs/introduction/#mantis-job-clusters","title":"Mantis Job Clusters","text":"<p>You define a Job Cluster before you submit Jobs. A Job Cluster is a containing entity for Jobs. It defines metadata and certain service-level agreements (SLAs). Job Clusters ease Job lifecycle management and Job revisioning.</p> <p>For example, by setting the SLA of a Job Cluster to Min=1 and Max=1, you ensure that exactly one Job instance is always running for that Cluster. The Job Cluster also has default Job parameters that any new Jobs submitted to the Cluster inherit. You can update new Job artifacts into the Job Cluster so that the next Job submission picks up the latest version.</p>"},{"location":"internals/mantis-jobs/introduction/#components-of-a-mantis-job","title":"Components of a Mantis Job","text":"<p>A Mantis Job has three components, each of which has a corresponding chapter in this documentation:</p> <ol> <li> <p>Source \u2014 Fetches data from an external source and makes it available in the form of    an Observable.</p> </li> <li> <p>Processing Stage \u2014 Transforms the Observable stream by means of a variety of    operators.</p> </li> <li> <p>Sink \u2014 Pushes the resulting Observable out in the form of a fresh stream.</p> </li> </ol> <p></p>"},{"location":"internals/mantis-jobs/introduction/#directory-structure-of-a-mantis-job","title":"Directory Structure of a Mantis Job","text":"<p>In addition to the source files, Mantis requires some meta-files to be present in the Job artifact.</p> <p>Here is a sample directory structure:</p> <pre><code>src/\n  - main/\n    - java/\n      - com/\n        - myorg/\n          - MyJob.java\n    - resources/\n      - META-INF/\n        - services/\n          - io.mantisrx.runtime.MantisJobProvider\n      - job.properties\n      - job-test.properties\n</code></pre> <ul> <li><code>io.mantisrx.runtime.MantisJobProvider</code> (required) \u2014 lists the fully-qualified name of the Java class that implements the <code>MantisJobProvider</code> interface</li> <li><code>job.properties</code> and <code>job-test.properties</code> (optional) \u2014 required only if you are initializing the platform via the <code>.lifecycle()</code> method</li> </ul>"},{"location":"internals/mantis-jobs/introduction/#creating-a-mantis-job","title":"Creating a Mantis Job","text":"<p>To create a Mantis Job, call <code>MantisJob\u2026create()</code>. When you do so, interpolate the following builder methods. The first three \u2014 <code>.source()</code>, <code>.stage()</code>, and <code>.sink()</code> \u2014 are required, they must be the first three of the methods that you call, and you must call them in that order:</p> <ol> <li><code>.source(AbstractJobSource)</code> \u2014 required, see The Source Component</li> <li><code>.stage(Stage, stageConfig)</code> \u2014 required, (call this one or more times) see The Processing Stage Component</li> <li><code>.sink(Sink)</code> \u2014 required, see The Sink Component</li> <li><code>.lifecycle(Lifecycle)</code> \u2014 optional, allows for start-up and shut-down hooks</li> <li><code>.parameterDefinition(ParameterDefinition)</code> \u2014 optional, (call this zero to many times) define any parameters your job requires here</li> <li><code>.metadata(Metadata)</code> \u2014 optional, (call this zero to many times) define your job name and description here</li> </ol> of this class this method returns an object of this class <code>MantisJob</code> \u27f6 <code>source()</code> \u27f6 <code>SourceHolder</code>\u2936 <code>SourceHolder</code> \u27f6 <code>stage()</code> \u27f6 <code>Stages</code>\u2936 [ <code>Stages</code> \u27f6 <code>stage()</code> \u27f6 <code>Stages</code> ]\u2936 <code>Stages</code> \u27f6 <code>sink()</code> \u27f6 <code>Config</code>\u2936 [ <code>Config</code> \u27f6 <code>lifecycle()</code> \u27f6 <code>Config</code> ]\u2936 [ <code>Config</code> \u27f6 <code>parameterDefinition()</code> \u27f6 <code>Config</code> ]\u2936 [ <code>Config</code> \u27f6 <code>metadata()</code> \u27f6 <code>Config</code> ]\u2936 <code>Config</code> \u27f6 <code>create()</code> \u27f6 <code>Job</code>"},{"location":"internals/mantis-jobs/introduction/#lifecycle-management","title":"Lifecycle Management","text":"<p>You can establish start-up and shut-down procedures for your Mantis Job by means of the <code>.lifecycle().</code> builder method.</p> <p>Pass this method a <code>Lifecycle</code> object, that is to say, an object that implements the following methods:</p> <ul> <li><code>startup()</code> \u2014 initialize arbitrary application configs, perform dependency injection, and any long running or shared service libraries.</li> <li><code>shutdown()</code> \u2014 gracefully close connections, shut down long running or shared service libraries, and general process cleanup.</li> <li><code>getServiceLocator()</code> \u2014 returns a <code>ServiceLocator</code> that implements the <code>service(key)</code> method. Implement this method to return your dependency injection object such as Guice.</li> </ul>"},{"location":"internals/mantis-jobs/introduction/#defining-parameters","title":"Defining Parameters","text":"<p>To create a Parameter in order to pass it to the <code>.parameterDefinition()</code> builder method of the <code>MantisJob</code> builder, use the following <code>new ParameterVariety()\u2026build()</code> builder methods:</p> <ul> <li><code>.name(string)</code> \u2014 a user-friendly name for your Parameter</li> <li><code>.description(string)</code> \u2014 a user-friendly description of your Parameter</li> <li><code>.defaultValue(value)</code> \u2014 the value of this Parameter if the Job does not override it</li> <li><code>.validator(Validator)</code> \u2014 a way of checking the proposed Parameter values for validity so bad values can be rejected before you submit the Job</li> <li><code>.required()</code> \u2014 call this builder method if the Job must provide a value for this Parameter</li> </ul> <p>There are some built-in Parameter varieties you can choose from that correspond to common data types:</p> <ul> <li><code>BooleanParameter</code></li> <li><code>DoubleParameter</code></li> <li><code>IntParameter</code></li> <li><code>StringParameter</code></li> </ul>"},{"location":"internals/mantis-jobs/introduction/#validators","title":"Validators","text":"<p>There are some standard Validators you can choose from that cover some common varieties of parameters:</p> <ul> <li><code>Validators.range(start, end)</code> \u2014 will reject as invalid Parameter values that do not lie between the indicated start and end numerical values (where start and end themselves are valid Parameter values)</li> <li><code>Validators.notNullOrEmpty()</code> \u2014 will reject empty strings or null values</li> <li><code>Validators.alwaysPass()</code> \u2014 will not reject any Parameter values as invalid</li> </ul>"},{"location":"internals/mantis-jobs/introduction/#example","title":"Example","text":"<p>For example:</p> <pre><code>myStringParameter = new StringParameter().name(\"MyParameter\")\n.description(\"This is a human-friendly description of my parameter\")\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"SomeValue\")\n.required()\n.build();\n</code></pre>"},{"location":"internals/mantis-jobs/introduction/#defining-metadata","title":"Defining Metadata","text":"<p>To create metadata in order to pass it to the <code>.metadata()</code> builder method of the <code>MantisJob</code> builder, use the following <code>new Metadata.Builder()\u2026build()</code> builder methods:</p> <ul> <li><code>.name(string)</code></li> <li><code>.description(string)</code></li> </ul> <p>For example:</p> <pre><code>myMetadata = new Metadata.Builder().name(\"MyMetadata\")\n.description(\"Description of my metadata\")\n.build();\n</code></pre>"},{"location":"internals/mantis-jobs/processing-stage/","title":"Processing Stage","text":"<p>A Processing Stage component of a Mantis Job processes the stream of data by [transforming] it in some way. You can combine multiple Processing Stages into a single Job, or you can create a Job that consists of a single Processing Stage.</p> <p>In its simplest form, a Processing Stage is a chain of RxJava operators operating on the Observable provided by the Source component.</p> <p>Transformations can be broadly categorized into two types: Scalar or Grouped. There are four varieties of Processing Stage, based on what type of transformation they accomplish:</p> <ol> <li>Scalar-to-Scalar \u2014 Also known as \u201cnarrow transformation,\u201d this variety of Stage converts an <code>Observable&lt;T&gt;</code> into an <code>Observable&lt;R&gt;</code>. Such a Stage extends the <code>ScalarToScalar</code> class. </li> <li>Scalar-to-Key/Scalar-to-Group \u2014 Also known as \u201cwidening transformation,\u201d this variety of Stage groups each element emitted by the source Observable by key. Typically you use such a Stage when you build a map/reduce-style job in which you need to perform stateful computations but the volume of data is too large to fit on a single worker. In such a model, the incoming data is divided into multiple streams, one per group. A subsequent State will typically to the stateful computation (for instance, calculating percentiles for the group). The purpose of this Scalar-to-Key State is to tag each incoming element with the group that it belongs to. Mantis then takes care of routing all traffic for the same group to the same worker in the subsequent stage.<ol> <li>Scalar-to-Key \u2014 This is the legacy way of grouping data (it is more elegant but comes with a performance penalty). You extend the <code>ScalarToKey</code> class and transform an <code>Observable&lt;T&gt;</code> into an <code>Observable&lt;GroupedObservable&lt;K,R&gt;&gt;</code> (where <code>K</code> is the key). See the RxJava <code>groupBy</code> operator for more information.</li> <li>Scalar-to-Group \u2014 This is a more efficient way to group data. You extend the <code>ScalarToGroup</code> class and transform an <code>Observable&lt;T&gt;</code> into an <code>Observable&lt;MantisGroup&lt;K,R&gt;&gt;</code> (where <code>K</code> is the key). This avoids the overhead associated with creating a <code>GroupedObservable</code> which can limit the number of groups it is possible to create.</li> </ol> </li> <li>Key-to-Scalar/Group-to-Scalar \u2014 Once you have split a stream, you need a stage that can take grouped data and return it to a scalar form.<ol> <li>Key-to-Scalar \u2014 This is the legacy method and is designed for streams that have been split via a <code>ScalarToKey</code> Stage. You extend the <code>KeyToScalar</code> class and transform a <code>GroupedObservable&lt;K,T&gt;</code> into an <code>Observable&lt;T&gt;</code>.</li> <li>Group-to-Scalar \u2014 This is the newer, faster method. It is less elegant, in that you must transform an <code>Observable&lt;MantisGroup&gt;</code> that contains payloads from all groups, and you must therefore manage the per-group state. Typically you would do this via a map that holds per-group state and evicts entries that haven\u2019t been touched recently. This method has much better performance than the Key-to-Scalar method because it omits the overhead around RxJava\u2019s <code>GroupedObservable</code>.</li> </ol> </li> <li>Key-to-Key/Group-to-Group \u2014 You can further split a keyed stream by grouping it again if you have some use case that requires this.</li> </ol>"},{"location":"internals/mantis-jobs/sink/","title":"Sink","text":"<p>The Sink component of a Mantis Job serves two purposes:</p> <ol> <li> <p>Trigger job execution when a client subscribes to a Job</p> <p>RxJava cold Observables have a lazy execution model. Execution begins only when someone subscribes to the Observable. A Mantis Job is a complex Observable chain, and to trigger the execution of such a Job, somebody needs to subscribe to it. This happens in the Sink component.</p> </li> <li> <p>Output the results of job execution in a streaming fashion</p> <p>Once you are done processing the input stream in the Processing Stage component, you need to figure out what to do with the results. Most jobs that write to some other system do so within the Processing Stage component itself (in such a case, the Sink component is usually used for debugging purposes).</p> </li> </ol> <p>To create a Sink component, you implement the <code>Sink</code> interface:</p> <pre><code>import io.mantisrx.runtime.Context;\nimport io.mantisrx.runtime.PortRequest;\nimport rx.Observable;\nimport rx.functions.Action3;\n\npublic interface Sink&lt;T&gt; extends Action3&lt;Context, PortRequest, Observable&lt;T&gt;&gt; { }\n</code></pre>"},{"location":"internals/mantis-jobs/sink/#built-in-sinks","title":"Built-in Sinks","text":"<p>Some Sinks are provided by Mantis. To get access to these Sinks, add this line to your source:</p> <pre><code>import io.mantisrx.runtime.sink.Sinks;\n</code></pre>"},{"location":"internals/mantis-jobs/sink/#sse-sink","title":"SSE Sink","text":"<p>The SSE Sink is commonly used. It makes the results of your Stage transformation available in the form of an SSE stream.</p> <p>To get an SSE Sink, pass an encoder function to the <code>sse()</code> method that accepts the data to be streamed as input and outputs data encoded as needed. The following example attaches a sink to a Mantis Job that passes the data from the last Processing Stage of the job, unchanged, to the SSE stream:</p> <pre><code>return MantisJob. \u2026 .sink(Sinks.sse((String data) -&gt; data)). \u2026 .create();\n</code></pre>"},{"location":"internals/mantis-jobs/sink/#sysout-sink","title":"sysout Sink","text":"<p>The <code>sysout</code> Sink simply outputs the results from the previous Processing Stage directly to sysout. For example:</p> <pre><code>return MantisJob. \u2026 .sink(Sinks.sysout()). \u2026 .create();\n</code></pre>"},{"location":"internals/mantis-jobs/sink/#eagersubscribe-sink","title":"eagerSubscribe Sink","text":"<p>A typical Sink subscribes to the output of the previous Processing Stage at some point during the call to its <code>call()</code> method. You may want some of your Mantis Jobs to start executing as soon as you launch them. These include Jobs that run perpetually and power things like alerts, dashboards, and so forth.</p> <p>You can modify a Sink with <code>eagerSubscribe()</code> to create a Sink that that instead subscribes to the output of the previous Processing Stage immediately when its <code>call()</code> method is called, even if it has more processing to do within that method before it can respond to the output. This will start your Mantis Job more quickly, but may mean some of its initial data is dropped.</p> <p>For example:</p> <pre><code>return MantisJob. \u2026 .sink(Sinks.eagerSubscribe(Sinks.sse((String data) -&gt; data))). \u2026 .create();\n</code></pre> <p>Here is a more complete example:</p> <pre><code>return MantisJob        // Reuse existing class that does all the plumbing of connecting to source jobs\n.source(new JobSource())\n\n// Groups requests by ESN\n.stage(new GroupByStage(), GroupByStage.config())\n\n// Computes sum over a window\n.stage(new FastAggregateStage(), FastAggregateStage.config())\n\n// Collects the data and makes it availabe over SSE\n.stage(new CollectStage(), CollectStage.config())\n\n// Reuse built in sink that eagerly subscribes and delivers data over SSE\n.sink(Sinks.eagerSubscribe(\nSinks.sse((String data) -&gt; data)\n))\n\n// Here we define the job parameter overrides        \n\n// The query sent to the source job.\n// Here we fetch the esn for all requests hitting the source\n.parameterDefinition(new StringParameter()\n.name(MantisSourceJobConnector.MANTIS_SOURCEJOB_CRITERION)\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"SELECT customer_id, client_ip WHERE true\")\n.build())    .metadata(new Metadata.Builder()\n.name(\"GroupByIp\")\n.description(\"Connects to a source job and counts the number of requests sent from each ip within a window\")\n.build())\n.create();\n</code></pre>"},{"location":"internals/mantis-jobs/sink/#tomany-sink","title":"toMany Sink","text":"<p>You can hook up multiple Sinks to the same final Stage of a Job by using the <code>toMany</code> Sink. To do this, pass each Sink to the <code>toMany()</code> method. For example:</p> <pre><code>return MantisJob. \u2026 .sink(Sinks.toMany(Sinks.sysout(), Sinks.sse((String data) -&gt; data))). \u2026 .create();\n</code></pre>"},{"location":"internals/mantis-jobs/sink/#custom-sinks","title":"Custom Sinks","text":"<p>If you do not want to use one of the provided Sinks, or if you need to customize one of those (for instance, if you need access to the query parameters supplied by a client who is connecting into your Job, or if you need pre and post hooks to perform operations as a new client connects or disconnects), you can create your own Sink. To do so, implement the <code>Sink</code> interface.</p> <p>The <code>ServerSentEventsSink</code> builder takes three parameters:</p> <ol> <li> <p>Preprocess function</p> <p>this callback gives you access to the query parameters; it is invoked before job execution begins</p> </li> <li> <p>Postprocess function</p> <p>this callback allows you to perform any clean-up actions; it is invoked just after the client connection is terminated</p> </li> <li> <p>Predicate function</p> <p>allows you to filter your output stream based on the given predicate, which allows you to filter based on query parameters sent by the client</p> </li> </ol> <p>Here is an example of a custom Sink that uses the <code>ServerSentEventsSink</code> builder:</p> <pre><code>package com.netflix.mantis.sourcejob;\n\nimport io.mantisrx.runtime.Context;\nimport io.mantisrx.runtime.PortRequest;\nimport io.mantisrx.runtime.sink.ServerSentEventsSink;\nimport io.mantisrx.runtime.sink.Sink;\nimport io.mantisrx.runtime.sink.predicate.Predicate;\n\nimport java.util.List;\nimport java.util.Map;\n\nimport rx.Observable;\nimport rx.functions.Func1;\nimport rx.functions.Func2;\n\nimport com.google.common.base.Charsets;\n\npublic class SourceSink implements Sink&lt;String&gt; {\n\nFunc2&lt;Map&lt;String,List&lt;String&gt;&gt;,Context,Void&gt; preProcessor = new NoOpProcessor();\nFunc2&lt;Map&lt;String,List&lt;String&gt;&gt;,Context,Void&gt; postProcessor = new NoOpProcessor();\nprivate String clientId = \"DEFAULT_CLIENT_ID\";\n\nstatic class NoOpProcessor implements Func2&lt;Map&lt;String,List&lt;String&gt;&gt;,Context,Void&gt; {\n@Override\npublic Void call(Map&lt;String, List&lt;String&gt;&gt; t1, Context t2) {\nreturn null;\n}\n}\n\npublic SourceSink() {\n}\n\npublic SourceSink(Func2&lt;Map&lt;String,List&lt;String&gt;&gt;,Context,Void&gt; preProcessor,\nFunc2&lt;Map&lt;String,List&lt;String&gt;&gt;,Context,Void&gt; postProcessor,\nString mantisClientId) {\nthis.postProcessor = postProcessor;\nthis.preProcessor = preProcessor;\nthis.clientId = mantisClientId;\n}\n\n@Override\npublic void call(Context context, PortRequest portRequest,\nObservable&lt;String&gt; observable) {\n\nobservable = observable.filter(new Func1&lt;String,Boolean&gt;() {\n@Override\npublic Boolean call(String t1) {\nreturn !t1.isEmpty();\n}\n});\n\nServerSentEventsSink&lt;String&gt; sink = new ServerSentEventsSink.Builder&lt;String&gt;()\n.withEncoder(new Func1&lt;String, String&gt;() {\n@Override\npublic String call(String data) {\nreturn data;\n}\n})\n.withPredicate(new Predicate&lt;String&gt;(\"description\",new EventFilter(clientId)))\n.withRequestPreprocessor(preProcessor)\n.withRequestPostprocessor(postProcessor)\n.build();\n\nobservable.subscribe();\n\nsink.call(context, portRequest, observable);\n}\n\npublic static void main(String [] args) {\nString s = \"{\\\"amazon.availability-zone\\\":\\\"us-east-1e\\\",\\\"status\\\":200,\\\"type\\\":\\\"EVENT\\\",\\\"matched-clients\\\":\\\"client6\\\",\\\"currentTime\\\":1409595016697,\\\"duration-millis\\\":172}\";\n\nbyte [] barr = s.getBytes(Charsets.UTF_8);\nSystem.out.println(\"size: \" + barr.length);\n}\n}\n</code></pre>"},{"location":"internals/mantis-jobs/source/","title":"Source","text":"<p>To implement a Mantis Job Source component, you must implement the <code>io.mantisrx.runtime.Source</code> interface. A Source returns <code>Observable&lt;Observable&lt;T&gt;&gt;</code>, that is, an Observable that emits Observables. Each of the emitted Observables represents a stream of data from a single target server.</p>"},{"location":"internals/mantis-jobs/source/#varieties-of-sources","title":"Varieties of Sources","text":"<p>Sources can be roughly divided into two categories:</p> <ol> <li>Sources that read data from the output of other Mantis Jobs<ol> <li>this may include Source Jobs (more on this below)</li> <li>or ordinary Mantis Jobs</li> </ol> </li> <li>Custom sources that read data directly from Amazon S3, SQS, Apache Kafka, etc.</li> </ol>"},{"location":"internals/mantis-jobs/source/#mantis-job-sources","title":"Mantis Job Sources","text":"<p>You can string Mantis Jobs together by using the output of one Mantis Job as the input to another. This is useful if you want to break up your processing into multiple, reusable components and to take advantage of code and data reuse.</p> <p>In such a case, you do not have access to the complete set of Mantis Query Language (MQL) capabilities that you do in the case of a Source Job, but you can use MQL in client mode.</p>"},{"location":"internals/mantis-jobs/source/#connecting-to-a-mantis-job","title":"Connecting to a Mantis Job","text":"<p>To connect to a Mantis Job, use a <code>JobSource</code> when you call <code>MantisJob.create()</code> \u2014 declare the following parameters when you use this class:</p> <ol> <li><code>sourceJobName</code> (required) \u2014 the name of any valid Job Cluster (not necessarily a \u201cSource Job\u201d) </li> <li><code>sample</code> (optional) \u2014 use this if you want to sample the output <code>sample</code> times per second, or set this to <code>-1</code> to disable sampling </li> </ol> <p>For example:</p> <pre><code>MantisJob.source(new JobSource())\n.stage(\u2026)\n.sink(\u2026)\n.parameterDefinition(new StringParameter().name(\"sourceJobName\")\n.description(\"The name of the job\")\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"MyDefaultJob\")\n.build())\n.parameterDefinition(new IntParameter().name(\"sample\")\n.description(\"The number of samples per second\")\n.validator(Validators.range(-1, 10000))\n.defaultValue(-1)\n.build())\n.lifecycle(\u2026)\n.metadata(\u2026)\n.create();\n</code></pre>"},{"location":"internals/mantis-jobs/source/#source-job-sources","title":"Source Job Sources","text":"<p>Mantis has a concept of Source Jobs which are Mantis Jobs with added conveniences and efficiences that simplify accessing data from certain sources. Your job can simply connect to a source job as its data source rather than trying to retrieve the data from its native home. There are two advantages to this approach:</p> <ol> <li>Source Jobs handle all of the implementation details around interacting with the native data    source.</li> <li>Source Jobs come with a simple query interface based on the Mantis Query Language (MQL),    which allows you to filter the data from the source before processing it. In the case of source    jobs that fetch data from application servers directly, this filter gets pushed all the way to    those target servers so that no data flows unless someone is asking for it.</li> <li>Source Jobs reuse data so that multiple matching MQL queries are forwarded downstream instead of    paying the cost to fetch and serialize/deserialize the same data multiple times from the upstream    source.</li> </ol>"},{"location":"internals/mantis-jobs/source/#broadcast-mode","title":"Broadcast Mode","text":"<p>By default, Mantis will distribute the data that is output from the Source Job among the various workers in the processing stage of your Mantis Job. Each of those workers will get a subset of the complete data from the Source Job.</p> <p>You can override this by instructing the Source Job to use \u201cbroadcast mode\u201d. If you do this, Mantis will send the complete set of data from the Source Job to every worker in your Job.</p>"},{"location":"internals/mantis-jobs/source/#connecting-to-a-source-job","title":"Connecting to a Source Job","text":"<p>Since Source Jobs are fundamentally Mantis Jobs, you should use a <code>JobSource</code> when you call <code>MantisJob.create()</code> to connect to a particular Source Job. The difference is that you should pass in additional parameters:</p> <ol> <li><code>sourceJobName</code> (required) \u2014 the name of the source Job Cluster you want to connect to</li> <li><code>sample</code> (required) \u2014 use this if you want to sample the output <code>sample</code> times per second, or set this to <code>-1</code> to disable sampling</li> <li><code>criterion</code> (required) \u2014 a query expression in MQL to filter the source</li> <li><code>clientId</code> (optional) \u2014 by default, the <code>jobId</code> of the client Job; the Source Job uses this to distribute data between all the subscriptions of the client Job</li> <li><code>enableMetaMessages</code> (optional) \u2014 the source job may occasionally inject meta messages (with the prefix <code>mantis.meta.</code>) that indicate things like data drops on the Source Job side.</li> </ol> <p>For example:</p> <pre><code>MantisJob.source(new JobSource())\n.stage(\u2026)\n.sink(\u2026)\n.parameterDefinition(new StringParameter().name(\"sourceJobName\")\n.description(\"The name of the job\")\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"MyDefaultSourceJob\")\n.build())\n.parameterDefinition(new IntParameter().name(\"sample\")\n.description(\"The number of samples per second\")\n.validator(Validators.range(-1, 10000))\n.defaultValue(-1)\n.build())\n.parameterDefinition(new StringParameter().name(\"criterion\")\n.description(\"Filter the source with this MQL statement\")\n.validator(Validators.notNullOrEmpty())\n.defaultValue(\"true\")\n.build())\n.parameterDefinition(new StringParameter().name(\"clientId\")\n.description(\"the ID of the client job\")\n.validator(Validators.alwaysPass())\n.build())\n.parameterDefinition(new BooleanParameter().name(\"enableMetaMessages\")\n.description(\"Is the source allowed to inject meta messages\")\n.validator(Validators.alwaysPass())\n.defaultValue(\"true\")\n.build())\n.lifecycle(\u2026)\n.metadata(\u2026)\n.create();\n</code></pre>"},{"location":"internals/mantis-jobs/source/#custom-sources","title":"Custom Sources","text":"<p>Custom sources may be implemented and used to access data sources for which Mantis does not have a Source Job. Implementers are free to implement the Source interface to fetch data from an external source. Here is an example in a source which implements the Source interface to consume data from Kafka.</p>"},{"location":"internals/mantis-jobs/source/#learning-when-source-data-is-incomplete","title":"Learning When Source Data is Incomplete","text":"<p>You may want to know whether or not the stream you are receiving from your source is complete. Streams may be incomplete for a number of reasons:</p> <ol> <li>A connection to one or more of the Source Job workers is lost.</li> <li>A connection exists but no data is flowing.</li> <li>Data is intentionally dropped from a source because of the backpressure strategy you are using.</li> </ol> <p>You can use the following <code>boolean</code> method within your <code>JobSource#call</code> method to determine whether or not all of your client connections are complete:</p> <pre><code>DefaultSinkConnectionStatusObserver.getInstance(true).isConnectedToAllSinks()\n</code></pre>"},{"location":"operate/autoscaling/","title":"Autoscaling","text":"<p>Being a cloud-native platform Mantis supports autoscaling out-of-the-box. Both the agent cluster and the Mantis Jobs can be configured to autoscale. </p> <p>You can define a policy for your Jobs in which they autoscale their resources based on the dynamic needs resulting from variation in the input data they process. </p> <p>This provides two benefits:</p> <ol> <li>You can define Jobs to process data without provisioning for peak usage all the time.</li> <li>Mantis uses cluster resources optimally without leaving resources idle.</li> </ol>"},{"location":"operate/autoscaling/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Your Mantis Jobs are composed of in part of Processing Stages, with each stage responsible for a different stream processing task. Because different stages may have different computational needs, each stage has its own autoscaling policy.</p> <p>A Processing Stage is further subdivided into workers. A worker is the smallest unit of work that is scheduled. Each worker requests a certain number of CPUs, some amount of memory, and a certain amount of network bandwidth.</p> <p>When a Processing Stage scales, the number of workers in that stage increases or decreases (the resources that Mantis allocates to an individual worker in the stage do not change as a result of scaling).</p>"},{"location":"operate/autoscaling/#scaling-a-processing-stage-manually","title":"Scaling a Processing Stage Manually","text":"<p>You may define a Processing Stage as scalable without defining an autoscaling policy for it. In such a case the stage is considered manually scalable and you can scale it by means of the Mantis UI or the Mantis API.</p>"},{"location":"operate/autoscaling/#setting-an-autoscaling-policy","title":"Setting an Autoscaling Policy","text":"<p>Warning</p> <p>You should take care that your autoscaling strategies do not contradict each other. For example, if you set a CPU-based strategy and a network-based strategy, one may want to trigger a scale-up and the other a scale-down at the same time.</p> <p>You define the autoscaling policy for a Processing Stage by setting the following parameters:</p> <ul> <li>Min and Max number of workers \u2014 This sets how many workers Mantis will guarantee to be working   within the Processing Stage at any particular time.</li> <li>Increment and decrement values \u2014 This indicates how many workers are added to or removed from   a stage each time the stage autoscales up or down.</li> <li>Cooldown seconds \u2014 This indicates how many seconds to wait after a scaling operation has been   completed before beginning another scaling operation.</li> <li>Strategies \u2014 See autoscaling strategies for details.</li> </ul> <p>The following example shows how you might establish the autoscaling policy for a stage in the Mantis UI:</p> <p></p> <p>The illustration above shows a stage with an autoscaling policy that specifies a minimum of 5 and a maximum of 20 workers. It uses a single strategy, that of network bandwidth usage.</p>"},{"location":"operate/autoscaling/#autoscaling-scenarios","title":"Autoscaling Scenarios","text":"<p>There are four varieties of autoscaling scenarios that you should consider for your Mantis Job:</p> <ol> <li> <p>The Processing Stage connects to a cold Source, such as a Kafka topic.</p> <p>Autoscaling works well for this type of stage (the initial stage in a Job that connects to the Source). For example, if your stage connects to a Kafka source, a change in the number of workers in the first stage of your Mantis Job causes the Kafka client to redistribute the partitions of the topic among the new number of workers.</p> </li> <li> <p>The Processing Stage connects to a hot source, such as a working server.</p> <p>The Source stage (stage #1) will have to re-partition the Source servers after an autoscale event on the Processing Stage. This is mainly a concern for Source Jobs. Upon receipt of a modified number of workers, each worker re-partitions the current servers into its own index of the new total. This results in a new list of servers to connect to (for both scale up and scale down), some of which may be already connected. Making a new connection to a Source server evicts any old existing connection from the same Job. This guarantees that no duplicate messages are sent to a Mantis Job. (The solution for this scenario is currently in development.)</p> <p>Rewrite this; it\u2019s not very clear.</p> </li> <li> <p>The Processing Stage connects to another Mantis Job.</p> <p>In this case, the initial Processing Stage in a Job that connects to the output of the previous Mantis Job has strong connectivity into the Source Job via the use of Mantis Java client. In suc a case, all workers from this Processing Stage connect to all workers of the source Job\u2019s Sink. Therefore, autoscaling this type of Job works well. </p> </li> <li> <p>The Processing Stage connects to a previous Processing Stage in the same Mantis Job.</p> <p>Each Processing Stage is strongly connected to its previous Processing Stage. Therefore, autoscaling of this type typically works well. However, a Processing Stage following a grouped stage (a Processing Stage that does a <code>group by</code> operation) receives a grouped Observable or <code>MantisGroup</code>. When Mantis scales such a grouped stage, these groups are repartitioned on to the new number of workers. The Processing Stage following such a grouped stage must, therefore, be prepared to potentially receive a different set of groups after a rescale operation.</p> <p>How does a subsequent stage learn that the previous stage has autoscaled?</p> <p>Note that the number of groups resulting from a <code>group by</code> operation is the maximum limit on the number of workers that can be expected to work on such groups (unless a subsequent processing stage subdivides those groups). In the following illustrations, a processing stage that does a <code>group by</code> operation groups the incoming data into three groups, each one of which is handled by a single worker in the subsequent processing stage. When that second stage scales up and adds another worker, that worker remains idle and does not assist in processing data because there are not enough groups to distribute among the larger number of workers.</p> <p>Before autoscaling: </p> <p>After autoscaling: </p> </li> </ol>"},{"location":"operate/autoscaling/#updating-autoscalable-jobs","title":"Updating Autoscalable Jobs","text":"<p>To upload a Mantis Job when you have new code to push, upload the <code>.jar</code> or <code>.zip</code> artifact file to Mantis, and make any necessary adjustments to its behavior and policies by using the Mantis UI.</p> <p>You can also do this in two ways via the Mantis API:</p> <ol> <li>Update the Job Cluster with a new version    for its artifact file along with new scheduling information. This updated JAR and scheduling info    are available to use with the next Job submission. However, currently-running Jobs continue to    run with whatever artifact file they were started with.</li> <li>Quick update the Job Cluster with only a new    artifact file version and submit a new Job with it. The new Job is submitted by using the    scheduling info from the last Job submitted. </li> </ol> <p>The latter of the above two is convenient not only because you provide the minimal information needed to update the Job. But, also, because when it picks up the scheduling info from the last Job submitted, if it is running, the new Job is started with the same number of workers as the last one. That is, if it had scaled up, the new Job starts scaled up as well.</p>"},{"location":"operate/autoscalingstrategies/","title":"Autoscaling Strategies","text":"<p>There are various strategies for autoscaling. These strategies affect when an autoscale activity will occur and also how many workers to scale up/down a stage. They fall into 2 main categories. Rule based strategies monitor a specific resource and scale up/down when a certain threshold is reached. PID control based strategies pick a resource utilization level and scale up/down dynamically to maintain that level.</p>"},{"location":"operate/autoscalingstrategies/#rule-based-strategy","title":"Rule Based Strategy","text":"<p>Rule based strategy can be defined for the following resources:</p> Resource Metric <code>CPU</code> group: <code>ResourceUsage</code> name: <code>cpuPctUsageCurr</code> aggregation: <code>AVG</code> <code>Memory</code> group: <code>ResourceUsage</code> name: <code>totMemUsageCurr</code> aggregation: <code>AVG</code> <code>Network</code> group: <code>ResourceUsage</code> name: <code>nwBytesUsageCurr</code> aggregation: <code>AVG</code> <code>JVMMemory</code> group: <code>ResourceUsage</code> name: <code>jvmMemoryUsedBytes</code> aggregation: <code>AVG</code> <code>DataDrop</code> group: <code>DataDrop</code> name: <code>dropCount</code> aggregation: <code>AVG</code> <code>KafkaLag</code> group: <code>consumer-fetch-manager-metrics</code> name: <code>records-lag-max</code> aggregation: <code>MAX</code> <code>KafkaProcessed</code> group: <code>consumer-fetch-manager-metrics</code> name: <code>records-consumed-rate</code> aggregation: <code>AVG</code> <code>UserDefined</code> Metric is defined by user with job parameter <code>mantis.jobmaster.autoscale.metric</code> in this format <code>{group}::{name}::{aggregation}</code>. <p>Each strategy has the following parameters:</p> Name Description <code>Scale down below percentage</code> When the aggregated value for all workers falls below this value, the stage will scale down. It will scale down by the decrement value specified in the policy. For data drop, this is calculated as the number of data items dropped divided by the total number of data items, dropped+processed. For CPU, Memory, etc., it is calculated as a percentage of allocated resource when you defined the worker. <code>Scale up above percentage</code> When the aggregated value for all workers rises above this value, the stage will scale up. <code>Rolling count</code> This value helps to keep jitter out of the autoscaling process. Instead of scaling immediately the first time values fall outside of the scale-down and scale-up percentage thresholds you define, Mantis will wait until the thresholds are exceeded a certain number of times within a certain window. For example, a rolling count of \u201c6 of 10\u201d means that only if in ten consecutive observations six or more of the observations fall below the scale-down threshold will the stage be scaled down. <p>It is possible to employ multiple rule based strategies for a stage. In this case, as soon as 1 strategy triggers a scaling action, the cooldown will prevent subsequent strategies from scaling for that duration.</p> <p>Note</p> <p>Ideally, there should be zero data drop, so there isn\u2019t an elegant way to express \u201cscale down below percentage\u201d for data drop. Specifying \u201c0%\u201d as the \u201cscale down below percentage\u201d effectively means the data drop percentage never trigger a scale down. For this reason, it is best to use the data drop strategy in conjunction with another strategy that provides the scale-down trigger.</p>"},{"location":"operate/autoscalingstrategies/#pid-control-based-strategy","title":"PID Control Based Strategy","text":"<p>PID control system uses a continuous feedback loop to maintain a signal at a target level (set point). Mantis offers variations of this strategy that operates on different signals. Additionally, they try to learn the appropriate target over time without the need for user input. The PID controller computes the magnitude of scale up/down based on the drift between the observed signal and the target. Thus, this strategy can react quicker to big changes compared to rule based strategies, since rule based strategies use fixed step size. Cooldown still applies between scaling activities.</p>"},{"location":"operate/autoscalingstrategies/#clutch","title":"Clutch","text":"<p>The strategy operates on CPU, Memory, Network and UserDefined. Every 24 hours, it will pick 1 dominant resource and use the P99 value as the target set point. For the next 24 hours, it will monitor that resource metric and scale the stage to keep the metric close to the target set point. In the initial 24 hours after the job is first launched, this strategy will scale the stage to max in order to learn the first dominant resource and set point. This also happens if the job is restarted.</p>"},{"location":"operate/autoscalingstrategies/#clutch-with-user-defined-configs","title":"Clutch with User Defined Configs","text":"<p>With this strategy, the user defines the target for each resource without relying on the system to learn it. There is no need for an initial 24 hour pin high period, the PID controller can start working right away. You can supply the configuration as JSON in the job parameter <code>mantis.jobmaster.clutch.config</code>. Example:</p> <pre><code>{\n\"minSize\": 3,\n\"maxSize\": 25,\n\"cooldownSeconds\": 300,\n\"rps\": 8000,\n\"cpu\": {\n\"setPoint\": 60.0,\n\"rope\": [25.0, 0.0],\n\"kp\": 0.01,\n\"kd\": 0.01\n},\n\"memory\": {\n\"setPoint\": 100.0,\n\"rope\": [0.0, 0.0],\n\"kp\": 0.01,\n\"kd\": 0.01\n},\n\"network\": {\n\"setPoint\": 60.0,\n\"rope\": [25.0, 0.0],\n\"kp\": 0.01,\n\"kd\": 0.01\n}\n}\n</code></pre> Field Description <code>minSize</code> Minimum number of workers in the stage. It will not scale down below this number. <code>maxSize</code> Maximum number of workers in the stage. It will not scale up above this number. <code>cooldownSeconds</code> This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. <code>maxAdjustment</code> Optional. The maximum number of workers to scale up/down in a single operation. <code>rps</code> Expected RPS per worker. Must be &gt; 0. <code>cpu</code>, <code>memory</code>, <code>network</code> Configure PID controller for each resource. <code>setPoint</code> Target set point for the resource. This is expressed as a percentage of allocated resource to the worker. For example, <code>60.0</code> on <code>network</code> means network bytes should be 60% of the network limit on machine definition. <code>rope</code> Lower and upper buffer around the set point. Metric values within this buffer are assumed to be at set point, and thus contributes an error of 0 to the PID controller. <code>kp</code> Multiplier for the proportional term of the PID controller. This will affect the size of scaling actions. <code>kd</code> Multiplier for the derivative term of the PID controller. This will affect the size of scaling actions."},{"location":"operate/autoscalingstrategies/#clutch-rps","title":"Clutch RPS","text":"<p>This strategy scales the stage base on number of events processed. The target set point is a percentile of RPS. The signal is the sum of RPS, inbound drops, Kafka lag, and outbound drops from source jobs. Therefore, it effectively tries to keep drops and lag at 0. It takes the first 10 minutes after job launch to learn the first RPS set point. This also applies if the job is restarted, the set point does not carry over. Afterwards, it may adjust the set point once every hour. Set point should become stable the longer a job runs, since it simply takes a percentile of historical RPS metric.</p> <p>The source job drop metric is not enabled by default. It is only applicable if your job connects to an upstream job as input. You can enable this metric by setting the job parameter <code>mantis.jobmaster.autoscale.sourcejob.metric.enabled</code> to true. Further, you need to specify the source job targets in the job parameter <code>mantis.jobmaster.autoscale.sourcejob.target</code>. You can omit this if your job already has a <code>target</code> parameter for connecting to source jobs, the auto scaler will pick that up automatically. Example:</p> <pre><code>{\n\"targets\": [\n{\n\"sourceJobName\": \"ConsolidatedLoggingEventKafkaSource\"\n}\n]\n}\n</code></pre> <p>Optionally, it is possible to further customize the behavior of the PID controller. You can supply the configuration as JSON in the job parameter <code>mantis.jobmaster.clutch.config</code>. Example:</p> <pre><code>{\n\"rpsConfig\": {\n\"setPointPercentile\": 50.0,\n\"rope\": [30.0, 0.0],\n\"scaleDownBelowPct\": 40.0,\n\"scaleUpAbovePct\": 10.0,\n\"scaleDownMultiplier\": 0.5,\n\"scaleDownMultiplier\": 3.0\n}\n}\n</code></pre> Field Description <code>setPointPercentile</code> Percentile of historical RPS metric to use as the set point. Valid input is between <code>[1.0, 100.0]</code> Default is <code>75.0</code>. <code>rope</code> Lower and upper buffer around the set point. The value is interpreted as percentage of set point. For example, <code>[30.0, 30.0]</code> means values within 30% of set point is considered to have 0 error. Valid input is between <code>[0.0, 100.0]</code> Default is <code>[30.0, 0.0]</code>. <code>scaleDownBelowPct</code> Only scale down if the PID controller output is below this number. It can be used to delay a scaling action. Valid input is between <code>[0.0, 100.0]</code>. Default is <code>0.0</code>. <code>scaleUpAbovePct</code> Only scale up if the PID controller output is above this number. It can be used to delay a scaling action. Valid input is between <code>[0.0, 100.0]</code>. Default is <code>0.0</code>. <code>scaleDownMultiplier</code> Artificially increase/decrease the size of scale down by this factor. Default is <code>1.0</code>. <code>scaleUpMultiplier</code> Artificially increase/decrease the size of scale up by this factor. Default is <code>1.0</code>."},{"location":"operate/autoscalingstrategies/#clutch-experimental-developmental-use-only","title":"Clutch Experimental (Developmental Use Only)","text":"<p>This strategy is internally used for testing new Clutch implementations. It should not be used for production jobs.</p>"},{"location":"operate/profiling/","title":"Memory and CPU Profiling","text":"<p>Mantis allows you to directly connect profiling tools to workers in order to do performance or memory profiling. This can be helpful for debugging OOMs. Below are the steps to start profiling your running Mantis job:</p> <ol> <li>Download a profiling tool. We recommend Zulu Mission Control (ZMC).</li> <li>Untar ZMC     <code>tar -xzvf &lt;zulu_mission_control_package&gt;.tar.gz</code></li> <li> <p>Run ZMC.</p> <p>Note</p> <p>On Mac the <code>Zulu Mission Control</code> app is actually a directory even though it has an icon. The <code>zmc</code> executable is inside and needs to be called directly.</p> <p><code>&lt;your dir&gt;/&lt;zulu_mission_control_package&gt;/Zulu\\ Mission\\ Control.app/Contents/MacOS/zmc</code>  1. Find a worker you want to debug in your job and click on it to get its ip address and debug port In this example these are <code>100.82.159.148</code> and <code>7156</code> respectively.</p> <p></p> </li> <li> <p>Setup an SSH tunnel with local portforwarding for your worker and port. The tunnel command for the example worker     in the previous step would be:     <code>ssh -L 7156:127.0.0.1:7156 100.82.159.148</code></p> <p>Note</p> <p>Tunneling is only required if you can't access the Mantis worker's ip directly.</p> </li> <li> <p>After setting up local port forwarding. Add the worker JVM to ZMC by clicking the plug and star icon in the top right corner.</p> <p></p> </li> <li> <p>Input your mantis worker information</p> <p></p> </li> <li> <p>Start the flight recorder</p> <p></p> </li> <li> <p>Wait for the Flight recording to complete</p> <p></p> </li> <li> <p>Look at the collected profiling info. Below is an example of the memory profiling results. You can see the      percentage of the heap occuppied by different objects, and some stack profiling information about where the     allocations happen.</p> <p></p> </li> </ol>"},{"location":"operate/tuning/","title":"Tuning","text":""},{"location":"operate/tuning/#background","title":"Background","text":"<p>Mantis Jobs consist of one or more independent stages. Since stages are independent of one another, they are also sized independently.</p> <p>Stages have one or more workers, which are the fundamental unit of parallelism for a stage. Workers execute individual instances of a stage and are each allocated CPU, memory, disk, and network resources. Workers are also isolated from one another, which means they process events independently of each other using their own dedicated resources.</p> <p>You can horizontally scale your Mantis Jobs by modifying the number of workers of your stages. You can also vertically scale your Mantis Jobs by tuning the number of resources for each worker.</p> <p>The following sections are guidelines for sizing different types of Mantis Jobs.</p> <p>Note</p> <p>You can modify worker resources any time, even after you have already launched the Mantis Job. This is useful in cases where you might want to adjust your Mantis Job to new traffic patterns. You can do this by marking your stage as Stage is Scalable.</p> <p>You can also mark the stage as AutoScale this stage to have Mantis automatically scale the stage.</p> <p>You can horizontally scale your Mantis Jobs without restarting the entire job. However, vertically scaling your jobs requires a new job submission for changes to take effect.</p>"},{"location":"operate/tuning/#mantis-jobs","title":"Mantis Jobs","text":"<p>In addition to scaling the number of workers of a Mantis Job, you should consider tuning your workers, which have two tunable properties: processing resources and processing parallelism.</p> <p>For processing resources, you should generally determine if CPU, memory, or network resources will be a bottleneck in your processing. You should increase:</p> <ul> <li>the number of CPUs if your job has CPU-intensive transformations such as serialization and   deserialization</li> <li>memory if your job plans to hold many objects or large objects in memory</li> <li>network resources if your job needs high throughput for network I/O such as external API calls</li> </ul> <p>For processing parallelism, you must choose between serial or concurrent input processing for each stage.</p> <p>Serial input processing means your Processing Stage will receive and process events within a single thread. With serial input, you lose parallelism but your processing logic becomes straightforward without race conditions.</p> <pre><code>// Specifying serial input for a stage.\npublic class SerialStage implements ScalarComputation&lt;T1, T2&gt; {\nstatic public ScalarToScalar.Config&lt;T1, T2&gt; config() {\nreturn new ScalarToScalar.Config&lt;T1, T2&gt;().serialInput();\n}\n}\n</code></pre> <p>Concurrent input means your Processing Stage will have multiple threads which each receive and process events. With concurrent input enabled, race conditions must be considered because the stage\u2019s threads operate independently from one another and may process events at different speeds.</p> <pre><code>// Specifying concurrent input for a stage.\npublic class SerialStage implements ScalarComputation&lt;T1, T2&gt; {\nstatic public ScalarToScalar.Config&lt;T1, T2&gt; config() {\nreturn new ScalarToScalar.Config&lt;T1, T2&gt;().concurrentInput();\n}\n}\n</code></pre> <p>Note</p> <p>Workers use serial input by default.</p>"},{"location":"operate/tuning/#kafka-source-jobs","title":"Kafka Source Jobs","text":"<p>Kafka Source Jobs are Mantis Jobs that share the same properties described above, except Kafka Source Jobs use concurrent input processing by default. There are additional job parameters to consider when tuning Kafka Source Jobs: <code>numConsumerInstances</code> and <code>stageConcurrency</code>.</p> <p>The <code>numConsumerInstances</code> property  determines how many Kafka consumers will be created for each worker. For example, if you have <code>numConsumerInstances</code> set to <code>2</code> and have 5 workers, then you will have 10 Kafka consumers in total for your Mantis Job consuming from a Kafka topic.</p> <p>The <code>stageConcurrency</code> property determines a pool of threads which receive events by the Kafka consumers. You can control your processing parallelism with this property. For example, if you have <code>numConsumerInstances</code> set to <code>2</code> and <code>stageConcurrency</code> set to 5 on a worker, then two Kafka consumers will read events from a Kafka topic and asynchronously send them to a pool of 5 processing threads.</p> <p>There are three considerations for using <code>numConsumerInstances</code> and <code>stageConcurrency</code> to tune your Kafka Source Job.</p> <p>First, you can pin <code>numConsumerInstances</code> to <code>1</code> and add more workers to load balance Kafka consumers across worker instances.</p> <p>If you find that your workers are under-utilized, you can increase <code>numConsumerInstances</code> for each worker. You can also give each worker more CPU, memory, and network resources and increase <code>numConsumerInstances</code> further so that you have fewer workers doing more work.</p> <p>Lastly, if you find that your Kafka topic\u2019s lag is increasing, then processing might be a bottleneck. In this case, you can increase <code>stageConcurrency</code> to increase processing throughput.</p> <p>Note</p> <p>Adding more workers to scale your Kafka Source Jobs to increase throughput or address lag has a hard upper limit. Ensure that you do not have more Kafka consumers than the number of partitions of the Kafka topic that your job is reading from. Specifically, ensure that:</p> <p>(numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions</p> <p>This is because the number of partitions is a Kafka topic\u2019s upper bound for parallelism. If you exceed this number, then you will have idle consumers wasting resources which means adding more workers to your Kafka Source Job will not have any positive effect.</p>"},{"location":"reference/api/","title":"API","text":"<p>In addition to the Mantis UI, there is a REST API with which you can maintain Mantis Jobs and Job Clusters. This page describes the open source version of the Mantis REST API.</p> <p>You can use the Mantis REST API to submit Jobs based on existing Job Cluster or to connect to the output of running Jobs. It is easier to set up or update new Job Clusters by using the Mantis UI, but you can also do this with the Mantis REST API.</p> <p>Response Content Type</p> <p>Mantis API endpoints always return JSON, and do not respect content-type headers in API requests.</p>"},{"location":"reference/api/#summary-of-rest-api","title":"Summary of REST API","text":""},{"location":"reference/api/#cluster-apis","title":"Cluster APIs","text":"endpoint verb purpose <code>/api/v1/jobClusters</code> <code>GET</code> Return a list of Mantis clusters. <code>/api/v1/jobClusters</code> <code>POST</code> Create a new cluster. <code>/api/v1/jobClusters/clusterName</code> <code>GET</code> Return information about a single cluster by name. <code>/api/v1/jobClusters/clusterName</code> <code>PUT</code> Update the information about a particular cluster. <code>/api/v1/jobClusters/clusterName</code> <code>DELETE</code> Permanently delete a paticular cluster. <code>/api/v1/jobClusters/clusterName/actions/updateArtifact</code> <code>POST</code> Update the job cluster artifact and optionally resubmit the job. <code>/api/v1/jobClusters/clusterName/actions/updateSla</code> <code>POST</code> Update cluster SLA information. <code>/api/v1/jobClusters/clusterName/actions/updateMigrationStrategy</code> <code>POST</code> Update the cluster migration strategy. <code>/api/v1/jobClusters/clusterName/actions/updateLabel</code> <code>POST</code> Update cluster labels. <code>/api/v1/jobClusters/clusterName/actions/enableCluster</code> <code>POST</code> Enable a disabled cluster. <code>/api/v1/jobClusters/clusterName/actions/disableCluster</code> <code>POST</code> Disable a cluster. <code>/api/v1/mantis/publish/streamJobClusterMap</code> <code>GET</code> Return a mapping of Mantis Publish push-based streams to clusters."},{"location":"reference/api/#job-apis","title":"Job APIs","text":"endpoint verb purpose <code>/api/v1/jobs</code> <code>GET</code> Return a list of jobs. <code>/api/v1/jobClusters/clusterName/jobs</code> <code>GET</code> Return a list of jobs for a particular cluster. <code>/api/v1/jobs/jobID</code> <code>GET</code> Return information about a particular job. <code>/api/v1/jobClusters/clusterName/jobs/jobID</code> <code>GET</code> Return information about a particular job. <code>/api/v1/jobs/jobID</code> <code>DELETE</code> Permanently kill a particular job. <code>/api/v1/jobClusters/clusterName/jobs</code> <code>POST</code> Submit a new job. <code>/api/v1/jobs/actions/quickSubmit</code> <code>POST</code> Update a job cluster and submit a new job at the same time. <code>/api/v1/jobs/jobID/actions/postJobStatus</code> <code>POST</code> Post job heartbeat status. <code>/api/v1/jobs/jobID/actions/scaleStage</code> <code>POST</code> Horizontally scale a stage. <code>/api/v1/jobs/jobID/actions/resubmitWorker</code> <code>POST</code> Resubmit a worker."},{"location":"reference/api/#admin-apis","title":"Admin APIs","text":"endpoint verb purpose <code>/api/v1/masterInfo</code> <code>GET</code> Return Job Master information. <code>/api/v1/masterConfigs</code> <code>GET</code> Return Job Master configs. <code>/api/v1/agentClusters/</code> <code>GET</code> Get information about active agent clusters. <code>/api/v1/agentClusters/</code> <code>POST</code> Activate or deactivate an agent cluster. <code>/api/v1/agentClusters/jobs</code> <code>GET</code> Get jobs and host information for an agent cluster. <code>/api/v1/agentClusters/autoScalePolicy</code> <code>GET</code> Retrieve the Agent Cluster Scaling Policy."},{"location":"reference/api/#streaming-websocketsse-apis","title":"Streaming WebSocket/SSE APIs","text":"endpoint verb purpose <code>ws://masterHost:7101/api/v1/jobStatusStream/jobID</code> n/a Stream Job Status Changes. <code>/api/v1/jobDiscoveryStream/jobID</code> (SSE) <code>GET</code> Return streaming (SSE) scheduling information for a particular job. <code>/api/v1/jobs/schedulingInfo/jobID</code> (SSE) <code>GET</code> Return streaming (SSE) scheduling information for a particular job. <code>/api/v1/jobClusters/discoveryInfoStream/clusterName</code> (SSE) <code>GET</code> Return streaming (SSE) discovery info for the given job cluster. <code>/api/v1/lastSubmittedJobIdStream/clusterName</code> (SSE) <code>GET</code> Return streaming (SSE) job information for a particular cluster. <code>/api/v1/jobConnectbyid/jobID</code> (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. <code>/api/v1/jobConnectbyname/jobName</code> (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. <code>/api/v1/jobsubmitandconnect</code> (SSE) <code>POST</code> Submit a job, collect messages from the job sinks, and merge them into a single websocket or SSE stream."},{"location":"reference/api/#cluster-tasks","title":"Cluster Tasks","text":"<p>TBD</p>"},{"location":"reference/api/#get-a-list-of-clusters","title":"Get a List of Clusters","text":"<ul> <li><code>/api/v1/jobClusters</code> (<code>GET</code>)</li> </ul> <p>To retrieve a list of JSON objects that include details about the available Job Clusters, issue a <code>GET</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/</code>.</p> Query Parameters Query Parameter Purpose <code>ascending</code> (optional) You can use this to indicate whether or not to sort the records in ascending order (<code>true</code> <code>fields</code> (optional) By default this endpoint will return all of the fields in the payload. You can set <code>fields</code> to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. For example <code>?fields=name</code>. <code>offset</code> (optional) The record number to begin with in the set of records to return in this request (use this with <code>pageSize</code> to get records by the page). See Pagination for more details. <code>pageSize</code> (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. <code>sortBy</code> (optional) You can set this to the name of any payload field whose values are <code>Comparable</code> and this endpoint will return its results sorted by that field. <code>matching</code> (optional) You can set this to a regular expression, and Mantis will filter the list of Job Clusters on the server side, and will return only those that match this expression. Example Response Format <p><code>GET /api/v1/jobClusters?pageSize=5&amp;fields=name&amp;sortBy=name&amp;ascending=false</code></p> <pre><code>{\n  \"list\":\n    [{\"name\":\"jschmoeSLATest\"},\n     {\"name\":\"sinefn\"},\n     {\"name\":\"Validation_ZV93BAWR2\"},\n     {\"name\":\"Validation_Z8NB06L1A\"},\n     {\"name\":\"Validation_Y828QAI01\"}],\n  \"prev\":null,\n  \"next\":\"/api/v1/jobClusters?pageSize=5&amp;fields=name&amp;sortBy=name&amp;ascending=false&amp;offset=5\"\n}\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#create-a-new-cluster","title":"Create a New Cluster","text":"<ul> <li><code>/api/v1/jobClusters</code> (<code>POST</code>)</li> </ul> <p>Before you submit a Mantis Job, you must first have set up a Job Cluster. A Job Cluster contains a unique name for the Job, a URL of the Job\u2019s <code>.jar</code> or <code>.zip</code> artifact file, resource requirements to run your Job, and other optional information such as SLA values for minimum and maximum Jobs to keep active for this Cluster, or a cron-based schedule to launch a Job for this Cluster. Each new Job can be considered as an instance of the Job Cluster, and is given a unique ID by appending a number suffix to the Cluster name. The Job inherits the resource requirements from the Cluster unless whoever submits the Job overrides these at submit time.</p> <p>A Job Cluster name must match this regular expression: <code>^[A-Za-z]+[A-Za-z0-9+-_=:;]*</code></p> <p>To create a new Job Cluster, issue a POST command to the Mantis REST API endpoint <code>/api/v1/jobClusters</code> with a request body that matches format of the following example:</p> Example Request Body <pre><code>{\n  \"jobDefinition\": {\n    \"name\": \"jschmoe_validation\",\n    \"user\": \"jschmoe\",\n    \"jobJarFileLocation\": \"https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\",\n    \"version\": \"0.2.9 2019-03-19 17:01:36\",\n    \"schedulingInfo\": {\n      \"stages\": {\n        \"1\": {\n          \"numberOfInstances\": \"1\",\n          \"machineDefinition\": {\n            \"cpuCores\": \"1\",\n            \"memoryMB\": \"1024\",\n            \"diskMB\": \"1024\",\n            \"networkMbps\": \"128\",\n            \"numPorts\": \"1\"\n          },\n          \"scalable\": false,\n          \"softConstraints\": [],\n          \"hardConstraints\": []\n        }\n      }\n    },\n    \"parameters\": [],\n    \"labels\": [\n      {\n        \"name\": \"_mantis.user\",\n        \"value\": \"jschmoe\"\n      },\n      {\n        \"name\": \"_mantis.ownerEmail\",\n        \"value\": \"jschmoe@netflix.com\"\n      },\n      {\n        \"name\": \"_mantis.artifact\",\n        \"value\": \"mantis-examples-sine-function\"\n      },\n      {\n        \"name\": \"_mantis.artifact.version\",\n        \"value\": \"0.2.9\"\n      }\n    ],\n    \"migrationConfig\": {\n      \"strategy\": \"PERCENTAGE\",\n      \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\"\n    },\n    \"slaMin\": \"0\",\n    \"slaMax\": \"0\",\n    \"cronSpec\": null,\n    \"cronPolicy\": \"KEEP_EXISTING\"\n  },\n  \"owner\": {\n    \"contactEmail\": \"jschmoe@netflix.com\",\n    \"description\": \"\",\n    \"name\": \"Joe Schmoe\",\n    \"repo\": \"\",\n    \"teamName\": \"\"\n  }\n}\n</code></pre> Example Response Format <pre><code>{\n  \"name\": \"jschmoe_validation1\",\n  \"jars\": [\n    {\n      \"url\": \"https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\",\n      \"uploadedAt\": 1553040262171,\n      \"version\": \"0.2.9 2019-03-19 17:01:36\",\n      \"schedulingInfo\": {\n        \"stages\": {\n          \"1\": {\n            \"numberOfInstances\": 1,\n            \"machineDefinition\": {\n              \"cpuCores\": 1,\n              \"memoryMB\": 1024,\n              \"networkMbps\": 128,\n              \"diskMB\": 1024,\n              \"numPorts\": 1\n            },\n            \"hardConstraints\": [],\n            \"softConstraints\": [],\n            \"scalingPolicy\": null,\n            \"scalable\": false\n          }\n        }\n      }\n    }\n  ],\n  \"sla\": {\n    \"min\": 0,\n    \"max\": 0,\n    \"cronSpec\": null,\n    \"cronPolicy\": null\n  },\n  \"parameters\": [],\n  \"owner\": {\n    \"name\": \"Joe Schmoe\",\n    \"teamName\": \"\",\n    \"description\": \"\",\n    \"contactEmail\": \"jschmoe@netflix.com\",\n    \"repo\": \"\"\n  },\n  \"lastJobCount\": 0,\n  \"disabled\": false,\n  \"isReadyForJobMaster\": false,\n  \"migrationConfig\": {\n    \"strategy\": \"PERCENTAGE\",\n    \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\"\n  },\n  \"labels\": [\n    {\n      \"name\": \"_mantis.user\",\n      \"value\": \"jschmoe\"\n    },\n    {\n      \"name\": \"_mantis.ownerEmail\",\n      \"value\": \"jschmoe@netflix.com\"\n    },\n    {\n      \"name\": \"_mantis.artifact\",\n      \"value\": \"mantis-examples-sine-function\"\n    },\n    {\n      \"name\": \"_mantis.artifact.version\",\n      \"value\": \"0.2.9\"\n    }\n  ],\n  \"cronActive\": false,\n  \"latestVersion\": \"0.2.9 2019-03-19 17:01:36\"\n}\n</code></pre> Possible Response Codes Response Code Reason <code>201</code> normal response <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>409</code> cluster name already exists <code>500</code> unknown server error"},{"location":"reference/api/#setting-jobs-to-launch-at-particular-times","title":"Setting Jobs to Launch at Particular Times","text":"<p>You can use the <code>cronSpec</code> field in the body of this request to specify when to launch the Jobs in the Cluster. By default this is blank (<code>\"\"</code>).</p> <p>If you set <code>cronSpec</code> to a non-blank value, this also sets the <code>min</code> and <code>max</code> values for the Job Cluster to 0 and 1 respectively. That is to say, you can have no more than one Job running at any one time for that Cluster.</p> <p>Optionally, you can provide a policy (<code>cronpolicy</code>) to use when a cron trigger fires while a previosuly submitted Job for the Job Cluster is still running. The possible policy values are <code>KEEP_EXISTING</code> (do not replace the current Job) and <code>KEEP_NEW</code> (replace the current Job with a new one). The default policy is <code>KEEP_EXISTING</code>.</p> <p>Note</p> <p>If the Mantis Master is down during a time window when cron would normally have fired, that cron trigger time window is lost. Mantis does not check for this upon restart. The next cron trigger will resume normally.</p>"},{"location":"reference/api/#get-information-about-a-cluster","title":"Get Information about a Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName</code> (<code>GET</code>)</li> </ul> <p>To retrieve a JSON object that includes details about a Job Cluster, issue a <code>GET</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName</code>.</p> Query Parameters Query Parameter Purpose <code>fields</code> (optional) By default this endpoint will return all of the fields in the payload. You can set <code>fields</code> to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. Example Response Format <pre><code>{\n  \"name\": \"jschmoe_validation1\",\n  \"jars\": [\n    {\n      \"url\": \"https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\",\n      \"uploadedAt\": 1553040262171,\n      \"version\": \"0.2.9 2019-03-19 17:01:36\",\n      \"schedulingInfo\": {\n        \"stages\": {\n          \"1\": {\n            \"numberOfInstances\": 1,\n            \"machineDefinition\": {\n              \"cpuCores\": 1,\n              \"memoryMB\": 1024,\n              \"networkMbps\": 128,\n              \"diskMB\": 1024,\n              \"numPorts\": 1\n            },\n            \"hardConstraints\": [],\n            \"softConstraints\": [],\n            \"scalingPolicy\": null,\n            \"scalable\": false\n          }\n        }\n      }\n    }\n  ],\n  \"sla\": {\n    \"min\": 0,\n    \"max\": 0,\n    \"cronSpec\": null,\n    \"cronPolicy\": null\n  },\n  \"parameters\": [],\n  \"owner\": {\n    \"name\": \"Joe Schmoe\",\n    \"teamName\": \"\",\n    \"description\": \"\",\n    \"contactEmail\": \"jschmoe@netflix.com\",\n    \"repo\": \"\"\n  },\n  \"lastJobCount\": 0,\n  \"disabled\": false,\n  \"isReadyForJobMaster\": false,\n  \"migrationConfig\": {\n    \"strategy\": \"PERCENTAGE\",\n    \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\"\n  },\n  \"labels\": [\n    {\n      \"name\": \"_mantis.user\",\n      \"value\": \"jschmoe\"\n    },\n    {\n      \"name\": \"_mantis.ownerEmail\",\n      \"value\": \"jschmoe@netflix.com\"\n    },\n    {\n      \"name\": \"_mantis.artifact\",\n      \"value\": \"mantis-examples-sine-function\"\n    },\n    {\n      \"name\": \"_mantis.artifact.version\",\n      \"value\": \"0.2.9\"\n    }\n  ],\n  \"cronActive\": false,\n  \"latestVersion\": \"0.2.9 2019-03-19 17:01:36\"\n}\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no cluster with that cluster name was found <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#change-information-about-a-cluster","title":"Change Information about a Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName</code> (<code>PUT</code>)</li> </ul> <p>To update an existing Job Cluster, send a <code>PUT</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName</code> with the same sort of body described above in the case of a Job Cluster create operation. Increment the version number so as to differentiate your new Cluster from the previous Job artifacts. If you try to update an existing Job Cluster by reusing the version number of an existing one, the operation will fail.</p> Example Request Body <pre><code>{\n  \"jobDefinition\": {\n    \"name\": \"Validation_jschmoe\",\n    \"user\": \"validator\",\n    \"jobJarFileLocation\": \"https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\",\n    \"parameters\": [\n      {\n        \"name\": \"useRandom\",\n        \"value\": false\n      }\n    ],\n    \"schedulingInfo\": {\n      \"stages\": {\n        \"0\": {\n          \"numberOfInstances\": 1,\n          \"machineDefinition\": {\n            \"cpuCores\": 2,\n            \"memoryMB\": 4096,\n            \"diskMB\": 10,\n            \"numPorts\": 1\n          },\n          \"hardConstraints\": null,\n          \"softConstraints\": null,\n          \"scalable\": false\n        },\n        \"1\": {\n          \"numberOfInstances\": 1,\n          \"machineDefinition\": {\n            \"cpuCores\": 2,\n            \"memoryMB\": 4096,\n            \"diskMB\": 10,\n            \"numPorts\": 1\n          },\n          \"hardConstraints\": null,\n          \"softConstraints\": null,\n          \"scalable\": false\n        }\n      }\n    },\n    \"slaMin\": 0,\n    \"slaMax\": 0,\n    \"cronSpec\": null,\n    \"cronPolicy\": \"KEEP_EXISTING\",\n    \"migrationConfig\": {\n      \"configString\": \"{\\\"percentToMove\\\":60, \\\"intervalMs\\\":30000}\",\n      \"strategy\": \"PERCENTAGE\"\n    }\n  },\n  \"owner\": {\n    \"name\": \"validator\",\n    \"teamName\": \"Mantis\",\n    \"description\": \"integration validator\",\n    \"contactEmail\": \"mantisteam@netflix.com\"\n  }\n}\n</code></pre> Example Response Format <p>sine-function Job cluster updated</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>400</code> client failure <code>404</code> no existing cluster with that cluster name was found <code>405</code> incorrect HTTP verb (use <code>PUT</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#delete-a-cluster","title":"Delete a Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName</code> (<code>DELETE</code>)</li> </ul> <p>To permanently delete an existing Job Cluster, send a <code>DELETE</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName</code>.</p> Query Parameters Query Parameter Purpose <code>user</code> (required) Must match the original user in the cluster payload. Example Response Format <p><p>TBD</p></p> Possible Response Codes Response Code Reason <code>202</code> normal response: asynchronous delete has been scheduled <code>405</code> incorrect HTTP verb (use <code>DELETE</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#update-a-clusters-artifacts","title":"Update a Cluster\u2019s Artifacts","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/actions/updateArtifact</code> (<code>POST</code>)</li> </ul> <p>You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts. This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. Mantis copies the scheduling information, Job parameters, and so forth, from the last Job submitted.</p> <p>To do this, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/actions/updateArtifact</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{\n  \"name\": \"ValidatorDemo\",\n  \"version\": \"0.0.1 2019-02-06 11:30:49\",\n  \"url\": \"mantis-artifacts/demo-0.0.1-dev201901231434.zip\",\n  \"skipsubmit\": false,\n  \"user\": \"jschmoe\"\n}\n</code></pre> Example Response Format <p>sine-function Job cluster updated</p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error <p>You will receive in the response the Job ID of the newly submitted Job (unless you set <code>skipsubmit</code> to <code>true</code> in the request body, in which case no such Job will be created).</p>"},{"location":"reference/api/#update-a-clusters-sla","title":"Update a Cluster\u2019s SLA","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/actions/updateSla</code> (<code>POST</code>)</li> </ul> <p>To update the SLA of a Job Cluster without having to submit a new version, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/actions/updateSla</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{ \n  \"user\": \"YourUserName\",\n  \"name\": \"Foo\",\n  \"min\": 0,\n  \"max\": 2,\n  \"cronspec\": \"5 * * * * ?\",\n  \"cronpolicy\": \"KEEP_EXISTING\",\n  \"forceenable\": true\n}\n</code></pre> <p>The fields of this body are as follows:</p> SLA Field Purpose <code>user</code> (required) name of the user calling this endpoint <code>name</code> (required) name of the Job Cluster <code>min</code> minimum number of Jobs to keep running <code>max</code> maximum number of Jobs to allow running simultaneously <code>cronspec</code> cron specification, see below for format and examples <code>cronpolicy</code> either <code>KEEP_EXISTING</code> or <code>KEEP_NEW</code> (see above for details) <code>forceenable</code> either <code>true</code> or <code>false</code>; reenable the Job Cluster if it is in the disabled state <p>Note</p> <p>While <code>min</code>, <code>max</code>, <code>cronspec</code>, <code>cronpolicy</code>, and <code>forceenable</code> are all optional fields, you should provide at minimum either <code>cronspec</code> or the combination of <code>min</code> &amp; <code>max</code>.</p> <p>The cron specification string is defined by Quartz CronTrigger. Here are some examples:</p> Examples of <code>cronspec</code> Values example <code>cronspec</code> value resulting job trigger time <code>\"0 0 12 * * ?\"</code> Fire at 12\u00a0p.m. (noon) every day. <code>\"0 15 10 ? * *\"</code> Fire at 10:15\u00a0a.m. every day. <code>\"0 15 10 * * ?\"</code> Fire at 10:15\u00a0a.m. every day. <code>\"0 0-5 14 * * ?\"</code> Fire every minute starting at 2\u00a0p.m. and ending at 2:05\u00a0p.m., every day. <p>Scheduling information for Jobs launched by means of cron triggers is inherited from the scheduling information for the Job Cluster.</p> <p>Warning</p> <p>If a Job takes required parameters, the Job will not launch successfully if the Job Cluster does not establish defaults for those parameters. A Job launched by means of a cron trigger always uses these default parameters to launch the Job.</p> <p>If you provide an invalid cron specification, this will disable the Job Cluster. To fix this, when you reformulate your cron specification, also set <code>forceenable</code> to <code>\"true\"</code> in the body that you send via <code>POST</code> to <code>/api/v2/jobClusters/clusterName/actions/updateSla</code>.</p> Example Response Format <p>sine-function SLA updated</p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error Example: Creating a Job Cluster with a cron Specification <p>The following <code>POST</code> body to <code>/api/v2/jobClusters/clusterName/actions/updateSla</code> would specify Jobs that are launched based on a timed schedule:</p> <pre><code>{\n  \"jobDefinition\": {\n    \"name\": \"Foo\",\n    \"user\": \"YourUserName\",\n    \"version\": \"1.0\",\n    \"parameters\": [\n      {\n        \"name\": \"param1\",\n        \"value\": \"value1\"\n      },\n      {\n        \"name\": \"param2\",\n        \"value\": \"value2\"\n      }\n    ],\n    \"schedulingInfo\": {\n      \"stages\": {\n        \"1\": {\n          \"numberOfInstances\": 1,\n          \"machineDefinition\": {\n            \"cpuCores\": 2,\n            \"memoryMB\": 4096,\n            \"diskMB\": 10\n          },\n          \"hardConstraints\": null,\n          \"softConstraints\": null,\n          \"scalable\": false\n        }\n      }\n    },\n    \"slaMin\": 0,\n    \"slaMax\": 0,\n    \"cronSpec\": \"2 * * * * ?\",\n    \"cronPolicy\":\"KEEP_EXISTING\",\n    \"jobJarFileLocation\": \"http://www.jobjars.com/foo\"\n  },\n  \"owner\": {\n    \"name\": \"MyName\",\n    \"teamName\": \"myTeam\",\n    \"description\": \"description\",\n    \"contactEmail\": \"email@company.com\",\n    \"repo\": \"http://repos.com/myproject.git\"\n  }\n}\n</code></pre>"},{"location":"reference/api/#update-a-clusters-migration-strategy","title":"Update a Cluster\u2019s Migration Strategy","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/actions/updateMigrationStrategy</code> (<code>POST</code>)</li> </ul> <p>You can quickly update the migration strategy of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/actions/updateMigrationStrategy</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{\n  \"name\": \"NameOfJobCluster\",\n  \"migrationConfig\": {\n    \"strategy\": \"PERCENTAGE\",\n    \"configString\": \"{\\\"percentToMove\\\":10, \\\"intervalMs\\\":1000}\"\n  },\n  \"user\": \"YourUserName\"\n}\n</code></pre> <p>You will receive in the response the migration strategy config that you have updated the Job Cluster to.</p> Example Response Format <p>sine-function worker migration config updated</p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#update-a-clusters-labels","title":"Update a Cluster\u2019s Labels","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/actions/updateLabel</code> (<code>POST</code>)</li> </ul> <p>You can quickly update the labels of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/actions/updateLabel</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{\n  \"name\": \"SPaaSBackpressureDemp\",\n  \"labels\": [\n    {\n      \"name\": \"_mantis.user\",\n      \"value\": \"jschmoe\"\n    },\n    {\n      \"name\": \"_mantis.ownerEmail\",\n      \"value\": \"jschmoe@netflix.com\"\n    },\n    {\n      \"name\": \"_mantis.artifact\",\n      \"value\": \"backpressure-demo-aggregator-0.0.1\"\n    },\n    {\n      \"name\": \"_mantis.artifact.version\",\n      \"value\": \"dev201901231434\"\n    },\n    {\n      \"name\": \"_mantis.jobType\",\n      \"value\": \"aggregator\"\n    },\n    {\n      \"name\": \"_mantis.criticality\",\n      \"value\": \"medium\"\n    },\n    {\n      \"name\": \"myTestLabel\",\n      \"value\": \"bingo\"\n    }\n  ],\n  \"user\": \"jschmoe\"\n}\n</code></pre> Example Response Format <p>sine-function labels updated</p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#enable-a-cluster","title":"Enable a Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/actions/enableCluster</code> (<code>POST</code>)</li> </ul> <p>You can quickly change the state of an existing Job Cluster to <code>enabled=true</code> without having to update the entirety of the Cluster definition.</p> <p>To do this, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/actions/enableCluster</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{\n  \"name\": \"SPaaSBackpressureDemp\",\n  \"user\": \"jschmoe\"\n}\n</code></pre> Example Response Format <p>sine-function enabled</p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#disable-a-cluster","title":"Disable a Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/actions/disableCluster</code> (<code>POST</code>)</li> </ul> <p>You can quickly change the state of an existing Job Cluster to <code>enabled=false</code> without having to update the entirety of the Cluster definition.</p> <p>When you disable a Job Cluster Mantis will not allow new Job submissions under that Cluster and it will terminate any Jobs from that Cluster that are currently running. Mantis will also stop enforcing the SLA requirements for the Cluster, including any cron setup that would otherwise launch new Jobs.</p> <p>This is useful when a Job Cluster must be temporarily made inactive, for instance if you have determined that there is a problem with it.</p> <p>To do this, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/actions/disableCluster</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{\n  \"name\": \"SPaaSBackpressureDemp\",\n  \"user\": \"jschmoe\"\n}\n</code></pre> Example Response Format <p>sine-function disabled</p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#get-a-map-of-mantis-publish-push-based-streams-to-clusters","title":"Get a Map of Mantis Publish Push-Based Streams to Clusters","text":"<ul> <li><code>/api/v1/mantis/publish/streamJobClusterMap</code> (<code>GET</code>)</li> </ul> <p>describe</p> Example Response Format <pre><code>{\n  \"version\": \"1\",\n  \"timestamp\": 2,\n  \"mappings\": {\n    \"__default__\": {\n      \"requestEventStream\": \"SharedMantisPublishEventSource\",\n      \"__default__\": \"SharedMantisPublishEventSource\"\n    }\n  }\n}\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#job-tasks","title":"Job Tasks","text":"<p>TBD</p>"},{"location":"reference/api/#get-a-list-of-jobs","title":"Get a List of Jobs","text":"<ul> <li><code>/api/v1/jobs</code> (<code>GET</code>)</li> </ul> <p>To retrieve a JSON array of IDs of the active Jobs, issue a <code>GET</code> command to the Mantis REST API endpoint <code>/api/v1/jobs</code>.</p> Query Parameters Query Parameter Purpose <code>ascending</code> (optional) You can use this to indicate whether or not to sort the records in ascending order (<code>true</code> <code>compact</code> (optional) Ask the server to return compact responses (<code>true</code> <code>fields</code> (optional) By default this endpoint will return all of the fields in the payload. You can set <code>fields</code> to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. <code>limit</code> (optional) The maximum record size to return (default = no limit). <code>offset</code> (optional) The record number to begin with in the set of records to return in this request (use this with <code>pageSize</code> to get records by the page). See Pagination for more details. <code>pageSize</code> (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. <code>sortBy</code> (optional) You can set this to the name of any payload field whose values are <code>Comparable</code> and this endpoint will return its results sorted by that field. <p>There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters:</p> Query Parameter What It Filters <code>activeOnly</code> (optional) The <code>activeOnly</code> field (boolean). By default, this is <code>true</code>. <code>jobState</code> (optional) Job state, <code>Active</code> or <code>Terminal</code>. By default, this endpoint filters on <code>jobState=Active</code>. This query parameter has precedence over the <code>activeOnly</code> parameter. <code>labels</code> (optional) Labels in the <code>labels</code> array. You can express this by setting this parameter to a comma-delimited list of label strings. <code>labels.op</code> (optional) Use this parameter to tell the server whether to treat the list of <code>labels</code> you have provided as an <code>or</code> (default: a job that contains any of the labels will be returned in the list), or an <code>and</code> (only jobs that contain all of the labels will be returned). Set this to <code>or</code> or <code>and</code>. <code>matching</code> (optional) The cluster name. Set this to a regex string. You can also use the <code>/api/v1/jobClusters/clusterName/jobs</code> endpoint if you mean to match a specific cluster name and do not need the flexibility of a regex filter. <code>stageNumber</code> (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. <code>workerIndex</code> (optional) The <code>workerIndex</code> field (integer). <code>workerNumber</code> (optional) The <code>workerNumber</code> field (integer). <code>workerState</code> (optional) The <code>workerState</code> field (<code>Noop</code>, <code>Active</code> (default), or <code>Terminal</code>) <p>describe</p> Example Response Format <pre><code>{\n        \"list\": [\n            {\n                \"jobMetadata\": {\n                    \"jobId\": \"sine-test-4\",\n                    \"name\": \"sine-test\",\n                    \"user\": \"someuser\",\n                    \"submittedAt\": 1574188324276,\n                    \"startedAt\": 1574188354708,\n                    \"jarUrl\": \"http://mantis-examples-sine-function-0.2.9.zip\",\n                    \"numStages\": 1,\n                    \"sla\": {\n                        \"runtimeLimitSecs\": 0,\n                        \"minRuntimeSecs\": 0,\n                        \"slaType\": \"Lossy\",\n                        \"durationType\": \"Perpetual\",\n                        \"userProvidedType\": \"\"\n                    },\n                    \"state\": \"Launched\",\n                    \"subscriptionTimeoutSecs\": 0,\n                    \"parameters\": [\n                        {\n                            \"name\": \"useRandom\",\n                            \"value\": \"True\"\n                        }\n                    ],\n                    \"nextWorkerNumberToUse\": 40,\n                    \"migrationConfig\": {\n                        \"strategy\": \"PERCENTAGE\",\n                        \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\"\n                    },\n                    \"labels\": [\n                        {\n                            \"name\": \"_mantis.user\",\n                            \"value\": \"zxu\"\n                        },\n                        {\n                            \"name\": \"_mantis.ownerEmail\",\n                            \"value\": \"zxu@netflix.com\"\n                        },\n                        {\n                            \"name\": \"_mantis.artifact.version\",\n                            \"value\": \"0.2.9\"\n                        },\n                        {\n                            \"name\": \"_mantis.artifact\",\n                            \"value\": \"mantis-examples-sine-function-0.2.9.zip\"\n                        },\n                        {\n                            \"name\": \"_mantis.version\",\n                            \"value\": \"0.2.9 2019-03-19 17:01:36\"\n                        }\n                    ]\n                },\n                \"stageMetadataList\": [\n                    {\n                        \"jobId\": \"sine-test-4\",\n                        \"stageNum\": 1,\n                        \"numStages\": 1,\n                        \"machineDefinition\": {\n                            \"cpuCores\": 1.0,\n                            \"memoryMB\": 1024.0,\n                            \"networkMbps\": 128.0,\n                            \"diskMB\": 1024.0,\n                            \"numPorts\": 1\n                        },\n                        \"numWorkers\": 1,\n                        \"hardConstraints\": [],\n                        \"softConstraints\": [],\n                        \"scalingPolicy\": null,\n                        \"scalable\": false\n                    }\n                ],\n                \"workerMetadataList\": [\n                    {\n                        \"workerIndex\": 0,\n                        \"workerNumber\": 31,\n                        \"jobId\": \"sine-test-4\",\n                        \"stageNum\": 1,\n                        \"numberOfPorts\": 5,\n                        \"metricsPort\": 7150,\n                        \"consolePort\": 7152,\n                        \"debugPort\": 7151,\n                        \"customPort\": 7153,\n                        \"ports\": [\n                            7154\n                        ],\n                        \"state\": \"Started\",\n                        \"slave\": \"100.82.168.140\",\n                        \"slaveID\": \"f39108b0-da43-45df-8b12-c132d85de7c0-S1\",\n                        \"cluster\": \"mantisagent-staging-m5.2xlarge-1\",\n                        \"acceptedAt\": 1575675900626,\n                        \"launchedAt\": 1575675996506,\n                        \"startingAt\": 1575676025493,\n                        \"startedAt\": 1575676026661,\n                        \"completedAt\": -1,\n                        \"reason\": \"Normal\",\n                        \"resubmitOf\": 22,\n                        \"totalResubmitCount\": 4\n                    }\n                ],\n                \"version\": null\n            }\n        ],\n        \"prev\": null,\n        \"next\": null,\n        \"total\": 1\n    }\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#get-information-about-a-particular-job","title":"Get Information About a Particular Job","text":"<ul> <li><code>/api/v1/jobs/jobID</code> (<code>GET</code>)</li> <li><code>/api/v1/jobClusters/clusterName/jobs/jobID</code> (<code>GET</code>)</li> </ul> <p>To retrieve a JSON record of a particular Job, issue a <code>GET</code> command to the Mantis REST API endpoint <code>/api/v1/jobs/jobID</code> or <code>/api/v1/jobClusters/clusterName/jobs/jobID</code>.</p> Query Parameters Query Parameter Purpose <code>fields</code> (optional) By default this endpoint will return all of the fields in the payload. You can set <code>fields</code> to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. <code>archived</code> (optional) By default only information about an active job will be returned. Set this to <code>true</code> if you want information about the job returned even if it is an archived inactive job. <p>describe</p> Example Response Format <pre><code>{\n            \"jobMetadata\": {\n                \"jobId\": \"sine-function-7531\",\n                \"name\": \"sine-function\",\n                \"user\": \"someuser\",\n                \"submittedAt\": 1576002266997,\n                \"startedAt\": 0,\n                \"jarUrl\": \"http://mantis-examples-sine-function-0.2.9.zip\",\n                \"numStages\": 2,\n                \"sla\": {\n                    \"runtimeLimitSecs\": 0,\n                    \"minRuntimeSecs\": 0,\n                    \"slaType\": \"Lossy\",\n                    \"durationType\": \"Perpetual\",\n                    \"userProvidedType\": \"\"\n                },\n                \"state\": \"Accepted\",\n                \"subscriptionTimeoutSecs\": 0,\n                \"parameters\": [\n                    {\n                        \"name\": \"useRandom\",\n                        \"value\": \"False\"\n                    }\n                ],\n                \"nextWorkerNumberToUse\": 10,\n                \"migrationConfig\": {\n                    \"strategy\": \"PERCENTAGE\",\n                    \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\"\n                },\n                \"labels\": [\n                    {\n                        \"name\": \"_mantis.artifact\",\n                        \"value\": \"mantis-examples-sine-function-0.2.9.zip\"\n                    },\n                    {\n                        \"name\": \"_mantis.version\",\n                        \"value\": \"0.2.9 2018-04-23 13:22:02\"\n                    }\n                ]\n            },\n            \"stageMetadataList\": [\n                {\n                    \"jobId\": \"sine-function-7531\",\n                    \"stageNum\": 0,\n                    \"numStages\": 2,\n                    \"machineDefinition\": {\n                        \"cpuCores\": 2.0,\n                        \"memoryMB\": 4096.0,\n                        \"networkMbps\": 128.0,\n                        \"diskMB\": 1024.0,\n                        \"numPorts\": 1\n                    },\n                    \"numWorkers\": 1,\n                    \"hardConstraints\": [],\n                    \"softConstraints\": [],\n                    \"scalingPolicy\": null,\n                    \"scalable\": false\n                },\n                {\n                    \"jobId\": \"sine-function-7531\",\n                    \"stageNum\": 1,\n                    \"numStages\": 2,\n                    \"machineDefinition\": {\n                        \"cpuCores\": 1.0,\n                        \"memoryMB\": 1024.0,\n                        \"networkMbps\": 128.0,\n                        \"diskMB\": 1024.0,\n                        \"numPorts\": 1\n                    },\n                    \"numWorkers\": 1,\n                    \"hardConstraints\": [],\n                    \"softConstraints\": [],\n                    \"scalingPolicy\": null,\n                    \"scalable\": false\n                }\n            ],\n            \"workerMetadataList\": [\n                {\n                    \"workerIndex\": 0,\n                    \"workerNumber\": 1,\n                    \"jobId\": \"sine-function-7531\",\n                    \"stageNum\": 0,\n                    \"numberOfPorts\": 5,\n                    \"metricsPort\": -1,\n                    \"consolePort\": -1,\n                    \"debugPort\": -1,\n                    \"customPort\": -1,\n                    \"ports\": [],\n                    \"state\": \"Accepted\",\n                    \"slave\": null,\n                    \"slaveID\": null,\n                    \"cluster\": null,\n                    \"acceptedAt\": 1576002267005,\n                    \"launchedAt\": -1,\n                    \"startingAt\": -1,\n                    \"startedAt\": -1,\n                    \"completedAt\": -1,\n                    \"reason\": \"Normal\",\n                    \"resubmitOf\": 0,\n                    \"totalResubmitCount\": 0\n                },\n                {\n                    \"workerIndex\": 0,\n                    \"workerNumber\": 2,\n                    \"jobId\": \"sine-function-7531\",\n                    \"stageNum\": 1,\n                    \"numberOfPorts\": 5,\n                    \"metricsPort\": -1,\n                    \"consolePort\": -1,\n                    \"debugPort\": -1,\n                    \"customPort\": -1,\n                    \"ports\": [],\n                    \"state\": \"Accepted\",\n                    \"slave\": null,\n                    \"slaveID\": null,\n                    \"cluster\": null,\n                    \"acceptedAt\": 1576002267007,\n                    \"launchedAt\": -1,\n                    \"startingAt\": -1,\n                    \"startedAt\": -1,\n                    \"completedAt\": -1,\n                    \"reason\": \"Normal\",\n                    \"resubmitOf\": 0,\n                    \"totalResubmitCount\": 0\n                }\n            ],\n            \"version\": \"0.2.9 2018-04-23 13:22:02\"\n        }\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no job with that ID was found <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#kill-a-job","title":"Kill a Job","text":"<ul> <li><code>/api/v1/jobs/jobID</code> (<code>DELETE</code>)</li> </ul> <p>To permanently kill a particular Job, issue a <code>DELETE</code> command to the Mantis REST API endpoint endpoint <code>/api/v1/jobs/jobID</code>.</p> Query Parameters Query Parameter Purpose <code>reason</code> (required) Specify why you are killing this job. <code>user</code> (required) Specify which user is initiating this request. Example Response Format <p><p>empty</p> No response</p> Possible Response Codes Response Code Reason <code>202</code> the kill request has been accepted and is being processed asynchronously <code>404</code> no job with that ID was found <code>405</code> incorrect HTTP verb (use <code>DELETE</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#list-the-archived-workers-for-a-job","title":"List the Archived Workers for a Job","text":"<ul> <li><code>/api/v1/jobs/jobID/archivedWorkers</code> (<code>GET</code>)</li> </ul> <p>To list all of the archived workers for a particular Job, issue a <code>GET</code> command to the Mantis REST API endpoint endpoint <code>/api/v1/jobs/jobID/archivedWorkers</code>.</p> Query Parameters Query Parameter Purpose <code>ascending</code> (optional) You can use this to indicate whether or not to sort the records in ascending order (<code>true</code> <code>fields</code> (optional) By default this endpoint will return all of the fields in the payload. You can set <code>fields</code> to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. <code>limit</code> (optional) The maximum record size to return (default = no limit). <code>offset</code> (optional) The record number to begin with in the set of records to return in this request (use this with <code>pageSize</code> to get records by the page). See Pagination for more details. <code>pageSize</code> (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. <code>sortBy</code> (optional) You can set this to the name of any payload field whose values are <code>Comparable</code> and this endpoint will return its results sorted by that field. <p>describe</p> Example Response Format <pre><code>{\n        \"list\": [\n            {\n                \"workerIndex\": 0,\n                \"workerNumber\": 2,\n                \"jobId\": \"sine-function-7532\",\n                \"stageNum\": 1,\n                \"numberOfPorts\": 5,\n                \"metricsPort\": 7155,\n                \"consolePort\": 7157,\n                \"debugPort\": 7156,\n                \"customPort\": 7158,\n                \"ports\": [\n                    7159\n                ],\n                \"state\": \"Failed\",\n                \"slave\": \"100.85.130.224\",\n                \"slaveID\": \"079f4fa6-f910-4247-b5d0-f5574f36cace-S5274\",\n                \"cluster\": \"mantisagent-main-m5.2xlarge-1\",\n                \"acceptedAt\": 1576003739758,\n                \"launchedAt\": 1576003739861,\n                \"startingAt\": 1576003748544,\n                \"startedAt\": 1576003750069,\n                \"completedAt\": 1576003959366,\n                \"reason\": \"Relaunched\",\n                \"resubmitOf\": 0,\n                \"totalResubmitCount\": 0\n            }\n        ],\n        \"prev\": null,\n        \"next\": null,\n        \"total\": 1\n    }\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no job with that ID was found <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#get-a-list-of-jobs-for-a-particular-cluster","title":"Get a List of Jobs for a Particular Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/jobs</code> (<code>GET</code>)</li> </ul> <p>To retrieve a JSON array of IDs of the active Jobs in a particular cluster, issue a <code>GET</code> command to the Mantis REST API endpoint <code>/api/v1/jobClusters/clusterName/jobs</code>.</p> Query Parameters Query Parameter Purpose <code>ascending</code> (optional) You can use this to indicate whether or not to sort the records in ascending order (<code>true</code> <code>compact</code> (optional) Ask the server to return compact responses (<code>true</code> <code>fields</code> (optional) By default this endpoint will return all of the fields in the payload. You can set <code>fields</code> to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. <code>limit</code> (optional) The maximum record size to return (default = no limit). <code>offset</code> (optional) The record number to begin with in the set of records to return in this request (use this with <code>pageSize</code> to get records by the page). See Pagination for more details. <code>pageSize</code> (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. <code>sortBy</code> (optional) You can set this to the name of any payload field whose values are <code>Comparable</code> and this endpoint will return its results sorted by that field. <p>There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters:</p> Query Parameter What It Filters <code>activeOnly</code> (optional) The <code>activeOnly</code> field (boolean). By default, this is <code>true</code>. <code>jobState</code> (optional) Job state, <code>Active</code> or <code>Terminal</code>. By default, this endpoint filters on <code>jobState=Active</code>. This query parameter has precedence over the <code>activeOnly</code> parameter. <code>labels</code> (optional) Labels in the <code>labels</code> array. You can express this by setting this parameter to a comma-delimited list of label strings. <code>labels.op</code> (optional) Use this parameter to tell the server whether to treat the list of <code>labels</code> you have provided as an <code>or</code> (default: a job that contains any of the labels will be returned in the list), or an <code>and</code> (only jobs that contain all of the labels will be returned). Set this to <code>or</code> or <code>and</code>. <code>stageNumber</code> (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. <code>workerIndex</code> (optional) The <code>workerIndex</code> field (integer). <code>workerNumber</code> (optional) The <code>workerNumber</code> field (integer). <code>workerState</code> (optional) The <code>workerState</code> field (<code>Noop</code>, <code>Active</code> (default), or <code>Terminal</code>) <p>describe</p> Example Response Format <pre><code>{\n        \"list\": [\n            {\n                \"jobMetadata\": {\n                    \"jobId\": \"sine-function-7532\",\n                    \"name\": \"sine-function\",\n                    \"user\": \"someuser\",\n                    \"submittedAt\": 1576003739750,\n                    \"startedAt\": 1576003750076,\n                    \"jarUrl\": \"http://mantis-examples-sine-function-0.2.9.zip\",\n                    \"numStages\": 2,\n                    \"sla\": {\n                        \"runtimeLimitSecs\": 0,\n                        \"minRuntimeSecs\": 0,\n                        \"slaType\": \"Lossy\",\n                        \"durationType\": \"Perpetual\",\n                        \"userProvidedType\": \"\"\n                    },\n                    \"state\": \"Launched\",\n                    \"subscriptionTimeoutSecs\": 0,\n                    \"parameters\": [\n                        {\n                            \"name\": \"useRandom\",\n                            \"value\": \"False\"\n                        }\n                    ],\n                    \"nextWorkerNumberToUse\": 10,\n                    \"migrationConfig\": {\n                        \"strategy\": \"PERCENTAGE\",\n                        \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\"\n                    },\n                    \"labels\": [\n                        {\n                            \"name\": \"_mantis.isResubmit\",\n                            \"value\": \"true\"\n                        },\n                        {\n                            \"name\": \"_mantis.artifact\",\n                            \"value\": \"mantis-examples-sine-function-0.2.9.zip\"\n                        },\n                        {\n                            \"name\": \"_mantis.version\",\n                            \"value\": \"0.2.9 2018-04-23 13:22:02\"\n                        }\n                    ]\n                },\n                \"stageMetadataList\": [\n                    {\n                        \"jobId\": \"sine-function-7532\",\n                        \"stageNum\": 0,\n                        \"numStages\": 2,\n                        \"machineDefinition\": {\n                            \"cpuCores\": 2.0,\n                            \"memoryMB\": 4096.0,\n                            \"networkMbps\": 128.0,\n                            \"diskMB\": 1024.0,\n                            \"numPorts\": 1\n                        },\n                        \"numWorkers\": 1,\n                        \"hardConstraints\": [],\n                        \"softConstraints\": [],\n                        \"scalingPolicy\": null,\n                        \"scalable\": false\n                    },\n                    {\n                        \"jobId\": \"sine-function-7532\",\n                        \"stageNum\": 1,\n                        \"numStages\": 2,\n                        \"machineDefinition\": {\n                            \"cpuCores\": 1.0,\n                            \"memoryMB\": 1024.0,\n                            \"networkMbps\": 128.0,\n                            \"diskMB\": 1024.0,\n                            \"numPorts\": 1\n                        },\n                        \"numWorkers\": 1,\n                        \"hardConstraints\": [],\n                        \"softConstraints\": [],\n                        \"scalingPolicy\": null,\n                        \"scalable\": false\n                    }\n                ],\n                \"workerMetadataList\": [\n                    {\n                        \"workerIndex\": 0,\n                        \"workerNumber\": 1,\n                        \"jobId\": \"sine-function-7532\",\n                        \"stageNum\": 0,\n                        \"numberOfPorts\": 5,\n                        \"metricsPort\": 7150,\n                        \"consolePort\": 7152,\n                        \"debugPort\": 7151,\n                        \"customPort\": 7153,\n                        \"ports\": [\n                            7154\n                        ],\n                        \"state\": \"Started\",\n                        \"slave\": \"100.85.130.224\",\n                        \"slaveID\": \"079f4fa6-f910-4247-b5d0-f5574f36cace-S5274\",\n                        \"cluster\": \"mantisagent-main-m5.2xlarge-1\",\n                        \"acceptedAt\": 1576003739756,\n                        \"launchedAt\": 1576003739861,\n                        \"startingAt\": 1576003748558,\n                        \"startedAt\": 1576003749967,\n                        \"completedAt\": -1,\n                        \"reason\": \"Normal\",\n                        \"resubmitOf\": 0,\n                        \"totalResubmitCount\": 0\n                    },\n                    {\n                        \"workerIndex\": 0,\n                        \"workerNumber\": 3,\n                        \"jobId\": \"sine-function-7532\",\n                        \"stageNum\": 1,\n                        \"numberOfPorts\": 5,\n                        \"metricsPort\": 7165,\n                        \"consolePort\": 7167,\n                        \"debugPort\": 7166,\n                        \"customPort\": 7168,\n                        \"ports\": [\n                            7169\n                        ],\n                        \"state\": \"Started\",\n                        \"slave\": \"100.85.130.224\",\n                        \"slaveID\": \"079f4fa6-f910-4247-b5d0-f5574f36cace-S5274\",\n                        \"cluster\": \"mantisagent-main-m5.2xlarge-1\",\n                        \"acceptedAt\": 1576003959366,\n                        \"launchedAt\": 1576003959407,\n                        \"startingAt\": 1576003961542,\n                        \"startedAt\": 1576003962822,\n                        \"completedAt\": -1,\n                        \"reason\": \"Normal\",\n                        \"resubmitOf\": 2,\n                        \"totalResubmitCount\": 1\n                    }\n                ],\n                \"version\": \"0.2.9 2018-04-23 13:22:02\"\n            }\n        ],\n        \"prev\": null,\n        \"next\": null,\n        \"total\": 1\n    }\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#submit-a-new-job-for-a-particular-cluster","title":"Submit a New Job for a Particular Cluster","text":"<ul> <li><code>/api/v1/jobClusters/clusterName/jobs</code> (<code>POST</code>)</li> </ul> <p>To submit a new Job based on a Job Cluster, issue a <code>POST</code> command to the <code>/api/v1/jobClusters/clusterName/jobs</code> endpoint with a request body like the following:</p> Example Request Body <pre><code>{\n  \"name\": \"myClusterName\",\n  \"user\": \"jschmoe\",\n  \"jobJarFileLocation\": null,\n  \"version\": \"0.0.1 2019-02-06 13:30:07\",\n  \"subscriptionTimeoutSecs\": 0,\n  \"jobSla\": {\n    \"runtimeLimitSecs\": \"0\",\n    \"slaType\": \"Lossy\",\n    \"durationType\": \"Perpetual\",\n    \"userProvidedType\": \"\"\n  },\n  \"schedulingInfo\": {\n    \"stages\": {\n      \"0\": {\n        \"numberOfInstances\": 1,\n        \"machineDefinition\": {\n          \"cpuCores\": 0.35,\n          \"memoryMB\": 600,\n          \"networkMbps\": 30,\n          \"diskMB\": 100,\n          \"numPorts\": 1\n        },\n        \"hardConstraints\": null,\n        \"softConstraints\": null,\n        \"scalable\": false\n      },\n      \"1\": {\n        \"numberOfInstances\": 1,\n        \"machineDefinition\": {\n          \"cpuCores\": 0.35,\n          \"memoryMB\": 600,\n          \"networkMbps\": 30,\n          \"diskMB\": 100,\n          \"numPorts\": 1\n        },\n        \"hardConstraints\": null,\n        \"softConstraints\": null,\n        \"scalable\": true\n      }\n    }\n  },\n  \"parameters\": [\n    {\n      \"name\": \"criterion\",\n      \"value\": \"mock\"\n    },\n    {\n      \"name\": \"sourceJobName\",\n      \"value\": \"RequestSource\"\n    },\n    {\n      \"name\": \"spaasJobId\",\n      \"value\": \"spaasjschmoe-clsessionizer_backpressuretest\"\n    }\n  ],\n  \"isReadyForJobMaster\": false\n}\n</code></pre> Example Response Format <p>(Same as \"Get Information about a Job\" below) (Same as \"Get Information about a Job\" above)</p> Possible Response Codes Response Code Reason <code>201</code> normal response <code>404</code> no cluster with the given cluster name was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#update-a-job-cluster-and-submit-a-new-job-at-the-same-time","title":"Update a Job Cluster and Submit a New Job at the Same Time","text":"<ul> <li><code>/api/v1/jobs/actions/quickSubmit</code> (<code>POST</code>)</li> </ul> <p>You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts. This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster.</p> <p>To do this, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobs/actions/quickSubmit</code> with a body that matches the format of the following example:</p> Example Request Body <pre><code>{\n  \"name\": \"NameOfJobCluster\",\n  \"user\": \"myusername\",\n  \"jobSla\": {\n    \"durationType\": \"Perpetual\",\n    \"runtimeLimitSecs\": \"0\",\n    \"minRuntimeSecs\": \"0\",\n    \"userProvidedType\": \"\"\n  }\n}\n</code></pre> Example Response Format <p>You will receive in the response the Job ID of the newly submitted Job. E.g sine-test-5  sample response body?</p> Possible Response Codes Response Code Reason <code>201</code> normal response <code>404</code> no cluster was found with a cluster name matching the value of <code>name</code> from the request body <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#post-job-heartbeat-status","title":"Post Job Heartbeat Status","text":"<ul> <li><code>/api/v1/jobs/actions/postJobStatus</code> (<code>POST</code>)</li> </ul> Query Parameters <p><p>TBD</p></p> Example Request Body <pre><code>{\n  \"jobId\": \"sine-function-1\",\n  \"status\": {\n    \"jobId\": \"sine-function-1\",\n    \"stageNum\": 1,\n    \"workerIndex\": 0,\n    \"workerNumber\": 2,\n    \"type\": \"HEARTBEAT\",\n    \"message\": \"heartbeat\",\n    \"state\": \"Noop\",\n    \"hostname\": null,\n    \"timestamp\": 1525813363585,\n    \"reason\": \"Normal\",\n    \"payloads\": [\n      {\n        \"type\": \"SubscriptionState\",\n        \"data\": \"false\"\n      },\n      {\n        \"type\": \"IncomingDataDrop\",\n        \"data\": \"{\\\"onNextCount\\\":0,\\\"droppedCount\\\":0}\"\n      }\n    ]\n  }\n}\n</code></pre> Example Response Format <p><p>TBD</p></p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#horizontally-scale-a-stage","title":"Horizontally Scale a Stage","text":"<ul> <li><code>/api/v1/jobs/jobID/actions/scaleStage</code> (<code>POST</code>)</li> </ul> <p>To manually scale a Job Processing Stage, that is, to alter the number of workers assigned to that stage, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobs/jobID/actions/scaleStage</code> with a request body in the following format:</p> <p>Note</p> <p>You can only manually scale a Processing Stage if the Job was submitted with the <code>scalable</code> flag turned on.</p> Example Request Body <pre><code>{\n  \"JobId\": \"ValidatorDemo-33\",\n  \"StageNumber\": 1,\n  \"NumWorkers\": 3\n}\n</code></pre> <p><code>NumWorkers</code> here is the number of workers you want to be assigned to the stage after the scaling action takes place (that is, it is not the delta by which you want to change the number of workers in the stage).</p> Example Response Format <p><p>TBD</p></p> Possible Response Codes Response Code Reason <code>204</code> normal response <code>404</code> no Job with that Job ID was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#resubmit-a-worker","title":"Resubmit a Worker","text":"<ul> <li><code>/api/v1/jobs/jobID/actions/resubmitWorker</code> (<code>POST</code>)</li> </ul> <p>To resubmit a particular worker for a particular Job, send a <code>POST</code> command to the Mantis REST API endpoint <code>/api/v1/jobs/jobID/actions/resubmitWorker</code> with a request body resembling the following:</p> Example Request Body <pre><code>{\n  \"user\": \"jschmoe\",\n  \"workerNumber\": 5,\n  \"reason\": \"test worker resubmit\"\n}\n</code></pre> <p>Note</p> <p><code>workerNumber</code> is the worker number not the worker index.</p> Example Response Format <p><p>TBD</p></p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no Job with that Job ID was found <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#administrative-tasks","title":"Administrative Tasks","text":"<p>TBD</p>"},{"location":"reference/api/#return-job-master-information","title":"Return Job Master Information","text":"<ul> <li><code>/api/v1/masterInfo</code> (<code>GET</code>)</li> </ul> <p>TBD</p> Example Response Body <pre><code>{\n  \"hostname\": \"100.86.121.198\",\n  \"hostIP\": \"100.86.121.198\",\n  \"apiPort\": 7101,\n  \"schedInfoPort\": 7101,\n  \"apiPortV2\": 7075,\n  \"apiStatusUri\": \"api/v1/jobs/actions/postJobStatus\",\n  \"consolePort\": 7101,\n  \"createTime\": 1548803881867,\n  \"fullApiStatusUri\": \"http://100.86.121.198:7101/api/v1/jobs/actions/postJobStatus\"\n}\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#return-job-master-configs","title":"Return Job Master Configs","text":"<ul> <li><code>/api/v1/masterConfigs</code> (<code>GET</code>)</li> </ul> <p>TBD</p> Example Response Body <pre><code>[\n  {\n    \"name\": \"JobConstraints\",\n    \"value\": \"[\\\"UniqueHost\\\",\\\"ExclusiveHost\\\",\\\"ZoneBalance\\\",\\\"M4Cluster\\\",\\\"M3Cluster\\\",\\\"M5Cluster\\\"]\"\n  },\n  {\n    \"name\": \"ScalingReason\",\n    \"value\": \"[\\\"CPU\\\",\\\"Memory\\\",\\\"Network\\\",\\\"DataDrop\\\",\\\"KafkaLag\\\",\\\"UserDefined\\\",\\\"KafkaProcessed\\\"]\"\n  },\n  {\n    \"name\": \"MigrationStrategyEnum\",\n    \"value\": \"[\\\"ONE_WORKER\\\",\\\"PERCENTAGE\\\"]\"\n  },\n  {\n    \"name\": \"WorkerResourceLimits\",\n    \"value\": \"{\\\"maxCpuCores\\\":8,\\\"maxMemoryMB\\\":28000,\\\"maxNetworkMbps\\\":1024}\"\n  }\n]\n</code></pre> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#get-information-about-agent-clusters","title":"Get Information about Agent Clusters","text":"<ul> <li><code>/api/v1/agentClusters/</code> (<code>GET</code>)</li> </ul> <p>Returns information about active agent clusters (agent clusters are physical AWS resources that Mantis connects to).</p> Example Response Body <p>response body?</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#activate-or-deactivate-an-agent-cluster","title":"Activate or Deactivate an Agent Cluster","text":"<ul> <li><code>/api/v1/agentClusters/</code> (<code>POST</code>)</li> </ul> <p>TBD</p> Example Request Body <pre><code>[\"mantisagent-staging-cl1-m5.2xlarge-1-v00\"]\n</code></pre> <p>(an array of active clusters)</p> Example Response Body <p>response body?</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>400</code> client failure <code>405</code> incorrect HTTP verb (use <code>POST</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#retrieve-the-agent-cluster-scaling-policy","title":"Retrieve the Agent Cluster Scaling Policy","text":"<ul> <li><code>/api/v1/agentClusters/autoScalePolicy</code> (<code>GET</code>)</li> </ul> <p>TBD</p> Example Response Body <p>response body?</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#get-jobs-and-host-information-for-an-agent-cluster","title":"Get Jobs and Host Information for an Agent Cluster","text":"<ul> <li><code>/api/v1/agentClusters/jobs</code> (<code>GET</code>)</li> </ul> <p>TBD</p> Example Response Body <p>response body?</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#streaming-websocketsse-tasks","title":"Streaming WebSocket/SSE Tasks","text":"<p>TBD: introduction</p>"},{"location":"reference/api/#stream-job-status-changes","title":"Stream Job Status Changes","text":"<ul> <li><code>ws://masterHost:7101/api/v1/jobStatusStream/jobID</code></li> </ul> <p>TBD: description</p> <p>See also: mantisapi/websocket/</p> Example Response Stream Excerpt <pre><code>{\n  \"status\": {\n    \"jobId\": \"SPaaSBackpressureDemp-26\",\n    \"stageNum\": 1,\n    \"workerIndex\": 0,\n    \"workerNumber\": 7,\n    \"type\": \"INFO\",\n    \"message\": \"SPaaSBackpressureDemp-26-worker-0-7 worker status update\",\n    \"state\": \"StartInitiated\",\n    \"hostname\": null,\n    \"timestamp\": 1549404217065,\n    \"reason\": \"Normal\",\n    \"payloads\": []\n  }\n}\n</code></pre>"},{"location":"reference/api/#stream-scheduling-information-for-a-job","title":"Stream Scheduling Information for a Job","text":"<ul> <li><code>/api/v1/jobDiscoveryStream/jobID</code> (<code>GET</code>)</li> <li><code>/api/v1/jobs/schedulingInfo/jobID</code> (<code>GET</code>)</li> </ul> <p>To retrieve an SSE stream of scheduling information for a particular job, send an HTTP <code>GET</code> command to either the Mantis REST API endpoint <code>/api/v1/jobDiscoveryStream/jobID</code> or <code>/api/v1/jobs/schedulingInfo/jobID</code>.</p> Query Parameters <code>jobDiscoveryStream</code> Query Parameter Purpose <code>sendHB</code> (optional) Indicate whether or not to send heartbeats (default=<code>false</code>). <code>jobs/schedulingInfo</code> Query Parameter Purpose <code>jobId</code> (required) The job ID of the job for which scheduling information is to be streamed. Example Response Stream Excerpt <p>response body?</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no Job with that Job ID was found <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#get-streaming-sse-discovery-info-for-a-cluster","title":"Get Streaming (SSE) Discovery Info for a Cluster","text":"<ul> <li><code>/api/v1/jobClusters/discoveryInfoStream/clusterName</code> (<code>GET</code>)</li> </ul> <p>describe</p> Example Response Format <p><p>TBD</p></p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#stream-job-information-for-a-cluster","title":"Stream Job Information for a Cluster","text":"<ul> <li><code>/api/v1/lastSubmittedJobIdStream/clusterName</code> (<code>GET</code>)</li> </ul> <p>To retrieve an SSE stream of job information for a particular cluster, send an HTTP <code>GET</code> command to the Mantis REST API endpoint <code>/api/v1/lastSubmittedJobIdStream/clusterName</code>.</p> Query Parameters Query Parameter Purpose <code>sendHB</code> Indicate whether or not to send heartbeats. Example Response Stream Excerpt <p>response body?</p> Possible Response Codes Response Code Reason <code>200</code> normal response <code>404</code> no Cluster with that Cluster name was found <code>405</code> incorrect HTTP verb (use <code>GET</code> instead) <code>500</code> unknown server error"},{"location":"reference/api/#stream-job-sink-messages","title":"Stream Job Sink Messages","text":"<ul> <li><code>/api/v1/jobConnectbyid/jobID</code> (SSE)</li> <li><code>/api/v1/jobConnectbyname/jobName</code> (SSE)</li> </ul> <p>Collect messages from the job sinks and merge them into a single websocket or SSE stream.</p> Example Response Stream Excerpt <pre><code>data: {\"x\": 928400.000000, \"y\": 3.139934}\ndata: {\"x\": 928402.000000, \"y\": -9.939772}\ndata: {\"x\": 928404.000000, \"y\": 5.132876}\ndata: {\"x\": 928406.000000, \"y\": 5.667712}\n</code></pre>"},{"location":"reference/api/#submit-job-and-stream-job-sink-messages","title":"Submit Job and Stream Job Sink Messages","text":"<ul> <li><code>/api/v1/jobsubmitandconnect</code> (<code>POST</code>)</li> </ul> <p>To submit a job and then collect messages from the job sinks and merge them into a single websocket or SSE stream, send a <code>POST</code> request to <code>/api/v1/jobsubmitandconnect</code> with a request body that describes the job.</p> Example Request Body <pre><code>{\n  \"name\": \"ValidatorDemo\",\n  \"user\": \"jschmoe\",\n  \"jobSla\": {\n    \"durationType\": \"Perpetual\",\n    \"runtimeLimitSecs\": \"0\",\n    \"minRuntimeSecs\": \"0\",\n    \"userProvidedType\": \"\"\n  }\n}\n</code></pre> Example Response Stream Excerpt <pre><code>data: {\"x\": 928400.000000, \"y\": 3.139934}\ndata: {\"x\": 928402.000000, \"y\": -9.939772}\ndata: {\"x\": 928404.000000, \"y\": 5.132876}\ndata: {\"x\": 928406.000000, \"y\": 5.667712}\n</code></pre>"},{"location":"reference/api/#pagination","title":"Pagination","text":"<p>You can configure some endpoints to return their responses in pages. This is to say that rather than returning all of the data responsive to a request all at once, the endpoint can return a specific subset of the data at a time.</p> <p>An endpoint that supports pagination will typically do so by means of two query parameters:</p> Query Parameter Purpose <code>pageSize</code> the maximum number of records to return in this request (default = 0, which means all records) <code>offset</code> the record number to begin with in the set of records to return in this request <p>So, for example, you might make consecutive requests for\u2026</p> <ol> <li><code>endpoint?pageSize=10&amp;offset=0</code></li> <li><code>endpoint?pageSize=10&amp;offset=10</code></li> <li><code>endpoint?pageSize=10&amp;offset=20</code></li> </ol> <p>\u2026and so forth, until you reach the final page of records.</p> <p>When you request records in a paginated fashion like this, the Mantis API will enclose them in a JSON structure like the following:</p>  {   \"list\": [     {       \"Id\": originalOffset,       \u22ee     },     {       \"Id\": originalOffset+1,       \u22ee     },     \u22ee   ],   \"prev\": \"/api/v1/endpoint?pageSize=pageSize&amp;offset=originalOffset+pageSize\",   \"next\": \"/api/v1/endpoint?pageSize=pageSize&amp;offset=originalOffset\u2212pageSize\" }  <p>For example:</p> <pre><code>{\n\"list\": [\n{\n\"Id\": 101,\n\u22ee\n},\n{\n\"Id\": 102,\n\u22ee\n}\n],\n\"next\": \"/api/v1/someResource?pageSize=20&amp;offset=120\",\n\"prev\": \"/api/v1/someResource?pageSize=20&amp;offset=80\"\n}\n</code></pre> <p><code>prev</code> and/or <code>next</code> will be <code>null</code> if there is no previous or next page, that is, if you are at the first and/or last page in the pagination.</p>"},{"location":"reference/api/#websocket","title":"WebSocket","text":"<p>Some desktops have trouble dealing with SSE. In such a case you can use WebSocket to connect to a Job (other API features are only available via the Mantis REST API).</p> <p>The WebSocket API runs from the same servers as the Mantis REST API, and you can reach it by using the <code>ws://</code> protocol on port 7102 or the <code>wss://</code> protocol on port 7103.</p>"},{"location":"reference/api/#connecting-to-job-output-sink","title":"Connecting to Job Output (Sink)","text":"<ul> <li> <p>Append this to the WebSocket URI to connect to a Job by name: <code>/jobconnectbyname/JobName</code></p> </li> <li> <p>Append this to the WebSocket URI to connect to a specific running Job ID: <code>/jobconnectbyid/JobID</code></p> </li> </ul> <p>Upon connecting, the server starts writing messages that are coming in from the corresponding Jobs. </p> <p>You can append query parameters to the WebSocket URI (preceded by \u201c<code>?</code>\u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c<code>sampleMSec=mSecs</code>\u201d Sink parameter which limits the rate at which the Sink output is sampled by the server.</p>"},{"location":"reference/api/#submitting-and-connecting-to-job-output","title":"Submitting and Connecting to Job Output","text":"<ul> <li>Append this to the WebSocket URI to submit and connect to the submitted Job: <code>/jobsubmitandconnect</code></li> </ul> <p>Note that you will have to send one message on the WebSocket, which is the same JSON payload that you would send as a POST body for the equivalent REST API. The server then submits the Job and then starts writing messages into the WebSocket as it gets output from the Job.</p> <p>You can append query parameters to the WebSocket URI (preceded by \u201c<code>?</code>\u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c<code>sampleMSec=mSecs</code>\u201d sink parameter which limits the rate at which the Sink output is sampled by the server.</p>"},{"location":"reference/api/#example-javascript-to-connect-a-job","title":"Example: Javascript to Connect a Job","text":"<pre><code>&lt;!DOCTYPE html&gt;  \n&lt;meta charset=\"utf-8\" /&gt;  \n&lt;title&gt;WebSocket Test&lt;/title&gt;  \n&lt;script language=\"javascript\" type=\"text/javascript\"&gt;  var wsUri = \"ws://your-domain.com/jobconnectbyname/YourJobName?sampleMSec=2000\";\nvar output;  function init() { output = document.getElementById(\"output\"); testWebSocket(); }\nfunction testWebSocket() { websocket = new WebSocket(wsUri);\nwebsocket.onopen = function(evt) { onOpen(evt) }; websocket.onclose = function(evt) { onClose(evt) }; websocket.onmessage = function(evt) { onMessage(evt) }; websocket.onerror = function(evt) { onError(evt) }; }  function onOpen(evt) {\nwriteToScreen(\"CONNECTED\"); }\nfunction onClose(evt) { writeToScreen(\"DISCONNECTED\"); }\nfunction onMessage(evt) {\nwriteToScreen('&lt;span style=\"color: blue;\"&gt;RESPONSE: ' + evt.data+'&lt;/span&gt;');\n}\nfunction onError(evt) {\nwriteToScreen('&lt;span style=\"color: red;\"&gt;ERROR:&lt;/span&gt; ' + evt.data);\n}\nfunction doSend(message) { websocket.send(message); }\nfunction writeToScreen(message) { var pre = document.createElement(\"p\"); pre.style.wordWrap = \"break-word\"; pre.innerHTML = message; output.appendChild(pre); }\nwindow.addEventListener(\"load\", init, true);  &lt;/script&gt;  \n&lt;h2&gt;WebSocket Test&lt;/h2&gt;  \n&lt;div id=\"output\"&gt;&lt;/div&gt;\n</code></pre>"},{"location":"reference/api/#example-javascript-to-submit-a-job-and-connect-to-it","title":"Example: Javascript to Submit a Job and Connect to It","text":"<pre><code>&lt;!DOCTYPE html&gt;  \n&lt;meta charset=\"utf-8\" /&gt;  \n&lt;title&gt;WebSocket Test&lt;/title&gt;  \n&lt;script language=\"javascript\" type=\"text/javascript\"&gt;  var wsUri = \"ws://your-domain.com/jobsubmitandconnect/\"; var output;  function init() { output = document.getElementById(\"output\"); testWebSocket(); }\nfunction testWebSocket() { websocket = new WebSocket(wsUri);\nwebsocket.onopen = function(evt) { onOpen(evt) }; websocket.onclose = function(evt) { onClose(evt) }; websocket.onmessage = function(evt) { onMessage(evt) }; websocket.onerror = function(evt) { onError(evt) }; }  function onOpen(evt) { writeToScreen(\"CONNECTED\"); // Change this to your job's submit json content.\n// See job submit REST API above for another example.\ndoSend(\"{\\n\" +\n\"  \\\"name\\\":\\\"Outliers-mock3\\\",\\n\" +\n\"  \\\"version\\\":\\\"\\\",\\n\" +\n\"  \\\"parameters\\\":[],\\n\" +\n\"  \\\"jobSla\\\":{\\\"runtimeLimitSecs\\\":0,\\\"durationType\\\":\\\"Transient\\\",\\\"userProvidedType\\\":\\\"{\\\\\\\"unique\\\\\\\":\\\\\\\"foobar\\\\\\\"}\\\"},\\n\" +\n\"  \\\"subscriptionTimeoutSecs\\\":\\\"90\\\",\\n\" +\n\"  \\\"jobJarFileLocation\\\":null,\\n\" +\n\"  \\\"schedulingInfo\\\":{\\\"stages\\\":{\\\"1\\\":{\\\"numberOfInstances\\\":1,\\\"machineDefinition\\\":{\\\"cpuCores\\\":1.0,\\\"memoryMB\\\":2048.0,\\\"diskMB\\\":1.0,\\\"scalable\\\":\\\"true\\\"}}}\\n\" +\n\"}\");\n}  function onClose(evt) { writeToScreen(\"DISCONNECTED\"); }\nfunction onMessage(evt) {\nwriteToScreen('&lt;span style=\"color: blue;\"&gt;RESPONSE: ' + evt.data+'&lt;/span&gt;');\n}\nfunction onError(evt) {\nwriteToScreen('&lt;span style=\"color: red;\"&gt;ERROR:&lt;/span&gt; ' + evt.data);\n}\nfunction doSend(message) { websocket.send(message); }\nfunction writeToScreen(message) { var pre = document.createElement(\"p\"); pre.style.wordWrap = \"break-word\"; pre.innerHTML = message; output.appendChild(pre); }\nwindow.addEventListener(\"load\", init, true);  &lt;/script&gt;  \n&lt;h2&gt;WebSocket Test&lt;/h2&gt;  \n&lt;div id=\"output\"&gt;&lt;/div&gt;\n</code></pre>"},{"location":"reference/cli/","title":"CLI","text":""},{"location":"reference/cli/#introduction","title":"Introduction","text":"<p>You can interact with Mantis Job Clusters by means of a command-line interface (CLI). This page explains how to install and use this interface.</p>"},{"location":"reference/cli/#installation","title":"Installation","text":"<p>You can install the Mantis CLI locally in your development environment by following the instructions on the Mantis CLI Readme.</p> <p>Additionally, you can install the Mantis CLI onto your system as a package by following instructions on the Mantis CLI Releasing section of the readme.</p>"},{"location":"reference/cli/#mantis-commands","title":"Mantis Commands","text":"<p>A Mantis command takes the following form:</p> mantis command <p>You can also issue the following command to learn about the various Mantis commands available to you from the command line:</p> mantis --help <p>Or you can issue the following command to learn about the use of a specific Mantis command:</p> mantis command --help"},{"location":"reference/cli/#setting-up-your-aws-credentials","title":"Setting Up Your AWS Credentials","text":"<p>Mantis can operate natively on different clouds. By default, Mantis provides CLI facilities for operating Mantis in the Amazon AWS environment.</p> <p>Before you can use the Mantis CLI to interact with Mantis Clusters in AWS, you must first establish your AWS credentials with Mantis. To do this, first obtain your AWS access key and secret access key, then issue the following command:</p> mantis aws:configure [-k|--key=access-key] [-s|--secret=secret-access-key] <p>This command will store your AWS credentials to the same location and with the same file format as the Amazon AWS CLI.</p>"},{"location":"reference/cli/#bootstrapping-a-mantis-cluster-in-aws","title":"Bootstrapping a Mantis Cluster in AWS","text":"<p>You can use the Mantis CLI to bootstrap a Mantis Cluster in AWS. This will create the following:</p> <ol> <li>AWS key pair</li> <li>Default VPC</li> <li>Security groups</li> <li>Single Zookeeper EC2 instance backed by EBS volume</li> <li>Single Mesos Master EC2 instance backed by EBS volume</li> <li>Single Mesos Agent EC2 instance backed by EBS volume</li> <li>Single Mantis Master EC2 instance backed by EBS volume</li> <li>Single Mantis API EC2 instance backed by EBS volume</li> </ol> <p>To do this issue the following command:</p> mantis aws:bootstrap <p>Note</p> <p>Before you issue the <code>mantis aws:bootstrap</code> command you must have first set up your AWS credentials. You can do that with the <code>mantis aws:configure</code> command (see above).</p> <p>By default, the Mantis CLI bootstrap command is interactive and will prompt you for provisioning choices. Additionally, you can pass the following command-line parameters to skip some of the prompts from the <code>mantis aws:bootstrap</code> command:</p> prompt parameter valid inputs select a region [<code>-r</code>|<code>--region</code>] <code>region</code> <code>us-east-1</code>, <code>us-east-2</code>, <code>us-west-2</code> confirm <code>-y</code> n/a <p>After you have finished executing the <code>mantis aws:bootstrap</code> command, you can submit Jobs into the Mantis cluster that it creates.</p>"},{"location":"reference/dsl/","title":"Job DSL","text":"<p>Mantis leverages Reactive Streams for its Job DSL, specifically RxJava 1.x.</p> <p>Refer to the RxJava 1.x Operator Guide for more details.</p>"},{"location":"reference/mql/","title":"MQL","text":"<p>This page introduces the MQL operators, including a syntax summary, usage examples, and a brief description of each operator.</p> <p>For the most part these operations use their RxJava analogs:</p> MQL operator RxJava operator <code>select</code> <code>map</code> <code>window</code> <code>window</code> <code>where</code> <code>filter</code> <code>group by</code> <code>groupBy</code> <code>order by</code> n/a <code>limit</code> <code>take</code> <p>For this reason you may find the ReactiveX documentation useful as well, though it is a goal of MQL to provide you with a layer of abstraction such that you will not need to use or think about ReactiveX when writing queries.</p> <p>MQL attempts to stay true to the SQL syntax in order to reduce friction for new users writing queries and to allow them to leverage their experiences with SQL.</p> <p>An essential concept in MQL is the \u201cproperty,\u201d which can take on one of several forms:</p> <ul> <li><code>property.name.here</code></li> <li><code>e[\"this\"][\"accesses\"][\"nested\"][\"properties\"]</code></li> <li><code>15</code></li> <li><code>\"string literal\"</code></li> </ul> <p>Most of the MQL operators use these forms to refer to properties within the event stream, as well as string and numeric literals.</p> <p>The last detail of note concerns unbounded vs. discrete streams. ReactiveX Observables that are not expected to close are an unbounded stream and must be bounded/discretized in order for certain operators to function with them. The <code>window</code> operator is useful for partitioning unbounded streams into discrete bounded streams for operators such as aggregate, group by, or order by.</p>"},{"location":"reference/mql/#select","title":"<code>select</code>","text":"<p>Syntax: <code>select property from source</code></p> <p>Examples:</p> <ul> <li><code>\"select * from servo\"</code></li> <li><code>\"select nf.node from servo\"</code></li> <li><code>\"select e[\"tags\"][\"nf.cluster\"] from servo\"</code></li> </ul> <p>The <code>select</code> operator allows you to project data into a new object by specifying which properties should be carried forward into the output. Properties will bear the same name as was used to select them. In the case of numbers and string literals their value will also be their name. In the case of nested properties the result will be a top-level object joined with dots.</p> <p>For example, the following <code>select</code> example\u2026</p> <ul> <li><code>\"select \"string literal\", 45, e[\"tags\"][\"cluster\"], nf.node from servo\"</code></li> </ul> <p>\u2026would result in an object like:</p> <pre><code>{\n\"string literal\": \"string literal\",\n45: 45,\n\"tags.cluster\": \"mantisagent\",\n\"nf.node\": \"i-123456\"\n}\n</code></pre> <p>Warning</p> <p>Be careful to avoid collisions between the top-level objects with dotted names and the nested objects that result in top-level properties with dotted names.</p>"},{"location":"reference/mql/#aggregates","title":"Aggregates","text":"<p>Syntax: <code>\"select aggregate(property) from source\"</code></p> <p>Examples:</p> <ul> <li><code>\"select count(e[\"node\"]) from servo window 60\"</code></li> <li><code>\"select count(e[\"node\"]) from servo window 60 where e[\"metrics\"][\"latency\"] &gt; 350\"</code></li> <li><code>\"select average(e[\"metrics\"][\"latency\"]), e[\"node\"] from servo window 10\"</code></li> </ul> <p>Supported Aggregates:</p> <ul> <li><code>Min</code></li> <li><code>Max</code></li> <li><code>Average</code></li> <li><code>Count</code></li> <li><code>Sum</code></li> </ul> <p>Aggregates add analysis capabilities to MQL in order to allow you to answer interesting questions about data in real-time. They can be intermixed with regular properties in order to select properties and compute information about those properties, such as the last example above which computes average latency on a per-node basis in 10 second windows.</p> <p>Note</p> <p>Aggregates require that the stream on which they operate be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the <code>window</code> operator on an unbounded stream.</p> <p>You can use the <code>distinct</code> operator on a property to operate only on unique items within the window. This is particularly useful with the <code>count</code> aggregate if you want to count the number of items with some distinct property value in that window. For example:</p> <ul> <li><code>\"select count(distinct esn) from stream window 10 1\"</code></li> </ul>"},{"location":"reference/mql/#from","title":"<code>from</code>","text":"<p>Syntax: <code>\"select something from source\"</code></p> <p>Example:</p> <ul> <li><code>\"select * from servo\"</code></li> </ul> <p>The <code>from</code> clause indicates to MQL from which Observable it should draw data. This requires some explanation, as it bears different meaning in different contexts.</p> <p>Directly against queryable sources the <code>from</code> clause refers to the source Observable no matter which name is given. The operator is in fact optional, and the name of the source is arbitrary in this context, and the clause will be inserted for you if you omit it.</p> <p>When you use MQL as a library, the <code>source</code> corresponds with the names in the context map parameter. The second parameter to <code>eval-mql()</code> is a <code>Map&lt;String, Observable&lt;T&gt;&gt;</code> and the <code>from</code> clause will attempt to fetch the <code>Observable</code> from this <code>Map</code>.</p>"},{"location":"reference/mql/#window","title":"<code>window</code>","text":"<p>Syntax: <code>\"WINDOW integer\"</code></p> <p>Examples:</p> <ul> <li><code>\"select node, latency from servo window 10\"</code></li> <li><code>\"select MAX(latency) from servo window 60\"</code></li> </ul> <p>The <code>window</code> clause divides an otherwise unbounded stream of data into discrete time-bounded streams. The <code>integer</code> parameter is the number of seconds over which to perform this bounding. For example <code>\"select * from observable window 10\"</code> will produce 10-second windows of data.</p> <p>This discretization of streams is important for use with aggregate operations as well as group-by and order-by clauses which cannot be executed on, and will hang on, unbounded streams.</p>"},{"location":"reference/mql/#where","title":"<code>where</code>","text":"<p>Syntax: <code>\"select property from source where predicate\"</code></p> <p>Examples: </p> <ul> <li><code>\"select * from servo where node == \"i-123456\" AND e[\"metrics\"][\"latency\"] &gt; 350\"</code></li> <li><code>\"select * from servo where (node == \"i-123456\" AND e[\"metrics\"][\"latency\"] &gt; 350) OR node == \"1-abcdef\"\"</code></li> <li><code>\"select * from servo where node ==~ /i-123/\"</code></li> <li><code>\"select * from servo where e[\"metrics\"][\"latency\"] != null</code></li> <li><code>\"select * from servo where e[\"list_of_requests\"][*][\"status\"] == \"success\"</code></li> </ul> <p>The <code>where</code> clause filters any events out of the stream which do not match a given predictate. Predicates support <code>AND</code> and <code>OR</code> operations. Binary operators supported are <code>=</code>, <code>==</code>, <code>&lt;&gt;</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==~</code>. The first two above are both equality, and either of the next two represent not-equal. You can use the last of those operators, <code>==~</code>, with a regular expression as in: <code>\"where property ==~ /regex/\"</code> (any Java regular expression will suffice). To take an event with certain attribute, use <code>e[\"{{key}}\"] != null</code>.</p> <p>If the event contains a list field, you can use the <code>[*]</code> operator to match objects inside that list. For example, say the events have a field called <code>list_of_requests</code> and each item in the list has a field called <code>status</code>. Then, the condition <code>e[\"list_of_requests\"][*][\"status\"] == \"success\"</code> will return true if at least 1 item has <code>status</code> equals <code>success</code>. Further, you can combine multiple conditions on the list. For example,</p> <pre><code>e[\"list_of_requests\"][*][\"status\"] == \"success\" AND e[\"list_of_requests\"][*][\"url\"] == \"abc\"\n</code></pre> <p>This condition returns true if at least 1 item has <code>status</code> equals <code>success</code> and <code>url</code> equals <code>abc</code>.</p>"},{"location":"reference/mql/#group-by","title":"<code>group by</code>","text":"<p>Syntax: <code>\"GROUP BY property\"</code></p> <p>Examples:</p> <ul> <li><code>\"select node, latency from servo where latency &gt; 300.0 group by node\"</code></li> <li><code>\"select MAX(latency), e[\"node\"] from servo group by node\"</code></li> </ul> <p><code>group by</code> groups values in the output according to some property. This is particularly useful in conjunction with aggregate operators in which one can compute aggregate values over a group. For example, the following query calculates the maximum latency observed for each node in 60-second windows:</p> <ul> <li><code>\"select MAX(latency), e[\"node\"] from servo window 60 group by node\"</code></li> </ul> <p>Note</p> <p>The <code>group by</code> clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the <code>window</code> operator on an unbounded stream.</p>"},{"location":"reference/mql/#order-by","title":"<code>order by</code>","text":"<p>Syntax: <code>\"ORDER BY property\"</code></p> <p>Example:</p> <ul> <li><code>\"select node, latency from servo group by node order by latency\"</code></li> </ul> <p>The <code>order by</code> operator orders the results in the inner-most Observable by the specified property. For example, the query <code>\"select * from observable window 5 group by nf.node order by latency\"</code> would produce an Observable of Observables (windows) of Observables (groups). The events within the groups would be ordered by their latency property.</p> <p>Note</p> <p>The <code>order by</code> clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the <code>window</code> operator on an unbounded stream.</p>"},{"location":"reference/mql/#limit","title":"<code>limit</code>","text":"<p>Syntax: <code>\"LIMIT integer\"</code></p> <p>Examples:</p> <ul> <li><code>\"select * from servo limit 10\"</code></li> <li><code>\"select AVERAGE(latency) from servo window 60 limit 10\"</code></li> </ul> <p>The limit operator takes as a parameter a single integer and bounds the number of results to \u2264 <code>integer</code>.</p> <p>Note</p> <p>Limit does not discretize the stream for earlier operators such as <code>group by</code>, <code>order by</code>, aggregates.</p>"},{"location":"updates/gsoc2023-ideas/","title":"Project ideas for Google Summer of Code 2023","text":"<p>Here is a list of project ideas you can contribute to as part of the Google Summer of Code (GSoC) 2023 program!</p>"},{"location":"updates/gsoc2023-ideas/#mantis-in-a-process-for-local-testing","title":"Mantis in a process for local testing","text":""},{"location":"updates/gsoc2023-ideas/#abstract","title":"Abstract","text":"<p>Running Mantis jobs locally currently requires running multiple core components like Zookeeper, Mantis Master, and Mantis Task Executors, which involves complex configurations and multiple independent processes. This makes it challenging for developers to test and debug their Mantis jobs before submitting them to production clusters. By bringing all core components of Mantis within the same JVM, developers can test and debug their applications more easily without the need to set up a full cluster.</p>"},{"location":"updates/gsoc2023-ideas/#deliverables","title":"Deliverables","text":"<ul> <li>Mantis MiniCluster pluggable and usable in tests.</li> <li>Documentation on how to setup/use MiniCluster.</li> <li>Migrating N mantis examples to use the MiniCluster and validate the functionality.</li> </ul>"},{"location":"updates/gsoc2023-ideas/#required-skillset","title":"Required SkillSet","text":"<ul> <li>Java</li> </ul>"},{"location":"updates/gsoc2023-ideas/#skills-that-you-will-gain","title":"Skills that you will gain","text":"<ul> <li>You will learn about building distributed systems as part of this project.</li> <li>You will learn about you can leverage actor frameworks such as Akka to write safe, high performance code.</li> <li>You will also learn a lot about stream processing and how Mantis leverages stream processing to analyze operational data.</li> </ul>"},{"location":"updates/gsoc2023-ideas/#difficulty-rating","title":"Difficulty rating","text":"<p>Medium</p>"},{"location":"updates/gsoc2023-ideas/#mantis-connection-monitor","title":"Mantis Connection Monitor","text":""},{"location":"updates/gsoc2023-ideas/#abstract_1","title":"Abstract","text":"<p>Mantis is a highly reliable and scalable distributed systems platform, but there are instances where the callbacks on life cycle events such as channel inactive and exception are not triggered. This can lead to Mantis still thinking a connection is live even after it has been terminated by the client. To address this challenge, the aim of this project is to create a connection monitor within Mantis that tracks the connection status. This monitor will run in the background and periodically check the state of each connection. If it detects that a connection is no longer writable but Mantis has not triggered an inactive callback, it will force close the connection, ensuring the stability and reliability of the system.</p>"},{"location":"updates/gsoc2023-ideas/#expected-outcomes","title":"Expected outcomes","text":"<ul> <li>The connection monitor will be implemented using the Singleton design pattern, ensuring only one instance of the monitor is created in the system.</li> <li>The monitor will use a periodic checking mechanism to evaluate the state of each connection.</li> <li>If the monitor detects a connection that is no longer writable but Mantis has not triggered an inactive callback, it will force close the connection.</li> <li>The monitor will log extensive metrics to observe the severity and trends of the issue, providing valuable insights for future improvements.</li> <li>The monitor will undergo extensive testing to verify its reliability and robustness within the Mantis system.</li> </ul>"},{"location":"updates/gsoc2023-ideas/#recommended-skills","title":"Recommended skills","text":"<p>Java, RxJava, RxNetty</p>"},{"location":"updates/gsoc2023-ideas/#mentors","title":"Mentors","text":"<ul> <li>Calvin Cheung</li> <li>Sundaram Ananthanarayanan</li> <li>Harshit Mittal</li> </ul>"},{"location":"updates/gsoc2023-ideas/#difficulty-rating_1","title":"Difficulty rating","text":"<p>Hard</p>"},{"location":"updates/gsoc2023-ideas/#capability-to-publish-to-mantis-streams-from-non-jvm-languages","title":"Capability to publish to Mantis streams from non-JVM languages","text":""},{"location":"updates/gsoc2023-ideas/#abstract_2","title":"Abstract","text":"<p>Develop a new specification for selecting Mantis-Realtime-Events (MRE) from applications using a JSON or protobuf format with native support in Python and Node.js. Provide webassembly support for other languages as a fallback option.</p>"},{"location":"updates/gsoc2023-ideas/#context","title":"Context","text":"<p>Applications and web services use the mantis-publish library to create Mantis Real-time Events (MRE) data as requests and responses. These events are triggered based on user-defined criteria specified using the Mantis Query Language (MQL), a SQL-like interface implemented in Clojure. However, the use of a JVM-based language makes it challenging to port MQL to non-JVM languages like Python and Go.</p> <p>The mantis-publish library only requires a simplified version of MQL, known as Simplified-MQL, which includes projections, sampling, and filters. To address the language compatibility issue, we want to define a language-agnostic interface using either JSON or Protocol Buffers for this Simplified-MQL. This will enable support for other programming languages.</p>"},{"location":"updates/gsoc2023-ideas/#goals-for-project","title":"Goals for project","text":"<ol> <li>Define a new spec to specify filter, projection, and sample criteria currently supported by MQL. Document the new spec.</li> <li>Implement the spec in python, golang, or rust with unit-tests.</li> <li>Write a parser to convert simplified-MQL to the spec.</li> <li>Replicate mantis-publish java library in that programming language</li> </ol>"},{"location":"updates/gsoc2023-ideas/#difficulty-rating_2","title":"Difficulty rating","text":"<p>Hard</p>"},{"location":"updates/gsoc2023-ideas/#resource-cluster-management-user-interface","title":"Resource Cluster Management User Interface","text":""},{"location":"updates/gsoc2023-ideas/#abstract_3","title":"Abstract","text":"<p>Develop a user interface to allow Mantis admins to manage resource clusters and allow end-users to select resource clusters to run their jobs on.</p>"},{"location":"updates/gsoc2023-ideas/#deliverables_1","title":"Deliverables","text":"<ul> <li>Full support for CRUD operations on resource clusters</li> <li>Support for specifying scaling operations and upgrading clusters</li> <li>Allow users to select a resource cluster before submitting a job and a container SKU per job stage</li> </ul>"},{"location":"updates/gsoc2023-ideas/#required-skillset_1","title":"Required SkillSet","text":"<ul> <li>Javascript, HTML, CSS</li> <li>Experience using a front end framework such as React, Angular, Vue</li> </ul>"},{"location":"updates/gsoc2023-ideas/#skills-that-you-will-gain_1","title":"Skills that you will gain","text":"<ul> <li>You will learn about how to create a UI feature from design, implementation, and testing</li> </ul>"},{"location":"updates/gsoc2023-ideas/#difficulty-rating_3","title":"Difficulty rating","text":"<p>Medium</p>"},{"location":"updates/gsoc2023-ideas/#user-interface-for-querying-real-time-streaming-data-on-mantis","title":"User Interface for Querying Real-time Streaming Data on Mantis","text":""},{"location":"updates/gsoc2023-ideas/#abstract_4","title":"Abstract","text":"<p>Mantis provides a real-time streaming platform for processing data at scale. However, there is currently no way for a user to query their data and show the results in a user friendly way. This project aims to develop a UI that allows users to query their data in Mantis and visualize the results.</p>"},{"location":"updates/gsoc2023-ideas/#deliverables_2","title":"Deliverables","text":"<ul> <li>New Query page featuring a query editor to allow users to write queries</li> <li>Page to list of recent queries that the user has previously submitted</li> <li>Page to list and create recipes which are saved templates for queries</li> </ul>"},{"location":"updates/gsoc2023-ideas/#required-skillset_2","title":"Required SkillSet","text":"<ul> <li>Javascript, HTML, CSS</li> <li>Experience using a front end framework such as React, Angular, Vue</li> </ul>"},{"location":"updates/gsoc2023-ideas/#skills-that-you-will-gain_2","title":"Skills that you will gain","text":"<ul> <li>You will learn about how to create a UI feature from design, implementation, and testing</li> </ul>"},{"location":"updates/gsoc2023-ideas/#difficulty-rating_4","title":"Difficulty rating","text":"<p>Hard</p>"}]}