{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mantis \u00b6 Introduction \u00b6 Mantis is a platform to build an ecosystem of realtime stream processing applications. Similar to micro-services deployed in a cloud. Mantis applications (jobs) are deployed on the Mantis platform. The Mantis platform provides the APIs to manage the life cycle of jobs (like deploy, update and terminate), manages the underlying resources by containerizing a common pool of servers and similar to a traditional micro-service cloud allows jobs to discover and communicate with other jobs. By providing stream processing As-a-Service, Mantis allows developers to simply focus on their business logic to build powerful and cost-effective streaming applications. Why we built Mantis? \u00b6 Mantis evolved from the need to get better (faster and in-depth) operational insights in a rapidly growing complex micro-service ecosystem at Netflix. As complexity of a system increases, our comprehension of the system rapidly decreases. In order to counter this complexity we need newer approaches to operational insights. We need to change the way we generate and collect operational data: We should have access to raw events. Applications should be free to publish every single event. If we reduce the granularity at this stage, such as pre-aggregating or sampling, then we're already at a disadvantage when it comes to getting insight since the data in its original form is already lost. We should be able to access this data in realtime. Operational use cases are inherently time sensitive by nature. This becomes increasingly important with scale as the impact becomes much larger in less time. We should be able to ask new questions of this data without necessarily having to add new instrumentation to your applications. It's not possible to know ahead of time every single possible failure mode our systems might encounter despite all the rigor built in to make these systems resilient. When these failures do inevitably occur, it's important that we can derive new insights with this data. We need a new kind of execution environment : Can Process high volume data at low latency Has low Operational burden We need a managed platform where most of the operational tasks are handled automatically on behalf of the user. We don't need the additional overhead of operating our monitoring system. Is Elastic and Resilient We need a highly reliable system that can automatically recover from node failures and be able to scale the resources dynamically based on the data volume. Ecosystem of Streaming services A lot of use-cases often need the same data, Allowing jobs to discover each other and collaborate together by sharing data and results we can build cost-effective jobs that maximise code and data re-use. We should be able to do all of the above in a cost-effective way . As our business critical systems scale, we need to make sure the systems in support of these business critical systems don't end up costing more than the business critical systems themselves. Mantis was built to meet all the above needs. It was designed by Netflix . How can Mantis be used ? \u00b6 Mantis provides a robust, scalable platform that is ideally suited for high volume, low latency use cases like anomaly detection and alerting. Mantis has been in production at Netflix since 2014. It processes trillions of events and peta-bytes of data every day. As a streaming microservices ecosystem, the Mantis platform provides engineers with capabilities to minimize the costs of observing and operating complex distributed systems without compromising on operational insights. Engineers have built cost-efficient real-time applications on top of Mantis to quickly identify issues, trigger alerts, and apply remediations to minimize or completely avoid downtime to the Netflix service. Next Steps \u00b6 To learn more about Mantis, visit the Getting Started guide or check out the Concepts overview or browse through the list of use cases powered by Mantis. To get involved with community, visit the Community page where you can subscribe to one of our mailing lists. For a detailed Programming Guide on writing Mantis Jobs, refer to the Writing Mantis Jobs chapters.","title":"Welcome to Mantis"},{"location":"#mantis","text":"","title":"Mantis"},{"location":"#introduction","text":"Mantis is a platform to build an ecosystem of realtime stream processing applications. Similar to micro-services deployed in a cloud. Mantis applications (jobs) are deployed on the Mantis platform. The Mantis platform provides the APIs to manage the life cycle of jobs (like deploy, update and terminate), manages the underlying resources by containerizing a common pool of servers and similar to a traditional micro-service cloud allows jobs to discover and communicate with other jobs. By providing stream processing As-a-Service, Mantis allows developers to simply focus on their business logic to build powerful and cost-effective streaming applications.","title":"Introduction"},{"location":"#why-we-built-mantis","text":"Mantis evolved from the need to get better (faster and in-depth) operational insights in a rapidly growing complex micro-service ecosystem at Netflix. As complexity of a system increases, our comprehension of the system rapidly decreases. In order to counter this complexity we need newer approaches to operational insights. We need to change the way we generate and collect operational data: We should have access to raw events. Applications should be free to publish every single event. If we reduce the granularity at this stage, such as pre-aggregating or sampling, then we're already at a disadvantage when it comes to getting insight since the data in its original form is already lost. We should be able to access this data in realtime. Operational use cases are inherently time sensitive by nature. This becomes increasingly important with scale as the impact becomes much larger in less time. We should be able to ask new questions of this data without necessarily having to add new instrumentation to your applications. It's not possible to know ahead of time every single possible failure mode our systems might encounter despite all the rigor built in to make these systems resilient. When these failures do inevitably occur, it's important that we can derive new insights with this data. We need a new kind of execution environment : Can Process high volume data at low latency Has low Operational burden We need a managed platform where most of the operational tasks are handled automatically on behalf of the user. We don't need the additional overhead of operating our monitoring system. Is Elastic and Resilient We need a highly reliable system that can automatically recover from node failures and be able to scale the resources dynamically based on the data volume. Ecosystem of Streaming services A lot of use-cases often need the same data, Allowing jobs to discover each other and collaborate together by sharing data and results we can build cost-effective jobs that maximise code and data re-use. We should be able to do all of the above in a cost-effective way . As our business critical systems scale, we need to make sure the systems in support of these business critical systems don't end up costing more than the business critical systems themselves. Mantis was built to meet all the above needs. It was designed by Netflix .","title":"Why we built Mantis?"},{"location":"#how-can-mantis-be-used","text":"Mantis provides a robust, scalable platform that is ideally suited for high volume, low latency use cases like anomaly detection and alerting. Mantis has been in production at Netflix since 2014. It processes trillions of events and peta-bytes of data every day. As a streaming microservices ecosystem, the Mantis platform provides engineers with capabilities to minimize the costs of observing and operating complex distributed systems without compromising on operational insights. Engineers have built cost-efficient real-time applications on top of Mantis to quickly identify issues, trigger alerts, and apply remediations to minimize or completely avoid downtime to the Netflix service.","title":"How can Mantis be used ?"},{"location":"#next-steps","text":"To learn more about Mantis, visit the Getting Started guide or check out the Concepts overview or browse through the list of use cases powered by Mantis. To get involved with community, visit the Community page where you can subscribe to one of our mailing lists. For a detailed Programming Guide on writing Mantis Jobs, refer to the Writing Mantis Jobs chapters.","title":"Next Steps"},{"location":"autoscaling/","text":"Mantis provides autoscaling for the Cluster and for each Job . You can define a policy for your Jobs in which they autoscale their resources based on the dynamic needs resulting from variation in the input data they process. This provides two benefits: You can define Jobs to process data without planning ahead for peak usage. Mantis uses cluster resources optimally without leaving resources idle. Horizontal Scaling \u00b6 Your Mantis Jobs are composed of in part of Processing Stages , with each stage responsible for a different stream processing task. Because different stages may have different computational needs, each stage has its own autoscaling policy. A Processing Stage is further subdivided into workers . A worker is the smallest unit of work that is scheduled. Each worker requests a certain number of CPUs, some amount of memory, and a certain amount of network bandwidth. When a Processing Stage scales, the number of workers in that stage increases or decreases (the resources that Mantis allocates to an individual worker in the stage do not change as a result of scaling). Scaling a Processing Stage Manually \u00b6 You may define a Processing Stage as scalable without defining an autoscaling policy for it. In such a case the stage is considered manually scalable and you can scale it by means of the Mantis UI or the Mantis API . Setting an Autoscaling Policy \u00b6 Warning You should take care that your autoscaling strategies do not contradict each other. For example, if you set a CPU-based strategy and a network-based strategy, one may want to trigger a scale-up and the other a scale-down at the same time. You define the autoscaling policy for a Processing Stage by setting the following parameters: Min and Max number of workers \u2014 This sets how many workers Mantis will guarantee to be working within the Processing Stage at any particular time. Increment and decrement values \u2014 This indicates how many workers are added to or removed from a stage each time the stage autoscales up or down. Cooldown seconds \u2014 This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. Stragtegies \u2014 An autoscaling policy has the following strategy parameters: Type \u2014 CPU, memory, network, or data drop Scale down below percentage \u2014 When the average value for all workers falls below this value, the stage will scale down. This value is calculated as actual usage divided by requested amounts (for data drop, as the number of data items dropped divided by the total number of data items, dropped+processed). Scale up above percentage \u2014 When the average value for all workers rises above this value, the stage will scale up. Rolling count \u2014 This value helps to keep jitter out of the autoscaling process. Instead of scaling immediately the first time values fall outside of the scale-down and scale-up percentage thresholds you define, Mantis will wait until the thresholds are exceeded a certain number of times within a certain window. For example, a rolling count of \u201c6 of 10\u201d means that only if in ten consecutive observations six or more of the observations fall below the scale-down threshold will the stage be scaled down. Note Ideally, there should be zero data drop, so there isn\u2019t an elegant way to express \u201cscale down below percentage\u201d for data drop. Specifying \u201c0%\u201d as the \u201cscale down below percentage\u201d effectively means the data drop percentage never trigger a scale down. For this reason, it is best to use the data drop strategy in conjunction with another strategy that provides the scale-down trigger. The following example shows how you might establish the autoscaling policy for a stage in the Mantis UI: The illustration above shows a stage with an autoscaling policy that specifies a minimum of 5 and a maximum of 20 workers. It uses a single strategy, that of network bandwidth usage. Autoscaling Scenarios \u00b6 There are four varieties of autoscaling scenarios that you should consider for your Mantis Job: The Processing Stage connects to a cold Source , such as a Kafka topic. Autoscaling works well for this type of stage (the initial stage in a Job that connects to the Source). For example, if your stage connects to a Kafka source, a change in the number of workers in the first stage of your Mantis Job causes the Kafka client to redistribute the partitions of the topic among the new number of workers. The Processing Stage connects to a hot source, such as a working server. The Source stage (stage #1) will have to re-partition the Source servers after an autoscale event on the Processing Stage. This is mainly a concern for Source Jobs . Upon receipt of a modified number of workers, each worker re-partitions the current servers into its own index of the new total. This results in a new list of servers to connect to (for both scale up and scale down), some of which may be already connected. Making a new connection to a Source server evicts any old existing connection from the same Job. This guarantees that no duplicate messages are sent to a Mantis Job. (The solution for this scenario is currently in development.) Rewrite this; it\u2019s not very clear. The Processing Stage connects to another Mantis Job. In this case, the initial Processing Stage in a Job that connects to the output of the previous Mantis Job has strong connectivity into the Source Job via the use of Mantis Java client. In suc a case, all workers from this Processing Stage connect to all workers of the source Job\u2019s Sink . Therefore, autoscaling this type of Job works well. The Processing Stage connects to a previous Processing Stage in the same Mantis Job. Each Processing Stage is strongly connected to its previous Processing Stage. Therefore, autoscaling of this type typically works well. However, a Processing Stage following a grouped stage (a Processing Stage that does a group by operation) receives a grouped Observable or MantisGroup . When Mantis scales such a grouped stage, these groups are repartitioned on to the new number of workers. The Processing Stage following such a grouped stage must, therefore, be prepared to potentially receive a different set of groups after a rescale operation. How does a subsequent stage learn that the previous stage has autoscaled? Note that the number of groups resulting from a group by operation is the maximum limit on the number of workers that can be expected to work on such groups (unless a subsequent processing stage subdivides those groups). In the following illustrations, a processing stage that does a group by operation groups the incoming data into three groups, each one of which is handled by a single worker in the subsequent processing stage. When that second stage scales up and adds another worker, that worker remains idle and does not assist in processing data because there are not enough groups to distribute among the larger number of workers. Before autoscaling: After autoscaling: Updating Autoscalable Jobs \u00b6 To upload a Mantis Job when you have new code to push, upload the .jar or .zip artifact file to Mantis, and make any necessary adjustments to its behavior and policies by using the Mantis UI. You can also do this in two ways via the Mantis API: Update the Job Cluster with a new version for its artifact file along with new scheduling information. This updated JAR and scheduling info are available to use with the next Job submission. However, currently-running Jobs continue to run with whatever artifact file they were started with. Quick update the Job Cluster with only a new artifact file version and submit a new Job with it. The new Job is submitted by using the scheduling info from the last Job submitted. The latter of the above two is convenient not only because you provide the minimal information needed to update the Job. But, also, because when it picks up the scheduling info from the last Job submitted, if it is running, the new Job is started with the same number of workers as the last one. That is, if it had scaled up, the new Job starts scaled up as well.","title":"Mantis Job Autoscaling"},{"location":"autoscaling/#horizontal-scaling","text":"Your Mantis Jobs are composed of in part of Processing Stages , with each stage responsible for a different stream processing task. Because different stages may have different computational needs, each stage has its own autoscaling policy. A Processing Stage is further subdivided into workers . A worker is the smallest unit of work that is scheduled. Each worker requests a certain number of CPUs, some amount of memory, and a certain amount of network bandwidth. When a Processing Stage scales, the number of workers in that stage increases or decreases (the resources that Mantis allocates to an individual worker in the stage do not change as a result of scaling).","title":"Horizontal Scaling"},{"location":"autoscaling/#scaling-a-processing-stage-manually","text":"You may define a Processing Stage as scalable without defining an autoscaling policy for it. In such a case the stage is considered manually scalable and you can scale it by means of the Mantis UI or the Mantis API .","title":"Scaling a Processing Stage Manually"},{"location":"autoscaling/#setting-an-autoscaling-policy","text":"Warning You should take care that your autoscaling strategies do not contradict each other. For example, if you set a CPU-based strategy and a network-based strategy, one may want to trigger a scale-up and the other a scale-down at the same time. You define the autoscaling policy for a Processing Stage by setting the following parameters: Min and Max number of workers \u2014 This sets how many workers Mantis will guarantee to be working within the Processing Stage at any particular time. Increment and decrement values \u2014 This indicates how many workers are added to or removed from a stage each time the stage autoscales up or down. Cooldown seconds \u2014 This indicates how many seconds to wait after a scaling operation has been completed before beginning another scaling operation. Stragtegies \u2014 An autoscaling policy has the following strategy parameters: Type \u2014 CPU, memory, network, or data drop Scale down below percentage \u2014 When the average value for all workers falls below this value, the stage will scale down. This value is calculated as actual usage divided by requested amounts (for data drop, as the number of data items dropped divided by the total number of data items, dropped+processed). Scale up above percentage \u2014 When the average value for all workers rises above this value, the stage will scale up. Rolling count \u2014 This value helps to keep jitter out of the autoscaling process. Instead of scaling immediately the first time values fall outside of the scale-down and scale-up percentage thresholds you define, Mantis will wait until the thresholds are exceeded a certain number of times within a certain window. For example, a rolling count of \u201c6 of 10\u201d means that only if in ten consecutive observations six or more of the observations fall below the scale-down threshold will the stage be scaled down. Note Ideally, there should be zero data drop, so there isn\u2019t an elegant way to express \u201cscale down below percentage\u201d for data drop. Specifying \u201c0%\u201d as the \u201cscale down below percentage\u201d effectively means the data drop percentage never trigger a scale down. For this reason, it is best to use the data drop strategy in conjunction with another strategy that provides the scale-down trigger. The following example shows how you might establish the autoscaling policy for a stage in the Mantis UI: The illustration above shows a stage with an autoscaling policy that specifies a minimum of 5 and a maximum of 20 workers. It uses a single strategy, that of network bandwidth usage.","title":"Setting an Autoscaling Policy"},{"location":"autoscaling/#autoscaling-scenarios","text":"There are four varieties of autoscaling scenarios that you should consider for your Mantis Job: The Processing Stage connects to a cold Source , such as a Kafka topic. Autoscaling works well for this type of stage (the initial stage in a Job that connects to the Source). For example, if your stage connects to a Kafka source, a change in the number of workers in the first stage of your Mantis Job causes the Kafka client to redistribute the partitions of the topic among the new number of workers. The Processing Stage connects to a hot source, such as a working server. The Source stage (stage #1) will have to re-partition the Source servers after an autoscale event on the Processing Stage. This is mainly a concern for Source Jobs . Upon receipt of a modified number of workers, each worker re-partitions the current servers into its own index of the new total. This results in a new list of servers to connect to (for both scale up and scale down), some of which may be already connected. Making a new connection to a Source server evicts any old existing connection from the same Job. This guarantees that no duplicate messages are sent to a Mantis Job. (The solution for this scenario is currently in development.) Rewrite this; it\u2019s not very clear. The Processing Stage connects to another Mantis Job. In this case, the initial Processing Stage in a Job that connects to the output of the previous Mantis Job has strong connectivity into the Source Job via the use of Mantis Java client. In suc a case, all workers from this Processing Stage connect to all workers of the source Job\u2019s Sink . Therefore, autoscaling this type of Job works well. The Processing Stage connects to a previous Processing Stage in the same Mantis Job. Each Processing Stage is strongly connected to its previous Processing Stage. Therefore, autoscaling of this type typically works well. However, a Processing Stage following a grouped stage (a Processing Stage that does a group by operation) receives a grouped Observable or MantisGroup . When Mantis scales such a grouped stage, these groups are repartitioned on to the new number of workers. The Processing Stage following such a grouped stage must, therefore, be prepared to potentially receive a different set of groups after a rescale operation. How does a subsequent stage learn that the previous stage has autoscaled? Note that the number of groups resulting from a group by operation is the maximum limit on the number of workers that can be expected to work on such groups (unless a subsequent processing stage subdivides those groups). In the following illustrations, a processing stage that does a group by operation groups the incoming data into three groups, each one of which is handled by a single worker in the subsequent processing stage. When that second stage scales up and adds another worker, that worker remains idle and does not assist in processing data because there are not enough groups to distribute among the larger number of workers. Before autoscaling: After autoscaling:","title":"Autoscaling Scenarios"},{"location":"autoscaling/#updating-autoscalable-jobs","text":"To upload a Mantis Job when you have new code to push, upload the .jar or .zip artifact file to Mantis, and make any necessary adjustments to its behavior and policies by using the Mantis UI. You can also do this in two ways via the Mantis API: Update the Job Cluster with a new version for its artifact file along with new scheduling information. This updated JAR and scheduling info are available to use with the next Job submission. However, currently-running Jobs continue to run with whatever artifact file they were started with. Quick update the Job Cluster with only a new artifact file version and submit a new Job with it. The new Job is submitted by using the scheduling info from the last Job submitted. The latter of the above two is convenient not only because you provide the minimal information needed to update the Job. But, also, because when it picks up the scheduling info from the last Job submitted, if it is running, the new Job is started with the same number of workers as the last one. That is, if it had scaled up, the new Job starts scaled up as well.","title":"Updating Autoscalable Jobs"},{"location":"capacity/","text":"Background \u00b6 Mantis Jobs consist of one or more independent stages . Since stages are independent of one another, they are also sized independently. Stages have one or more workers , which are the fundamental unit of parallelism for a stage. Workers execute individual instances of a stage and are each allocated CPU, memory, disk, and network resources. Workers are also isolated from one another, which means they process events independently of each other using their own dedicated resources. You can horizontally scale your Mantis Jobs by modifying the number of workers of your stages. You can also vertically scale your Mantis Jobs by tuning the number of resources for each worker. The following sections are guidelines for sizing different types of Mantis Jobs. Note You can modify worker resources any time, even after you have already launched the Mantis Job. This is useful in cases where you might want to adjust your Mantis Job to new traffic patterns. You can do this by marking your stage as Stage is Scalable . You can also mark the stage as AutoScale this stage to have Mantis automatically scale the stage. You can horizontally scale your Mantis Jobs without restarting the entire job. However, vertically scaling your jobs requires a new job submission for changes to take effect. Mantis Jobs \u00b6 In addition to scaling the number of workers of a Mantis Job, you should consider tuning your workers, which have two tunable properties: processing resources and processing parallelism. For processing resources , you should generally determine if CPU , memory , or network resources will be a bottleneck in your processing. You should increase: the number of CPUs if your job has CPU-intensive transformations such as serialization and deserialization memory if your job plans to hold many objects or large objects in memory network resources if your job needs high throughput for network I/O such as external API calls For processing parallelism , you must choose between serial or concurrent input processing for each stage. Serial input processing means your Processing Stage will receive and process events within a single thread. With serial input, you lose parallelism but your processing logic becomes straightforward without race conditions. // Specifying serial input for a stage. public class SerialStage implements ScalarComputation < T1 , T2 > { static public ScalarToScalar . Config < T1 , T2 > config () { return new ScalarToScalar . Config < T1 , T2 >(). serialInput (); } } Concurrent input means your Processing Stage will have multiple threads which each receive and process events. With concurrent input enabled, race conditions must be considered because the stage\u2019s threads operate independently from one another and may process events at different speeds. // Specifying concurrent input for a stage. public class SerialStage implements ScalarComputation < T1 , T2 > { static public ScalarToScalar . Config < T1 , T2 > config () { return new ScalarToScalar . Config < T1 , T2 >(). concurrentInput (); } } Note Workers use serial input by default. Kafka Source Jobs \u00b6 Kafka Source Jobs are Mantis Jobs that share the same properties described above, except Kafka Source Jobs use concurrent input processing by default. There are additional job parameters to consider when tuning Kafka Source Jobs: numConsumerInstances and stageConcurrency . The numConsumerInstances property determines how many Kafka consumers will be created for each worker. For example, if you have numConsumerInstances set to 2 and have 5 workers, then you will have 10 Kafka consumers in total for your Mantis Job consuming from a Kafka topic. The stageConcurrency property determines a pool of threads which receive events by the Kafka consumers. You can control your processing parallelism with this property. For example, if you have numConsumerInstances set to 2 and stageConcurrency set to 5 on a worker, then two Kafka consumers will read events from a Kafka topic and asynchronously send them to a pool of 5 processing threads. There are three considerations for using numConsumerInstances and stageConcurrency to tune your Kafka Source Job. First, you can pin numConsumerInstances to 1 and add more workers to load balance Kafka consumers across worker instances. If you find that your workers are under-utilized, you can increase numConsumerInstances for each worker. You can also give each worker more CPU, memory, and network resources and increase numConsumerInstances further so that you have fewer workers doing more work. Lastly, if you find that your Kafka topic\u2019s lag is increasing, then processing might be a bottleneck. In this case, you can increase stageConcurrency to increase processing throughput. Note Adding more workers to scale your Kafka Source Jobs to increase throughput or address lag has a hard upper limit. Ensure that you do not have more Kafka consumers than the number of partitions of the Kafka topic that your job is reading from. Specifically, ensure that: (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions This is because the number of partitions is a Kafka topic\u2019s upper bound for parallelism. If you exceed this number, then you will have idle consumers wasting resources which means adding more workers to your Kafka Source Job will not have any positive effect.","title":"Mantis Job Capacity Planning & Sizing"},{"location":"capacity/#background","text":"Mantis Jobs consist of one or more independent stages . Since stages are independent of one another, they are also sized independently. Stages have one or more workers , which are the fundamental unit of parallelism for a stage. Workers execute individual instances of a stage and are each allocated CPU, memory, disk, and network resources. Workers are also isolated from one another, which means they process events independently of each other using their own dedicated resources. You can horizontally scale your Mantis Jobs by modifying the number of workers of your stages. You can also vertically scale your Mantis Jobs by tuning the number of resources for each worker. The following sections are guidelines for sizing different types of Mantis Jobs. Note You can modify worker resources any time, even after you have already launched the Mantis Job. This is useful in cases where you might want to adjust your Mantis Job to new traffic patterns. You can do this by marking your stage as Stage is Scalable . You can also mark the stage as AutoScale this stage to have Mantis automatically scale the stage. You can horizontally scale your Mantis Jobs without restarting the entire job. However, vertically scaling your jobs requires a new job submission for changes to take effect.","title":"Background"},{"location":"capacity/#mantis-jobs","text":"In addition to scaling the number of workers of a Mantis Job, you should consider tuning your workers, which have two tunable properties: processing resources and processing parallelism. For processing resources , you should generally determine if CPU , memory , or network resources will be a bottleneck in your processing. You should increase: the number of CPUs if your job has CPU-intensive transformations such as serialization and deserialization memory if your job plans to hold many objects or large objects in memory network resources if your job needs high throughput for network I/O such as external API calls For processing parallelism , you must choose between serial or concurrent input processing for each stage. Serial input processing means your Processing Stage will receive and process events within a single thread. With serial input, you lose parallelism but your processing logic becomes straightforward without race conditions. // Specifying serial input for a stage. public class SerialStage implements ScalarComputation < T1 , T2 > { static public ScalarToScalar . Config < T1 , T2 > config () { return new ScalarToScalar . Config < T1 , T2 >(). serialInput (); } } Concurrent input means your Processing Stage will have multiple threads which each receive and process events. With concurrent input enabled, race conditions must be considered because the stage\u2019s threads operate independently from one another and may process events at different speeds. // Specifying concurrent input for a stage. public class SerialStage implements ScalarComputation < T1 , T2 > { static public ScalarToScalar . Config < T1 , T2 > config () { return new ScalarToScalar . Config < T1 , T2 >(). concurrentInput (); } } Note Workers use serial input by default.","title":"Mantis Jobs"},{"location":"capacity/#kafka-source-jobs","text":"Kafka Source Jobs are Mantis Jobs that share the same properties described above, except Kafka Source Jobs use concurrent input processing by default. There are additional job parameters to consider when tuning Kafka Source Jobs: numConsumerInstances and stageConcurrency . The numConsumerInstances property determines how many Kafka consumers will be created for each worker. For example, if you have numConsumerInstances set to 2 and have 5 workers, then you will have 10 Kafka consumers in total for your Mantis Job consuming from a Kafka topic. The stageConcurrency property determines a pool of threads which receive events by the Kafka consumers. You can control your processing parallelism with this property. For example, if you have numConsumerInstances set to 2 and stageConcurrency set to 5 on a worker, then two Kafka consumers will read events from a Kafka topic and asynchronously send them to a pool of 5 processing threads. There are three considerations for using numConsumerInstances and stageConcurrency to tune your Kafka Source Job. First, you can pin numConsumerInstances to 1 and add more workers to load balance Kafka consumers across worker instances. If you find that your workers are under-utilized, you can increase numConsumerInstances for each worker. You can also give each worker more CPU, memory, and network resources and increase numConsumerInstances further so that you have fewer workers doing more work. Lastly, if you find that your Kafka topic\u2019s lag is increasing, then processing might be a bottleneck. In this case, you can increase stageConcurrency to increase processing throughput. Note Adding more workers to scale your Kafka Source Jobs to increase throughput or address lag has a hard upper limit. Ensure that you do not have more Kafka consumers than the number of partitions of the Kafka topic that your job is reading from. Specifically, ensure that: (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions (numConsumerInstances \u00d7 numberOfWorkers) \u2264 numberOfPartitions This is because the number of partitions is a Kafka topic\u2019s upper bound for parallelism. If you exceed this number, then you will have idle consumers wasting resources which means adding more workers to your Kafka Source Job will not have any positive effect.","title":"Kafka Source Jobs"},{"location":"cli/","text":"Introduction \u00b6 You can interact with Mantis Job Clusters by means of a command-line interface (CLI). This page explains how to install and use this interface. Installation \u00b6 You can install the Mantis CLI locally in your development environment by following the instructions on the Mantis CLI Readme . Additionally, you can install the Mantis CLI onto your system as a package by following instructions on the Mantis CLI Releasing section of the readme. Mantis Commands \u00b6 A Mantis command takes the following form: mantis command You can also issue the following command to learn about the various Mantis commands available to you from the command line: mantis --help Or you can issue the following command to learn about the use of a specific Mantis command: mantis command --help Setting Up Your AWS Credentials \u00b6 Mantis can operate natively on different clouds. By default, Mantis provides CLI facilities for operating Mantis in the Amazon AWS environment. Before you can use the Mantis CLI to interact with Mantis Clusters in AWS, you must first establish your AWS credentials with Mantis. To do this, first obtain your AWS access key and secret access key, then issue the following command: mantis aws:configure [ -k | --key= access-key ] [ -s | --secret= secret-access-key ] This command will store your AWS credentials to the same location and with the same file format as the Amazon AWS CLI . Bootstrapping a Mantis Cluster in AWS \u00b6 You can use the Mantis CLI to bootstrap a Mantis Cluster in AWS. This will create the following: AWS key pair Default VPC Security groups Single Zookeeper EC2 instance backed by EBS volume Single Mesos Master EC2 instance backed by EBS volume Single Mesos Agent EC2 instance backed by EBS volume Single Mantis Master EC2 instance backed by EBS volume Single Mantis API EC2 instance backed by EBS volume To do this issue the following command: mantis aws:bootstrap Note Before you issue the mantis aws:bootstrap command you must have first set up your AWS credentials. You can do that with the mantis aws:configure command (see above). By default, the Mantis CLI bootstrap command is interactive and will prompt you for provisioning choices. Additionally, you can pass the following command-line parameters to skip some of the prompts from the mantis aws:bootstrap command: prompt parameter valid inputs select a region [ -r | --region ] region us-east-1 , us-east-2 , us-west-2 confirm -y n/a After you have finished executing the mantis aws:bootstrap command, you can submit Jobs into the Mantis cluster that it creates.","title":"The Mantis CLI"},{"location":"cli/#introduction","text":"You can interact with Mantis Job Clusters by means of a command-line interface (CLI). This page explains how to install and use this interface.","title":"Introduction"},{"location":"cli/#installation","text":"You can install the Mantis CLI locally in your development environment by following the instructions on the Mantis CLI Readme . Additionally, you can install the Mantis CLI onto your system as a package by following instructions on the Mantis CLI Releasing section of the readme.","title":"Installation"},{"location":"cli/#mantis-commands","text":"A Mantis command takes the following form: mantis command You can also issue the following command to learn about the various Mantis commands available to you from the command line: mantis --help Or you can issue the following command to learn about the use of a specific Mantis command: mantis command --help","title":"Mantis Commands"},{"location":"cli/#setting-up-your-aws-credentials","text":"Mantis can operate natively on different clouds. By default, Mantis provides CLI facilities for operating Mantis in the Amazon AWS environment. Before you can use the Mantis CLI to interact with Mantis Clusters in AWS, you must first establish your AWS credentials with Mantis. To do this, first obtain your AWS access key and secret access key, then issue the following command: mantis aws:configure [ -k | --key= access-key ] [ -s | --secret= secret-access-key ] This command will store your AWS credentials to the same location and with the same file format as the Amazon AWS CLI .","title":"Setting Up Your AWS Credentials"},{"location":"cli/#bootstrapping-a-mantis-cluster-in-aws","text":"You can use the Mantis CLI to bootstrap a Mantis Cluster in AWS. This will create the following: AWS key pair Default VPC Security groups Single Zookeeper EC2 instance backed by EBS volume Single Mesos Master EC2 instance backed by EBS volume Single Mesos Agent EC2 instance backed by EBS volume Single Mantis Master EC2 instance backed by EBS volume Single Mantis API EC2 instance backed by EBS volume To do this issue the following command: mantis aws:bootstrap Note Before you issue the mantis aws:bootstrap command you must have first set up your AWS credentials. You can do that with the mantis aws:configure command (see above). By default, the Mantis CLI bootstrap command is interactive and will prompt you for provisioning choices. Additionally, you can pass the following command-line parameters to skip some of the prompts from the mantis aws:bootstrap command: prompt parameter valid inputs select a region [ -r | --region ] region us-east-1 , us-east-2 , us-west-2 confirm -y n/a After you have finished executing the mantis aws:bootstrap command, you can submit Jobs into the Mantis cluster that it creates.","title":"Bootstrapping a Mantis Cluster in AWS"},{"location":"community/","text":"Welcome \u00b6 All of the Mantis components are hosted on Github. Issues are also tracked on Github and we prefer to receive contributions as pull requests. Contributing \u00b6 Mantis components are spread across a few different repos. See the Components section in the README for more information. Before submitting a pull request, make sure to follow the instructions on the repo's CONTRIBUTING.md file. Mailing Lists \u00b6 Mantis has two mailing lists: Developers : mantis-oss-dev@googlegroups.com - used for discussing Mantis development. Users : mantis-oss-users@googlegroups.com - used for community discussions.","title":"Community"},{"location":"community/#welcome","text":"All of the Mantis components are hosted on Github. Issues are also tracked on Github and we prefer to receive contributions as pull requests.","title":"Welcome"},{"location":"community/#contributing","text":"Mantis components are spread across a few different repos. See the Components section in the README for more information. Before submitting a pull request, make sure to follow the instructions on the repo's CONTRIBUTING.md file.","title":"Contributing"},{"location":"community/#mailing-lists","text":"Mantis has two mailing lists: Developers : mantis-oss-dev@googlegroups.com - used for discussing Mantis development. Users : mantis-oss-users@googlegroups.com - used for community discussions.","title":"Mailing Lists"},{"location":"concepts/","text":"Mantis Concepts \u00b6 Mantis provides Stream-processing-As-a-Service. It is a self contained platform that manages all tasks associated with running thousands of stream processing jobs. Take a look at Infrastructure Overview to get an understanding of the physical components of the Mantis Platform. Let us walk through some of the key concepts and terminologies used in Mantis. Mantis Job Cluster \u00b6 A Mantis Job cluster represents metadata (artifact, configuration, resource requirements) associated with a job. A job can be thought of as a running instance of Job Cluster (like a Java Object is an instance of a Java class). A Job cluster can have 0 or more running instances of a job at any given time. Users can control how many jobs can be running at any given time by specifying the SLA for this cluster. E.g SLA of Min 1 / Max 1 means there is exactly one instance of job running at any given time. Users can also setup a Cron spec that can be used to submit jobs periodically. Mantis Jobs \u00b6 At the core of Mantis is the Mantis Job. Stream processing applications in Mantis are called Mantis Jobs. Logically a job represents the business logic to transform a stream of events from one or more sources and generate results. A developer writes a job using Mantis primitives and builds an artifact which is then deployed into the Mantis platform for execution. Each Mantis Job belongs to exactly one Mantis Job Cluster. Lets take a closer look at a Mantis Job. Tasks (Worker) \u00b6 A worker is the smallest unit of execution in Mantis. Physically a worker executes in a resource isolated Mesos container on the Mantis Agent fleet of servers. Each worker is assigned a unique monotonically increasing MantisWorkerIndex and a unique MantisWorkerNumber If a Mantis worker terminates abnormally, Mantis ensures a replacement worker gets launched with the same MantisWorkerIndex and a new MantisWorkerNumber . The resources like CPU, Memory, Network allocated to a worker are configured at Job submit time and cannot be updated later. Each Worker belongs to exactly one Mantis Stage. Stages (Group of workers) \u00b6 A Mantis job is logically divided into one or more stages . A stage is a collection of homogeneous Mantis Workers that perform the same computation. A typical map-reduce style job would be represented by three stages in Mantis (shuffle, window/aggregate and collect) Workers belonging to a stage establish network connections with all workers of the previous stage (if one exists) Workers of the same stage do not connect to each other. The presence of multiple stages imply network hops which allows users to distribute the job logic across several workers allowing for more scalability. Mantis allows each stage to dynamically scale the number of workers in the stage independently with the help of an autoscaling policy. Note Autoscaling is only recommended on stateless stages. For stateful stages the user would need to implement logic to move state to new workers. In cases where the state can be rebuilt rapidly this is not a concern. Mantis Runtime \u00b6 Mantis runtime is execution environment for Mantis Jobs. Broadly speaking it covers all aspects of the running a Mantis Job which includes Operators based Reactive Extensions used by Jobs to implement their business logic. Job topology management which builds and maintains the job execution DAG Exchanging control messages with the Mantis Master including heartbeats among other things. Source Job \u00b6 A source job is a type of Mantis Job that makes data available to other Mantis Jobs via an MQL interface. Downstream jobs connect to the Sink (Server Sent Event) of the Source job with an MQL query which denotes what data the job is interested in. Each event flowing through the Source job is evaluated against these MQL queries. Events matching a particular query are then streamed to the corresponding downstream job. The source jobs have several advantages Pluggable source : By abstracting out where the data is coming from, the source jobs allow the downstream jobs to focus on just their processing logic. The same job can then work with data from different sources by simply connecting to a different source jobs. E.g There can be one source job (say A) backed by a Kafka topic and another backed by S3 (say B) and the downstream job can either connect to A or B based on whether they want to process realtime data or historical data. Data re-use : Often a lot of jobs are interested in data from the same source. Just that they maybe interested in different subsets of this data. Instead of each job having to re-fetch the same data again and again from the same external source, The source jobs fetch the data once and then make it available for any other job to use. Cost Efficiency : A direct consequence of data re-use is fewer resources are required. E.g Let us assume there are 3 jobs interested in processing data from the same Kafka topic. But each is interested in different subsets of this data. Traditionally, each job would have to read the topic in its entirety and then filter out the data they are not interested in. This means 3x fanout on Kafka, and additionally each job now has to have enough resources to process the entire topic. If this topic is high volume then these jobs would have to be sufficiently scaled to keep up. All while throwing away large portions of the data. With a source job the Kafka fan out is just 1 and the three downstream jobs need to be scaled just enough to process their anticipated subset of the stream. Less operational load : Source jobs are stateless and thus good candidates for Autoscaling. With autoscaling enabled the Mantis administrators do not have to worry about right sizing the resources, the job will just adapt its size to meet the demands. Mantis OSS comes with the following source jobs Kafka Source job : Reads data from one or more Kafka topics and makes it available for downstream consumers to query via MQL. Publish Source Job : Works the the mantis-publish library to fetch data on-demand from external applications and make it available to downstream consumers. See the Mantis Publish example to see this in action. Users can also build their own source jobs see the Synthetic Source Job example. Job Chaining \u00b6 One of the unique capabilities of Mantis is the ability for Jobs to communicate with each other to form a kind of streaming microservices architecture. The Source Job -> Downstream job flow is an example of this Job chaining. In this case the source of the downstream job is the output of the upstream Data source job. All a job needs to do to connect to the sink of another job is to include the in-built Job Connnector See the Job Connector Sample to see this in action. These job to job communications happen directly via in memory socket connections with no intermediate disk persistence. If buffering/persistence of results is desired then it is recommended to sink the data into persistence queue like Kafka using the Kafka Connector Job chaining has proven to be extremely useful while operating at scale. It is widely used in the Netflix deployment of Mantis. Mantis Query Language (MQL) \u00b6 MQL is a SQL like language that allows users to work with streaming data without having to write Java code. Example MQL query: select * from defaultStream where status == 500 MQL is used in various parts of Mantis platform including the mantis-publish library, Source Jobs and sometimes directly as a library within a Job that can benefit from the query and aggregation features it brings. Mantis Master \u00b6 The Mantis Master is a leader elected control plane for the Mantis platform. It is responsible for managing the life cycle of Job Clusters, Jobs and workers. It also acts as a Resource scheduler to optimally allocate and schedule resources required by the Jobs. The master stores its meta-data into an external source. The OSS version ships with a sample file based store. For production deployments a highly available store is recommended. The Master is built using Akka principles, where each Job Cluster, Job etc are modelled as Actors. For scheduling of resources Mantis relies on the Mesos Framework The Master registers itself as a Mesos Framework . It receives resource offers from Mesos and uses Fenzo to optimally match tasks to these offers. Mantis API \u00b6 The Mantis API is almost like a traditional API server which proxies request to the Mantis Master. But has additional capabilities like - Allowing users to stream the output of a job via web sockets (/api/v1/jobConnectbyid/jobID API) - Acts as a discovery server for Jobs, allowing consumers to get a stream of scheduling information (like host, port for workers belonging to a job)","title":"Mantis Concepts"},{"location":"concepts/#mantis-concepts","text":"Mantis provides Stream-processing-As-a-Service. It is a self contained platform that manages all tasks associated with running thousands of stream processing jobs. Take a look at Infrastructure Overview to get an understanding of the physical components of the Mantis Platform. Let us walk through some of the key concepts and terminologies used in Mantis.","title":"Mantis Concepts"},{"location":"concepts/#mantis-job-cluster","text":"A Mantis Job cluster represents metadata (artifact, configuration, resource requirements) associated with a job. A job can be thought of as a running instance of Job Cluster (like a Java Object is an instance of a Java class). A Job cluster can have 0 or more running instances of a job at any given time. Users can control how many jobs can be running at any given time by specifying the SLA for this cluster. E.g SLA of Min 1 / Max 1 means there is exactly one instance of job running at any given time. Users can also setup a Cron spec that can be used to submit jobs periodically.","title":"Mantis Job Cluster"},{"location":"concepts/#mantis-jobs","text":"At the core of Mantis is the Mantis Job. Stream processing applications in Mantis are called Mantis Jobs. Logically a job represents the business logic to transform a stream of events from one or more sources and generate results. A developer writes a job using Mantis primitives and builds an artifact which is then deployed into the Mantis platform for execution. Each Mantis Job belongs to exactly one Mantis Job Cluster. Lets take a closer look at a Mantis Job.","title":"Mantis Jobs"},{"location":"concepts/#tasks-worker","text":"A worker is the smallest unit of execution in Mantis. Physically a worker executes in a resource isolated Mesos container on the Mantis Agent fleet of servers. Each worker is assigned a unique monotonically increasing MantisWorkerIndex and a unique MantisWorkerNumber If a Mantis worker terminates abnormally, Mantis ensures a replacement worker gets launched with the same MantisWorkerIndex and a new MantisWorkerNumber . The resources like CPU, Memory, Network allocated to a worker are configured at Job submit time and cannot be updated later. Each Worker belongs to exactly one Mantis Stage.","title":"Tasks (Worker)"},{"location":"concepts/#stages-group-of-workers","text":"A Mantis job is logically divided into one or more stages . A stage is a collection of homogeneous Mantis Workers that perform the same computation. A typical map-reduce style job would be represented by three stages in Mantis (shuffle, window/aggregate and collect) Workers belonging to a stage establish network connections with all workers of the previous stage (if one exists) Workers of the same stage do not connect to each other. The presence of multiple stages imply network hops which allows users to distribute the job logic across several workers allowing for more scalability. Mantis allows each stage to dynamically scale the number of workers in the stage independently with the help of an autoscaling policy. Note Autoscaling is only recommended on stateless stages. For stateful stages the user would need to implement logic to move state to new workers. In cases where the state can be rebuilt rapidly this is not a concern.","title":"Stages (Group of workers)"},{"location":"concepts/#mantis-runtime","text":"Mantis runtime is execution environment for Mantis Jobs. Broadly speaking it covers all aspects of the running a Mantis Job which includes Operators based Reactive Extensions used by Jobs to implement their business logic. Job topology management which builds and maintains the job execution DAG Exchanging control messages with the Mantis Master including heartbeats among other things.","title":"Mantis Runtime"},{"location":"concepts/#source-job","text":"A source job is a type of Mantis Job that makes data available to other Mantis Jobs via an MQL interface. Downstream jobs connect to the Sink (Server Sent Event) of the Source job with an MQL query which denotes what data the job is interested in. Each event flowing through the Source job is evaluated against these MQL queries. Events matching a particular query are then streamed to the corresponding downstream job. The source jobs have several advantages Pluggable source : By abstracting out where the data is coming from, the source jobs allow the downstream jobs to focus on just their processing logic. The same job can then work with data from different sources by simply connecting to a different source jobs. E.g There can be one source job (say A) backed by a Kafka topic and another backed by S3 (say B) and the downstream job can either connect to A or B based on whether they want to process realtime data or historical data. Data re-use : Often a lot of jobs are interested in data from the same source. Just that they maybe interested in different subsets of this data. Instead of each job having to re-fetch the same data again and again from the same external source, The source jobs fetch the data once and then make it available for any other job to use. Cost Efficiency : A direct consequence of data re-use is fewer resources are required. E.g Let us assume there are 3 jobs interested in processing data from the same Kafka topic. But each is interested in different subsets of this data. Traditionally, each job would have to read the topic in its entirety and then filter out the data they are not interested in. This means 3x fanout on Kafka, and additionally each job now has to have enough resources to process the entire topic. If this topic is high volume then these jobs would have to be sufficiently scaled to keep up. All while throwing away large portions of the data. With a source job the Kafka fan out is just 1 and the three downstream jobs need to be scaled just enough to process their anticipated subset of the stream. Less operational load : Source jobs are stateless and thus good candidates for Autoscaling. With autoscaling enabled the Mantis administrators do not have to worry about right sizing the resources, the job will just adapt its size to meet the demands. Mantis OSS comes with the following source jobs Kafka Source job : Reads data from one or more Kafka topics and makes it available for downstream consumers to query via MQL. Publish Source Job : Works the the mantis-publish library to fetch data on-demand from external applications and make it available to downstream consumers. See the Mantis Publish example to see this in action. Users can also build their own source jobs see the Synthetic Source Job example.","title":"Source Job"},{"location":"concepts/#job-chaining","text":"One of the unique capabilities of Mantis is the ability for Jobs to communicate with each other to form a kind of streaming microservices architecture. The Source Job -> Downstream job flow is an example of this Job chaining. In this case the source of the downstream job is the output of the upstream Data source job. All a job needs to do to connect to the sink of another job is to include the in-built Job Connnector See the Job Connector Sample to see this in action. These job to job communications happen directly via in memory socket connections with no intermediate disk persistence. If buffering/persistence of results is desired then it is recommended to sink the data into persistence queue like Kafka using the Kafka Connector Job chaining has proven to be extremely useful while operating at scale. It is widely used in the Netflix deployment of Mantis.","title":"Job Chaining"},{"location":"concepts/#mantis-query-language-mql","text":"MQL is a SQL like language that allows users to work with streaming data without having to write Java code. Example MQL query: select * from defaultStream where status == 500 MQL is used in various parts of Mantis platform including the mantis-publish library, Source Jobs and sometimes directly as a library within a Job that can benefit from the query and aggregation features it brings.","title":"Mantis Query Language (MQL)"},{"location":"concepts/#mantis-master","text":"The Mantis Master is a leader elected control plane for the Mantis platform. It is responsible for managing the life cycle of Job Clusters, Jobs and workers. It also acts as a Resource scheduler to optimally allocate and schedule resources required by the Jobs. The master stores its meta-data into an external source. The OSS version ships with a sample file based store. For production deployments a highly available store is recommended. The Master is built using Akka principles, where each Job Cluster, Job etc are modelled as Actors. For scheduling of resources Mantis relies on the Mesos Framework The Master registers itself as a Mesos Framework . It receives resource offers from Mesos and uses Fenzo to optimally match tasks to these offers.","title":"Mantis Master"},{"location":"concepts/#mantis-api","text":"The Mantis API is almost like a traditional API server which proxies request to the Mantis Master. But has additional capabilities like - Allowing users to stream the output of a job via web sockets (/api/v1/jobConnectbyid/jobID API) - Acts as a discovery server for Jobs, allowing consumers to get a stream of scheduling information (like host, port for workers belonging to a job)","title":"Mantis API"},{"location":"faq/","text":"Is it possible to set a sample threshold value lower than 1 in an MQL query? \u00b6 Yes. There is a sampling parameter called factor , 10000 by default, which determines the effective threshold via the formula \\frac{threshold}{factor} \\frac{threshold}{factor} . If you have reduced threshold as far as you can (to 1 ), you can continue to reduce the effective threshold by increasing the value of factor . See Random Sampling for more details. Is it possible to use the second stage of a three-stage Mantis Job as the source job for another MQL job? \u00b6 Jobs connect to the sink stage of other jobs by default, so while it may be technically possible to rig up a solution where you connect to an intermediate stage instead, this is not supported as an out-of-the-box solution. You may instead want to reorganize your original job, perhaps splitting it into two jobs.","title":"Mantis FAQ"},{"location":"faq/#is-it-possible-to-set-a-sample-threshold-value-lower-than-1-in-an-mql-query","text":"Yes. There is a sampling parameter called factor , 10000 by default, which determines the effective threshold via the formula \\frac{threshold}{factor} \\frac{threshold}{factor} . If you have reduced threshold as far as you can (to 1 ), you can continue to reduce the effective threshold by increasing the value of factor . See Random Sampling for more details.","title":"Is it possible to set a sample threshold value lower than 1 in an MQL query?"},{"location":"faq/#is-it-possible-to-use-the-second-stage-of-a-three-stage-mantis-job-as-the-source-job-for-another-mql-job","text":"Jobs connect to the sink stage of other jobs by default, so while it may be technically possible to rig up a solution where you connect to an intermediate stage instead, this is not supported as an out-of-the-box solution. You may instead want to reorganize your original job, perhaps splitting it into two jobs.","title":"Is it possible to use the second stage of a three-stage Mantis Job as the source job for another MQL job?"},{"location":"glossary/","text":"artifact file Each Mantis Job has an associated artifact file that contains its source code and JSON configuration. autoscaling You can establish an autoscaling policy for each component of your Mantis Job that governs how Mantis adjusts the number of workers assigned to that component as its workload changes. \u21d2 See Mantis Job Autoscaling backpressure Backpressure refers to a set of possible strategies for coping with ReactiveX Observables that produce items more rapidly than their observers consume them. \u21d2 See ReactiveX.io: backpressure operators Binary compression has a particular meaning in the Mantis context... see (Connecting to a Source Job)[/writingjobs/source#connecting-to-a-source-job] Broadcast mode In broadcast mode, each worker of your Mantis Job gets all the data from all workers of the Source Job rather than having that data distributed equally among the workers of your Job. \u21d2 See Source Job Sources: Broadcast Mode Cassandra Apache Cassandra is an open source, distributed database management system. \u21d2 See Apache Cassandra Documentation . Cluster A Job Cluster is a containing entity for Mantis Jobs . It defines metadata and certain service-level agreements . Job Clusters ease job lifecycle management and job revisioning. A Mantis Cluster is a group of cloud container instances that hold your Mantis resources. cold Observables A cold ReactiveX Observable waits until an observer subscribes to it before it begins to emit items. This means the observer is guaranteed to see the whole Observable sequence from the beginning. This is in contrast to a hot Observable, which may begin emitting items as soon as it is created, even before observers have subscribed to it. component A Mantis Job is composed of three types of component: a Source , one or more Processing Stages , and a Sink . Custom source In contrast to a Source Job , which is a built-in variety of Source component designed to pull data from a common sort of data source, a custom source typically accesses data from less-common sources or has unusual delivery guarantee semantics. Executor The stage executor is responsible for loading the bytecode for a Mantis Job and then executing its stages and workers in a coordinated fashion. In the Mesos UI, workers are also referred to as executors. Fenzo Fenzo is a Java library that implements a generic task scheduler for Mesos frameworks. \u21d2 See the Fenzo documentation . grouped data Grouped (or keyed) data is distinguished from scalar data in that each datum is accompanied by a key that indicates what group it belongs to. Grouped data can be processed by a RxJava GroupedObservable or by a MantisGroup . GRPC GRPC is an open-source RPC framework using Protocol Buffers. \u21d2 See GRPC.io hot Observables A hot ReactiveX Observable may begin emitting items as soon as it is created, even before observers have subscribed to it. This means the observer may miss items that were emitted before the observer subscribed. This is in contrast to a cold Observable, which waits until an observer subscribes to it before it begins to emit items. JMC Java Mission Control is a tool from Oracle with which developers can monitor and manage Java applications. \u21d2 See Java Components Job A Mantis Job takes in a stream of data, transforms it by using RxJava operators, and then outputs the results as another stream. It is composed of a Source , one or more Processing Stages , and a Sink . \u21d2 See Writing Mantis Jobs Job Cluster see Cluster Job Master If a job is configured with autoscaling , Mantis will add a Job Master component to it as its initial component. This component will send metrics back to Mantis to help it govern the autoscaling process. Kafka Apache Kafka is a large-scale, distributed streaming platform. \u21d2 See Apache Kafka . keyed data see grouped data Keystone Keystone is Netflix\u2019s data backbone, a stream processing platform that focuses on data analytics. \u21d2 See Keystone Real-time Stream Processing Platform label A label is a text key/value pair that you can add to a Job Cluster or to an individual Job to make it easier to search for or group. Log4j Log4j is a Java-based logging framework. \u21d2 See Apache Log4j Mantis Master The Mantis Master coordinates the execution of Mantis Jobs and starts the services on each Worker . Mesos Apache Mesos is an open-source technique for balancing resources across frameworks in clusters. \u21d2 See Apache Mesos Metadata Mantis inserts metadata into its Job payload. This may include information about where the data came from, for instance. You can define additional metadata to include in the payload when you establish the Job Cluster . meta message A Source Job may occasionally inject meta messages into its data stream that indicate things like data drops. migration strategy define Mantis Publish Mantis Publish (internally at Netflix known as Mantis Realtime Events or MRE) is a library that your application can use to stream events into Mantis while respecting MQL filters. \u21d2 See MQL . MQL You use Mantis Query Language to define filters and other data processing that Mantis applies to a Source data stream at its point of origin, so as to reduce the amount of data going over the wire. \u21d2 See MQL . Observable In ReactiveX an Observable is the method of processing a stream of data in a way that facilitates its transformation and consumption by observers. Observables come in hot and cold varieties. There is also a GroupedObservable that is specialized to grouped data. \u21d2 See ReactiveX.io: Observable . Parameter A Mantis Job may accept parameters that modify its behavior. You can define these in your Job Cluster definition, and set their values on a per-Job basis. Processing Stage A Processing Stage component of a Mantis Job transforms the RxJava Observables it obtains from the Source component. A Job with only one Processing Stage is called a single-stage Job. \u21d2 See The Processing Stage Component property A property is a particular named data value found within events in an event stream. Reactive Streams Reactive Streams is the latest advance of the ReactiveX project. It is an API for manipulating streams of asynchronous data in a non-blocking fashion, with backpressure . \u21d2 See Reactive Streams . ReactiveX ReactiveX is a software technique for transforming, combining, reacting to, and managing streams of data. RxJava is an example of a library that implements this technique. \u21d2 See reactivex.io . RxJava RxJava is the Java implementation of ReactiveX , a software technique for transforming, combining, reacting to, and managing streams of data. \u21d2 See reactivex.io . sampling Sampling is an MQL strategy for mitigating data volume issues. There are two sampling strategies: Random and Sticky. Random sampling uniformly downsamples the source stream to a percentage of its original volume. Sticky sampling selectively samples data from the source stream based on key values. \u21d2 See MQL: Sampling scalar data Scalar data is distinguished from keyed or grouped data in that it is not categorized into groups by key. Scalar data can be processed by an ordinary ReactiveX Observable . Sink The Sink is the final component of a Mantis Job . It takes the Observables that has been transformed by the Processing Stage and outputs it in the form of a new data stream. \u21d2 See The Sink Component SLA A service-level agreement , in the Mantis context, is defined on a per- Cluster basis. You use it to configure how many Jobs in the cluster will be in operation at any time, among other things. Source The Source component of a Mantis Job fetches data from a source outside of Mantis and makes it available to the Processing Stage component in the form of an RxJava Observable . There are two varieties of Source: a Source Job and a custom source . \u21d2 See The Source Component Source Job A Source Job is a Mantis Job that you can use as a Source , which wraps a data source external to Mantis and makes it easier for you to create a job that observes its data. \u21d2 See Mantis Source Jobs server-sent events Server-sent events (SSE) are a way for a browser to receive automatic updates from a server through an HTTP connection. Mantis includes an SSE Sink . \u21d2 See Wikipedia: Server-sent events Transformation A transformation acts on each datum from a stream or Observables of data, changing it in some manner before passing it along as a new stream or Observable. Transformations may change data between scalar and grouped forms. transient jobs A transient (or ephemeral) Mantis Job is automatically killed by Mantis after a certain amount of time has passed since the last subscriber to the job disconnects. WebSocket WebSocket is a two-way, interactive communication channel that works over HTTP. In the Mantis context, it is an alternative to SSE . \u21d2 See WebSocket.org . Worker A worker is the smallest unit of work that is scheduled within a Mantis component . You can configure how many resources Mantis allocates to each worker, and Mantis will adjust the number of workers your Mantis component needs based on its autoscaling policy. Zookeeper Apache Zookeeper is an open-source server that maintains configuration information and other services required by distributed applications. \u21d2 See Apache ZooKeeper","title":"Glossary"},{"location":"infrastructure/","text":"Mantis Infrastructure Overview \u00b6 At a high level the Mantis Architecture consists of the Control plane (Mantis Master) and the Mantis runtime. The Master is responsible for managing the life cycle of Jobs, including creation, scheduling and deletion of jobs. The Mantis runtime is responsible for the actual execution of Mantis jobs. Major components \u00b6 Here is a brief description of the major components of the Mantis Architecture Mantis control plane (Master) \u00b6 The Control plane plane is responsible for managing the lifecycle of Mantis Jobs. Metadata related to the jobs is stored in the Metadata store. The Open source version of Mantis ships with a simple file based store. For production use of a highly available store is recommended. The default resource manager for scheduling Job tasks is Mesos. The control plane registers itself as a framework into Mesos. The control plane uses Fenzo to match the job to the available Mesos offers in an optimal manner. Mantis Runtime \u00b6 Mantis jobs are executed on Mesos agent(s) as one or more tasks (Job Workers) based on the job topology. The Mantis runtime is responsible for the actual execution of the job. This includes bootstrapping user code, exchanging control messages with the Master and establishing/maintaining upstream and downstream network connections per the Job execution DAG. One of the unique features of Mantis is that it allows Mantis Jobs to discover other Mantis Jobs and stream results of one job as input to another. The Mantis runtime does the work of discovering, connecting and maintaining these intra-job network flows. Mantis API \u00b6 Mantis API provides an easy REST interface for interacting with the Mantis system. It allows users to create/submit/kill Mantis Jobs and Mantis Job Clusters. Additionally, it also allows users to stream the output of any running jobs via WebSocket or SSE. Zookeeper \u00b6 Mantis usage of Zookeeper is fairly minimum. The zookeeper is used primarily for Master leader election both by the Mantis Control Plane and the Mesos Master. External Dependencies \u00b6 Zookeeper : Used for leader election of Mantis and Mesos Masters. RxJava 1.x : Provides the fluent, functional programming model for Mantis Jobs. Mesos 1.x : Powers the underlying resource management system. RxNetty 0.4 : Used for the intra-task network communication.","title":"Infrastructure Overview"},{"location":"infrastructure/#mantis-infrastructure-overview","text":"At a high level the Mantis Architecture consists of the Control plane (Mantis Master) and the Mantis runtime. The Master is responsible for managing the life cycle of Jobs, including creation, scheduling and deletion of jobs. The Mantis runtime is responsible for the actual execution of Mantis jobs.","title":"Mantis Infrastructure Overview"},{"location":"infrastructure/#major-components","text":"Here is a brief description of the major components of the Mantis Architecture","title":"Major components"},{"location":"infrastructure/#mantis-control-plane-master","text":"The Control plane plane is responsible for managing the lifecycle of Mantis Jobs. Metadata related to the jobs is stored in the Metadata store. The Open source version of Mantis ships with a simple file based store. For production use of a highly available store is recommended. The default resource manager for scheduling Job tasks is Mesos. The control plane registers itself as a framework into Mesos. The control plane uses Fenzo to match the job to the available Mesos offers in an optimal manner.","title":"Mantis control plane (Master)"},{"location":"infrastructure/#mantis-runtime","text":"Mantis jobs are executed on Mesos agent(s) as one or more tasks (Job Workers) based on the job topology. The Mantis runtime is responsible for the actual execution of the job. This includes bootstrapping user code, exchanging control messages with the Master and establishing/maintaining upstream and downstream network connections per the Job execution DAG. One of the unique features of Mantis is that it allows Mantis Jobs to discover other Mantis Jobs and stream results of one job as input to another. The Mantis runtime does the work of discovering, connecting and maintaining these intra-job network flows.","title":"Mantis Runtime"},{"location":"infrastructure/#mantis-api","text":"Mantis API provides an easy REST interface for interacting with the Mantis system. It allows users to create/submit/kill Mantis Jobs and Mantis Job Clusters. Additionally, it also allows users to stream the output of any running jobs via WebSocket or SSE.","title":"Mantis API"},{"location":"infrastructure/#zookeeper","text":"Mantis usage of Zookeeper is fairly minimum. The zookeeper is used primarily for Master leader election both by the Mantis Control Plane and the Mesos Master.","title":"Zookeeper"},{"location":"infrastructure/#external-dependencies","text":"Zookeeper : Used for leader election of Mantis and Mesos Masters. RxJava 1.x : Provides the fluent, functional programming model for Mantis Jobs. Mesos 1.x : Powers the underlying resource management system. RxNetty 0.4 : Used for the intra-task network communication.","title":"External Dependencies"},{"location":"usecases/","text":"Mantis use-cases \u00b6 Here is a non-comprehensive list of use-cases powered by Mantis to give you an idea of the type of applications that can be built on Mantis. Realtime monitoring of Netflix streaming health \u00b6 At Netflix Stream Starts per Second (SPS) is a key metric used to track the health of the Netflix streaming service. Streaming starts per second tracks the number of people successfully hitting play on their streaming devices. Any abnormal change in the trend of this metric signifies a negative impact on user's viewing experience. This Mantis application monitors the SPS trend by processing data sourced directly from thousands of Netflix servers (using the mantis-publish library) in realtime. Using a version of Double Exponential Smoothing (DES) it can detect abnormal deviations in seconds and alerts key teams at Netflix. Contextual Alerting \u00b6 As Netflix has grown over the years so has the number of microservices. Getting alerted for outages for your own service is often not sufficient to root-cause issues. Engineers need to understand what is happening with downstream and upstream services as well to be able to quickly narrow down the root of the issue. The Contextual alerting application analyzes millions of interactions between dozens of Netflix microservices in realtime to identify anomalies and provide operators with rich and relevant context. The realtime nature of these Mantis-backed aggregations allows the Mean-Time-To-Detect to be cut down from tens of minutes to a few seconds. Given the scale of Netflix this makes a huge impact. Raven \u00b6 In large distributed systems, often there are cases where a user reports a problem but the overall health of the system is green. In such cases there is a need to explore events associated with the user/device/service in realtime to find a smoking gun. With the potential of a user request landing across thousands of servers it is often a laborious task to find the right servers and inspect their logs. The Raven applications makes this task trivial, The raven jobs work with the mantis-publish library to look for events matching a certain criterion (user-id/device-id etc) right at the server and stream matching results in realtime. It provides an intuitive UI that allows SREs to construct and submit simple MQL queries. Cassandra and Elastic Search Health Monitoring \u00b6 Netflix maintains hundreds of Cassandra and Elastic Search clusters. These clusters are critical for the day to day operation of Netflix. The Cassandra and Elastic Search health application analyzes rich operational events sent by the Priam side car in realtime to generate a holistic picture of the health of every Cassandra cluster at Netflix. Since this system has gone into operation the number of false pages has dropped down significantly. Alerting on Log \u00b6 The Alerting on Logs application allows users to create alerts which page when a certain pattern is detected within the application logs. This application analyzes logs from thousands of servers in realtime. Chaos Experimentation monitoring \u00b6 Chaos Testing is one of the pillars of resilience at Netflix. Dozens of chaos experiments are run daily to test the resilience of variety of applications. The Chaos experimentation application tracks user experience by analyzing client and server side events during a Chaos exercise in realtime and triggers an abort of the chaos exercise in case of an adverse impact. Realtime Personally Identifiable Information (PII) data detection \u00b6 With trillions of events flowing through Netflix data systems daily it is critical to ensure no sensitive data is accidentally passed along. This application samples data across all streaming sources and applies custom pattern detection algorithms to identify presence of such data.","title":"Use cases"},{"location":"usecases/#mantis-use-cases","text":"Here is a non-comprehensive list of use-cases powered by Mantis to give you an idea of the type of applications that can be built on Mantis.","title":"Mantis use-cases"},{"location":"usecases/#realtime-monitoring-of-netflix-streaming-health","text":"At Netflix Stream Starts per Second (SPS) is a key metric used to track the health of the Netflix streaming service. Streaming starts per second tracks the number of people successfully hitting play on their streaming devices. Any abnormal change in the trend of this metric signifies a negative impact on user's viewing experience. This Mantis application monitors the SPS trend by processing data sourced directly from thousands of Netflix servers (using the mantis-publish library) in realtime. Using a version of Double Exponential Smoothing (DES) it can detect abnormal deviations in seconds and alerts key teams at Netflix.","title":"Realtime monitoring of Netflix streaming health"},{"location":"usecases/#contextual-alerting","text":"As Netflix has grown over the years so has the number of microservices. Getting alerted for outages for your own service is often not sufficient to root-cause issues. Engineers need to understand what is happening with downstream and upstream services as well to be able to quickly narrow down the root of the issue. The Contextual alerting application analyzes millions of interactions between dozens of Netflix microservices in realtime to identify anomalies and provide operators with rich and relevant context. The realtime nature of these Mantis-backed aggregations allows the Mean-Time-To-Detect to be cut down from tens of minutes to a few seconds. Given the scale of Netflix this makes a huge impact.","title":"Contextual Alerting"},{"location":"usecases/#raven","text":"In large distributed systems, often there are cases where a user reports a problem but the overall health of the system is green. In such cases there is a need to explore events associated with the user/device/service in realtime to find a smoking gun. With the potential of a user request landing across thousands of servers it is often a laborious task to find the right servers and inspect their logs. The Raven applications makes this task trivial, The raven jobs work with the mantis-publish library to look for events matching a certain criterion (user-id/device-id etc) right at the server and stream matching results in realtime. It provides an intuitive UI that allows SREs to construct and submit simple MQL queries.","title":"Raven"},{"location":"usecases/#cassandra-and-elastic-search-health-monitoring","text":"Netflix maintains hundreds of Cassandra and Elastic Search clusters. These clusters are critical for the day to day operation of Netflix. The Cassandra and Elastic Search health application analyzes rich operational events sent by the Priam side car in realtime to generate a holistic picture of the health of every Cassandra cluster at Netflix. Since this system has gone into operation the number of false pages has dropped down significantly.","title":"Cassandra and Elastic Search Health Monitoring"},{"location":"usecases/#alerting-on-log","text":"The Alerting on Logs application allows users to create alerts which page when a certain pattern is detected within the application logs. This application analyzes logs from thousands of servers in realtime.","title":"Alerting on Log"},{"location":"usecases/#chaos-experimentation-monitoring","text":"Chaos Testing is one of the pillars of resilience at Netflix. Dozens of chaos experiments are run daily to test the resilience of variety of applications. The Chaos experimentation application tracks user experience by analyzing client and server side events during a Chaos exercise in realtime and triggers an abort of the chaos exercise in case of an adverse impact.","title":"Chaos Experimentation monitoring"},{"location":"usecases/#realtime-personally-identifiable-information-pii-data-detection","text":"With trillions of events flowing through Netflix data systems daily it is critical to ensure no sensitive data is accidentally passed along. This application samples data across all streaming sources and applies custom pattern detection algorithms to identify presence of such data.","title":"Realtime Personally Identifiable Information (PII) data detection"},{"location":"MQL/","text":"Mantis Query Language (MQL) is a dialect of SQL implemented as an abstraction over RxJava Observables . The purpose of MQL is to make it easy for Mantis users to query, transform, and analyze data flowing through Mantis. MQL maintains high fidelity to SQL syntax while adding capabilities for dealing with JSON structure, as well as a rich set of aggregates for answering analytical queries about the data in question. MQL currently supports the following SQL clauses: SELECT Aggregates ( Min , Max , Average , Count , Sum , Percentiles , List ) FROM WHERE GROUP BY ORDER BY LIMIT WINDOW HAVING JOIN SAMPLE ANOMALY Getting Started \u00b6 To use MQL, include the mql-jvm library in your dependencies. Refer to the example below for a minimalist getting started guide. Imagine we have a source of Map<String, Object> representing your data (recall that Mantis events can be easily parsed as such). Executing a query against that Observable is only a matter of putting that Observable into a Map<String, Observable> to represent the context and calling evalMql against said context. The result will be an Observable that represents the results of the query: package my.package ; import java.util.HashMap ; class MqlExample { public static void main ( String [] args ) { // Create a test observable source of x, y coordinates. Observable < HashMap < String , Object >> source = Observable . interval ( 100 , TimeUnit . MILLISECONDS ). map ( x -> { HashMap < String , Object > d = new HashMap <>(); d . put ( \"x\" , x ); d . put ( \"y\" , x ); }); HashMap < String , Observable < HashMap < String , Object >> context = new HashMap <>(); context . put ( \"observations\" , source ); // You don't have to block, and shouldn't. It is just to keep the example running. io . mantisrx . mql . Core . evalMql ( \"select y from observations where x > 50 OR y == 10\" , context ). toBlocking (). forEach ( System . out :: println ); }","title":"Introduction to MQL"},{"location":"MQL/#getting-started","text":"To use MQL, include the mql-jvm library in your dependencies. Refer to the example below for a minimalist getting started guide. Imagine we have a source of Map<String, Object> representing your data (recall that Mantis events can be easily parsed as such). Executing a query against that Observable is only a matter of putting that Observable into a Map<String, Observable> to represent the context and calling evalMql against said context. The result will be an Observable that represents the results of the query: package my.package ; import java.util.HashMap ; class MqlExample { public static void main ( String [] args ) { // Create a test observable source of x, y coordinates. Observable < HashMap < String , Object >> source = Observable . interval ( 100 , TimeUnit . MILLISECONDS ). map ( x -> { HashMap < String , Object > d = new HashMap <>(); d . put ( \"x\" , x ); d . put ( \"y\" , x ); }); HashMap < String , Observable < HashMap < String , Object >> context = new HashMap <>(); context . put ( \"observations\" , source ); // You don't have to block, and shouldn't. It is just to keep the example running. io . mantisrx . mql . Core . evalMql ( \"select y from observations where x > 50 OR y == 10\" , context ). toBlocking (). forEach ( System . out :: println ); }","title":"Getting Started"},{"location":"MQL/aggregation-scaling-limits/","text":"MQL Queries that implement aggregates ( COUNT , SUM , etc.) , as well as those that use windowing (like window or group by ), must necessarily maintain state. The Mantis Publish library) and the Source Jobs do not perform MQL aggregation on the server side; consequently this must take place on the client side. This limits the blast radius if a client runs a long window or runs an aggregate against a large amount of data. However this creates potential scaling problems on the client side. This page will help you address such problems. Details \u00b6 There are two primary technical concerns in these cases: Windowing must necessarily buffer the output until the end of the window. group by cannot run multithreaded as RxJava does not guarantee pairs with the same key end up on the same thread. In the first case, as a client you\u2019re simply facing a memory issue as you must have enough memory to buffer until the end of each window. However usually when you window, you are also performing some form of aggregate against the window (otherwise there is little point in windowing rather than just emitting the data immediately). The second case requires a single thread in order to guarantee that aggregate calculations against groups will be performed together. A naive solution would be to always run MQL on a single thread. This solves the problem but is the least-scalable solution and also impacts queries do not require such restrictive behavior. Solutions \u00b6 There are a number of possible approaches you can take to address this problem. These primarily involve a tradeoff between engineering effort and effectiveness. Solution A (Implemented) \u00b6 The MQL library includes isAggregate() , which is a function of String \u21d2 Boolean that indicates whether or not a given query requires an aggregate operation. This allows you to test a query to determine if if requires single-threading and handle it appropriately if so. This represents a low-effort / medium-effectiveness tradeoff; only queries that require the serialized behavior will suffer the performance penalty. Solution B (TBD?) \u00b6 A more scalable solution would be to enable MQL to emit additional formats. An example of this would be a Mantis topology backend that could emit multi- stage Mantis Jobs that implement the desired operation in a scalable fashion. A group-by stage that distributes values to downstream workers using consistent hashing would be able to scale horizontally based on the number of groups (still vertically for window size). This solution requires significantly more engineering effort, but represents a very large increase in scalability for these queries. It would be a possibly-fruitful road for the Mantis open source software project to travel.","title":"Scaling Limits"},{"location":"MQL/aggregation-scaling-limits/#details","text":"There are two primary technical concerns in these cases: Windowing must necessarily buffer the output until the end of the window. group by cannot run multithreaded as RxJava does not guarantee pairs with the same key end up on the same thread. In the first case, as a client you\u2019re simply facing a memory issue as you must have enough memory to buffer until the end of each window. However usually when you window, you are also performing some form of aggregate against the window (otherwise there is little point in windowing rather than just emitting the data immediately). The second case requires a single thread in order to guarantee that aggregate calculations against groups will be performed together. A naive solution would be to always run MQL on a single thread. This solves the problem but is the least-scalable solution and also impacts queries do not require such restrictive behavior.","title":"Details"},{"location":"MQL/aggregation-scaling-limits/#solutions","text":"There are a number of possible approaches you can take to address this problem. These primarily involve a tradeoff between engineering effort and effectiveness.","title":"Solutions"},{"location":"MQL/aggregation-scaling-limits/#solution-a-implemented","text":"The MQL library includes isAggregate() , which is a function of String \u21d2 Boolean that indicates whether or not a given query requires an aggregate operation. This allows you to test a query to determine if if requires single-threading and handle it appropriately if so. This represents a low-effort / medium-effectiveness tradeoff; only queries that require the serialized behavior will suffer the performance penalty.","title":"Solution A (Implemented)"},{"location":"MQL/aggregation-scaling-limits/#solution-b-tbd","text":"A more scalable solution would be to enable MQL to emit additional formats. An example of this would be a Mantis topology backend that could emit multi- stage Mantis Jobs that implement the desired operation in a scalable fashion. A group-by stage that distributes values to downstream workers using consistent hashing would be able to scale horizontally based on the number of groups (still vertically for window size). This solution requires significantly more engineering effort, but represents a very large increase in scalability for these queries. It would be a possibly-fruitful road for the Mantis open source software project to travel.","title":"Solution B (TBD?)"},{"location":"MQL/operators/","text":"This page introduces the MQL operators, including a syntax summary, usage examples, and a brief description of each operator. For the most part these operations use their RxJava analogs: MQL operator RxJava operator select map window window where filter group by groupBy order by n/a limit take For this reason you may find the ReactiveX documentation useful as well, though it is a goal of MQL to provide you with a layer of abstraction such that you will not need to use or think about ReactiveX when writing queries. MQL attempts to stay true to the SQL syntax in order to reduce friction for new users writing queries and to allow them to leverage their experiences with SQL. An essential concept in MQL is the \u201c property ,\u201d which can take on one of several forms: property.name.here e[\"this\"][\"accesses\"][\"nested\"][\"properties\"] 15 \"string literal\" Most of the MQL operators use these forms to refer to properties within the event stream, as well as string and numeric literals. The last detail of note concerns unbounded vs. discrete streams. ReactiveX Observables that are not expected to close are an unbounded stream and must be bounded/discretized in order for certain operators to function with them. The window operator is useful for partitioning unbounded streams into discrete bounded streams for operators such as aggregate, group by, or order by. select \u00b6 Syntax: select property from source Examples: \"select * from servo\" \"select nf.node from servo\" \"select e[\"tags\"][\"nf.cluster\"] from servo\" The select operator allows you to project data into a new object by specifying which properties should be carried forward into the output. Properties will bear the same name as was used to select them. In the case of numbers and string literals their value will also be their name. In the case of nested properties the result will be a top-level object joined with dots. For example, the following select example\u2026 \"select \"string literal\", 45, e[\"tags\"][\"cluster\"], nf.node from servo\" \u2026would result in an object like: { \"string literal\" : \"string literal\" , 45: 45, \"tags.cluster\" : \"mantisagent\" , \"nf.node\" : \"i-123456\" } Warning Be careful to avoid collisions between the top-level objects with dotted names and the nested objects that result in top-level properties with dotted names. Aggregates \u00b6 Syntax: \"select aggregate ( property ) from source \" Examples: \"select count(e[\"node\"]) from servo window 60\" \"select count(e[\"node\"]) from servo window 60 where e[\"metrics\"][\"latency\"] > 350\" \"select average(e[\"metrics\"][\"latency\"]), e[\"node\"] from servo window 10\" Supported Aggregates: Min Max Average Count Sum Aggregates add analysis capabilities to MQL in order to allow you to answer interesting questions about data in real-time. They can be intermixed with regular properties in order to select properties and compute information about those properties, such as the last example above which computes average latency on a per-node basis in 10 second windows. Note Aggregates require that the stream on which they operate be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. You can use the distinct operator on a property to operate only on unique items within the window. This is particularly useful with the count aggregate if you want to count the number of items with some distinct property value in that window. For example: \"select count(distinct esn) from stream window 10 1\" from \u00b6 Syntax: \"select something from source \" Example: \"select * from servo\" The from clause indicates to MQL from which Observable it should draw data. This requires some explanation, as it bears different meaning in different contexts. Directly against queryable sources the from clause refers to the source Observable no matter which name is given. The operator is in fact optional, and the name of the source is arbitrary in this context, and the clause will be inserted for you if you omit it. When you use MQL as a library, the source corresponds with the names in the context map parameter. The second parameter to eval-mql() is a Map<String, Observable<T>> and the from clause will attempt to fetch the Observable from this Map . window \u00b6 Syntax: \"WINDOW integer \" Examples: \"select node, latency from servo window 10\" \"select MAX(latency) from servo window 60\" The window clause divides an otherwise unbounded stream of data into discrete time-bounded streams. The integer parameter is the number of seconds over which to perform this bounding. For example \"select * from observable window 10\" will produce 10-second windows of data. This discretization of streams is important for use with aggregate operations as well as group-by and order-by clauses which cannot be executed on, and will hang on, unbounded streams. where \u00b6 Syntax: \"select property from source where predicate \" Examples: \"select * from servo where node == \"i-123456\" AND e[\"metrics\"][\"latency\"] > 350\" \"select * from servo where (node == \"i-123456\" AND e[\"metrics\"][\"latency\"] > 350) OR node == \"1-abcdef\"\" \"select * from servo where node ==~ /i-123/\" The where clause filters any events out of the stream which do not match a given predictate. Predicates support AND and OR operations. Binary operators supported are = , == , <> , != , < , <= , > , >= , ==~ . The first two above are both equality, and either of the next two represent not-equal. You can use the last of those operators, ==~ , with a regular expression as in: \"where property ==~ / regex /\" (any Java regular expression will suffice). group by \u00b6 Syntax: \"GROUP BY property \" Examples: \"select node, latency from servo where latency > 300.0 group by node\" \"select MAX(latency), e[\"node\"] from servo group by node\" group by groups values in the output according to some property. This is particularly useful in conjunction with aggregate operators in which one can compute aggregate values over a group. For example, the following query calculates the maximum latency observed for each node in 60-second windows: \"select MAX(latency), e[\"node\"] from servo window 60 group by node\" Note The group by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. order by \u00b6 Syntax: \"ORDER BY property \" Example: \"select node, latency from servo group by node order by latency\" The order by operator orders the results in the inner-most Observable by the specified property. For example, the query \"select * from observable window 5 group by nf.node order by latency\" would produce an Observable of Observables (windows) of Observables (groups). The events within the groups would be ordered by their latency property. Note The order by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. limit \u00b6 Syntax: \"LIMIT integer \" Examples: \"select * from servo limit 10\" \"select AVERAGE(latency) from servo window 60 limit 10\" The limit operator takes as a parameter a single integer and bounds the number of results to \u2264 integer . Note Limit does not discretize the stream for earlier operators such as group by , order by , aggregates.","title":"Operators"},{"location":"MQL/operators/#select","text":"Syntax: select property from source Examples: \"select * from servo\" \"select nf.node from servo\" \"select e[\"tags\"][\"nf.cluster\"] from servo\" The select operator allows you to project data into a new object by specifying which properties should be carried forward into the output. Properties will bear the same name as was used to select them. In the case of numbers and string literals their value will also be their name. In the case of nested properties the result will be a top-level object joined with dots. For example, the following select example\u2026 \"select \"string literal\", 45, e[\"tags\"][\"cluster\"], nf.node from servo\" \u2026would result in an object like: { \"string literal\" : \"string literal\" , 45: 45, \"tags.cluster\" : \"mantisagent\" , \"nf.node\" : \"i-123456\" } Warning Be careful to avoid collisions between the top-level objects with dotted names and the nested objects that result in top-level properties with dotted names.","title":"select"},{"location":"MQL/operators/#aggregates","text":"Syntax: \"select aggregate ( property ) from source \" Examples: \"select count(e[\"node\"]) from servo window 60\" \"select count(e[\"node\"]) from servo window 60 where e[\"metrics\"][\"latency\"] > 350\" \"select average(e[\"metrics\"][\"latency\"]), e[\"node\"] from servo window 10\" Supported Aggregates: Min Max Average Count Sum Aggregates add analysis capabilities to MQL in order to allow you to answer interesting questions about data in real-time. They can be intermixed with regular properties in order to select properties and compute information about those properties, such as the last example above which computes average latency on a per-node basis in 10 second windows. Note Aggregates require that the stream on which they operate be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream. You can use the distinct operator on a property to operate only on unique items within the window. This is particularly useful with the count aggregate if you want to count the number of items with some distinct property value in that window. For example: \"select count(distinct esn) from stream window 10 1\"","title":"Aggregates"},{"location":"MQL/operators/#from","text":"Syntax: \"select something from source \" Example: \"select * from servo\" The from clause indicates to MQL from which Observable it should draw data. This requires some explanation, as it bears different meaning in different contexts. Directly against queryable sources the from clause refers to the source Observable no matter which name is given. The operator is in fact optional, and the name of the source is arbitrary in this context, and the clause will be inserted for you if you omit it. When you use MQL as a library, the source corresponds with the names in the context map parameter. The second parameter to eval-mql() is a Map<String, Observable<T>> and the from clause will attempt to fetch the Observable from this Map .","title":"from"},{"location":"MQL/operators/#window","text":"Syntax: \"WINDOW integer \" Examples: \"select node, latency from servo window 10\" \"select MAX(latency) from servo window 60\" The window clause divides an otherwise unbounded stream of data into discrete time-bounded streams. The integer parameter is the number of seconds over which to perform this bounding. For example \"select * from observable window 10\" will produce 10-second windows of data. This discretization of streams is important for use with aggregate operations as well as group-by and order-by clauses which cannot be executed on, and will hang on, unbounded streams.","title":"window"},{"location":"MQL/operators/#where","text":"Syntax: \"select property from source where predicate \" Examples: \"select * from servo where node == \"i-123456\" AND e[\"metrics\"][\"latency\"] > 350\" \"select * from servo where (node == \"i-123456\" AND e[\"metrics\"][\"latency\"] > 350) OR node == \"1-abcdef\"\" \"select * from servo where node ==~ /i-123/\" The where clause filters any events out of the stream which do not match a given predictate. Predicates support AND and OR operations. Binary operators supported are = , == , <> , != , < , <= , > , >= , ==~ . The first two above are both equality, and either of the next two represent not-equal. You can use the last of those operators, ==~ , with a regular expression as in: \"where property ==~ / regex /\" (any Java regular expression will suffice).","title":"where"},{"location":"MQL/operators/#group-by","text":"Syntax: \"GROUP BY property \" Examples: \"select node, latency from servo where latency > 300.0 group by node\" \"select MAX(latency), e[\"node\"] from servo group by node\" group by groups values in the output according to some property. This is particularly useful in conjunction with aggregate operators in which one can compute aggregate values over a group. For example, the following query calculates the maximum latency observed for each node in 60-second windows: \"select MAX(latency), e[\"node\"] from servo window 60 group by node\" Note The group by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream.","title":"group by"},{"location":"MQL/operators/#order-by","text":"Syntax: \"ORDER BY property \" Example: \"select node, latency from servo group by node order by latency\" The order by operator orders the results in the inner-most Observable by the specified property. For example, the query \"select * from observable window 5 group by nf.node order by latency\" would produce an Observable of Observables (windows) of Observables (groups). The events within the groups would be ordered by their latency property. Note The order by clause requires that the stream on which it operates be discretized. You can ensure this either by feeding MQL a cold Observable in its context or by using the window operator on an unbounded stream.","title":"order by"},{"location":"MQL/operators/#limit","text":"Syntax: \"LIMIT integer \" Examples: \"select * from servo limit 10\" \"select AVERAGE(latency) from servo window 60 limit 10\" The limit operator takes as a parameter a single integer and bounds the number of results to \u2264 integer . Note Limit does not discretize the stream for earlier operators such as group by , order by , aggregates.","title":"limit"},{"location":"MQL/sampling/","text":"Sampling in MQL mitigates data volume issues. There are two sampling strategies \u2014 Random and Sticky: Random Sampling \u00b6 Random sampling uniformly downsamples the stream to a percentage of its original volume. You establish random sampling through a sampling clause like the following: select * from stream SAMPLE {'strategy': 'RANDOM', 'threshold': 200, 'factor': 10000} For each item in the stream, a numeric hash is generated. That hash is modded by factor to produce a result between 0 (inclusive) and factor (exclusive). If that result is less than threshold the item will be sampled, otherwise it will be ignored. You can determine the sampling percentage by remembering that \\frac{threshold}{factor} \\frac{threshold}{factor} values will be sampled \u2014 2% in the example above with a threshold of 200 and factor of 10000. You can also set a salt value, which will change the calculation of the hash. This is helpful if you want to sample over the same set of values but retrieve a different sample of those values. For example: select * from stream SAMPLE {'strategy': 'RANDOM', 'threshold': 200, 'factor': 10000, 'salt': 123} Sticky Sampling \u00b6 Sticky sampling \u201csticks\u201d to certain values for the provided keys. That is if you are sampling on \u201c zipcode \u201d (as in the following example) and you observe a specific zipcode in the stream, you will observe all events which contain that specific zipcode . Sticky sampling can be achieved with a query like such: select * from stream SAMPLE {'strategy':'STICKY', 'keys':['zipcode'], 'threshold':200, 'factor':10000, 'salt': 1} The query above should retrieve 2% of the total stream (see Random Sampling above for why), predicated on the events being uniformly distributed over the zipcode s.","title":"Sampling"},{"location":"MQL/sampling/#random-sampling","text":"Random sampling uniformly downsamples the stream to a percentage of its original volume. You establish random sampling through a sampling clause like the following: select * from stream SAMPLE {'strategy': 'RANDOM', 'threshold': 200, 'factor': 10000} For each item in the stream, a numeric hash is generated. That hash is modded by factor to produce a result between 0 (inclusive) and factor (exclusive). If that result is less than threshold the item will be sampled, otherwise it will be ignored. You can determine the sampling percentage by remembering that \\frac{threshold}{factor} \\frac{threshold}{factor} values will be sampled \u2014 2% in the example above with a threshold of 200 and factor of 10000. You can also set a salt value, which will change the calculation of the hash. This is helpful if you want to sample over the same set of values but retrieve a different sample of those values. For example: select * from stream SAMPLE {'strategy': 'RANDOM', 'threshold': 200, 'factor': 10000, 'salt': 123}","title":"Random Sampling"},{"location":"MQL/sampling/#sticky-sampling","text":"Sticky sampling \u201csticks\u201d to certain values for the provided keys. That is if you are sampling on \u201c zipcode \u201d (as in the following example) and you observe a specific zipcode in the stream, you will observe all events which contain that specific zipcode . Sticky sampling can be achieved with a query like such: select * from stream SAMPLE {'strategy':'STICKY', 'keys':['zipcode'], 'threshold':200, 'factor':10000, 'salt': 1} The query above should retrieve 2% of the total stream (see Random Sampling above for why), predicated on the events being uniformly distributed over the zipcode s.","title":"Sticky Sampling"},{"location":"gettingstarted/cloud/","text":"Spinning up your first Mantis cluster in the cloud using the Mantis CLI \u00b6 Prerequisites \u00b6 The Mantis CLI currently supports AWS by spinning up a minimal cluster using T2 micro instances. This is meant for basic testing and not meant to run production traffic. In order for you to spin up a cluster in AWS, you will need to create or use an existing AWS account. You can follow AWS's account creation instructions for more information. Once your account is created, you will need to create and download your AWS Access Keys . You can follow AWS's instructions on creating access keys for more information. Now that you have your AWS account and access keys on hand, you're ready to bootstrap your first Mantis cluster using the Mantis CLI. Bootstrapping your first Mantis cluster in AWS \u00b6 Download and install the Mantis CLI \u00b6 First, you'll need to download the Mantis CLI app. If you're on Mac OS, it's recommended that you download and install using the Mac package: mantis-v0.1.1.pkg If you're on other systems such as Linux or Windows, choose the appropriate package from the Release Assets: v0.1.1 Release Assets Configure AWS credentials \u00b6 Now that you have the Mantis CLI installed, you'll need to tell it about your AWS credentials: $ mantis aws:configure AWS access key id: <input access key id> AWS secret access key: <input secret access key> Configuring AWS credentials... done This command stores your AWS credentials in the same exact format and location as the AWS SDK. More on this at the Mantis CLI page . Bootstrap your cluster \u00b6 With AWS credentials configured, you can bootstrap your cluster in a single command: $ mantis aws:bootstrap ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2193 Create key pair [ skipped ] \u2192 Key-pair file already exists ( mantis-us-east-2 ) \u2193 Create default VPC [ skipped ] \u2192 Default VPC already exists \u2193 Create zookeeper security group [ skipped ] \u2192 Security group already exists ( zookeeper ) \u2193 Authorize zookeeper security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2193 Authorize zookeeper security group zookeeper port ingress [ skipped ] \u2192 Ingress rule already exists \u2714 Bootstrap Zookeeper node \u2193 Create mesos-master security group [ skipped ] \u2192 Security group already exists ( mesos-master ) \u2193 Authorize mesos-master security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2839 Authorize mesos-master security group mesos-master port 5050 ingress Bootstrap Mesos Master node Create mesos-slave security group Authorize mesos-slave security group ssh port ingress Authorize mesos-slave security group mantis-agent port 7104 ingress Authorize mesos-slave security group mantis-agent port 7150 -7400 ingress Authorize mesos-slave security group mesos-slave resource port ingress Bootstrap Mesos Slave node Create mantis-control-plane security group Authorize mantis-control-plane security group ssh port ingress Authorize mantis-control-plane security group remote debug port 5050 ingress Authorize mantis-control-plane security group api port 8100 ingress Authorize mantis-control-plane security group api v2 port 8075 ingress Authorize mantis-control-plane security group scheduling info port 8076 ingress Authorize mantis-control-plane security group metrics port 8082 ingress Authorize mantis-control-plane security group console port 9090 ingress Bootstrap Mantis Control Plane service Create mantis-api security group Authorize mantis-api security group ssh port ingress Authorize mantis-api security group web port 80 ingress Authorize mantis-api security group ssl port 443 ingress Authorize mantis-api security group api port 7101 ingress Authorize mantis-api security group websocket port 7102 ingress Authorize mantis-api security group tunnel port 7001 ingress \u2714 Bootstrap Mantis API service Mantis API will be provisioned in a few minutes with public DNS available at <ec2 address>:7101 Input this URL into your local Mantis UI to connect to the Mantis cluster. Mesos Master will be provisioned in a few minutes with public DNS available at <ec2 address>:5050 Input this URL into your local Mantis UI so it can connect to Mesos logs. This will launch and configure 5 AWS EC2 instances. Notice at the end of the bootstrap are 2 EC2 address. You will need these to input into the Mantis UI. Using the Mantis UI \u00b6 On your browser, navigate to the Mantis UI at: https://netflix.github.io/mantis-ui And fill out the Registration form as follows: Name: Example Email: example@example.com Master Name: Example Mantis API URL: <your ec2 Mantis API URL outputted from the Mantis CLI> Mesos URL: <your ec2 Mesos URL outputted from the Mantis CLI> Launching a Mantis Job \u00b6 When you go into the UI, you'll notice that the Mantis CLI has automatically preloaded a Job Cluster for you to try out. Simply click on the SineTest Job Cluster to go into the cluster details page. Once in the cluster details page, click on the green Submit latest version button on the top right to bring you to the Job Submit page. On the Job Submit page, everything has already been configured for you. All you have to do is hit the green Submit to Mantis button at the bottom of the page to launch your first Mantis Job. Now you can view the output of this job. If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585} Tearing down your cluster \u00b6 Once you're done, you can clean up all of your AWS resources by tearing down instances and deleting security groups. $ aws:teardown ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2714 Terminate instances Debugging your cluster \u00b6 You can debug your cluster by looking at the logs. To look at the logs, you'll need to go into your AWS EC2 Console and find instances with the Application: Mantis tag. From there, you can look at the instances with the following security groups: zookeeper mesos-slave mesos-master mantis-control-plane mantis-api You can connect to your EC2 instances by following instructions from the Connect button at the top. Note The Mantis CLI puts your EC2 .pem keys in the same folder as your AWS credentials, typically located in $HOME/.aws . Application logs, e.g. Mantis-related or Zookeeper, for all instances will be located in /logs . Mesos-related logs for mesos-master and mesos-slave will be located in /var/run/mesos .","title":"Mantis Cluster in the cloud"},{"location":"gettingstarted/cloud/#spinning-up-your-first-mantis-cluster-in-the-cloud-using-the-mantis-cli","text":"","title":"Spinning up your first Mantis cluster in the cloud using the Mantis CLI"},{"location":"gettingstarted/cloud/#prerequisites","text":"The Mantis CLI currently supports AWS by spinning up a minimal cluster using T2 micro instances. This is meant for basic testing and not meant to run production traffic. In order for you to spin up a cluster in AWS, you will need to create or use an existing AWS account. You can follow AWS's account creation instructions for more information. Once your account is created, you will need to create and download your AWS Access Keys . You can follow AWS's instructions on creating access keys for more information. Now that you have your AWS account and access keys on hand, you're ready to bootstrap your first Mantis cluster using the Mantis CLI.","title":"Prerequisites"},{"location":"gettingstarted/cloud/#bootstrapping-your-first-mantis-cluster-in-aws","text":"","title":"Bootstrapping your first Mantis cluster in AWS"},{"location":"gettingstarted/cloud/#download-and-install-the-mantis-cli","text":"First, you'll need to download the Mantis CLI app. If you're on Mac OS, it's recommended that you download and install using the Mac package: mantis-v0.1.1.pkg If you're on other systems such as Linux or Windows, choose the appropriate package from the Release Assets: v0.1.1 Release Assets","title":"Download and install the Mantis CLI"},{"location":"gettingstarted/cloud/#configure-aws-credentials","text":"Now that you have the Mantis CLI installed, you'll need to tell it about your AWS credentials: $ mantis aws:configure AWS access key id: <input access key id> AWS secret access key: <input secret access key> Configuring AWS credentials... done This command stores your AWS credentials in the same exact format and location as the AWS SDK. More on this at the Mantis CLI page .","title":"Configure AWS credentials"},{"location":"gettingstarted/cloud/#bootstrap-your-cluster","text":"With AWS credentials configured, you can bootstrap your cluster in a single command: $ mantis aws:bootstrap ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2193 Create key pair [ skipped ] \u2192 Key-pair file already exists ( mantis-us-east-2 ) \u2193 Create default VPC [ skipped ] \u2192 Default VPC already exists \u2193 Create zookeeper security group [ skipped ] \u2192 Security group already exists ( zookeeper ) \u2193 Authorize zookeeper security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2193 Authorize zookeeper security group zookeeper port ingress [ skipped ] \u2192 Ingress rule already exists \u2714 Bootstrap Zookeeper node \u2193 Create mesos-master security group [ skipped ] \u2192 Security group already exists ( mesos-master ) \u2193 Authorize mesos-master security group ssh port ingress [ skipped ] \u2192 Ingress rule already exists \u2839 Authorize mesos-master security group mesos-master port 5050 ingress Bootstrap Mesos Master node Create mesos-slave security group Authorize mesos-slave security group ssh port ingress Authorize mesos-slave security group mantis-agent port 7104 ingress Authorize mesos-slave security group mantis-agent port 7150 -7400 ingress Authorize mesos-slave security group mesos-slave resource port ingress Bootstrap Mesos Slave node Create mantis-control-plane security group Authorize mantis-control-plane security group ssh port ingress Authorize mantis-control-plane security group remote debug port 5050 ingress Authorize mantis-control-plane security group api port 8100 ingress Authorize mantis-control-plane security group api v2 port 8075 ingress Authorize mantis-control-plane security group scheduling info port 8076 ingress Authorize mantis-control-plane security group metrics port 8082 ingress Authorize mantis-control-plane security group console port 9090 ingress Bootstrap Mantis Control Plane service Create mantis-api security group Authorize mantis-api security group ssh port ingress Authorize mantis-api security group web port 80 ingress Authorize mantis-api security group ssl port 443 ingress Authorize mantis-api security group api port 7101 ingress Authorize mantis-api security group websocket port 7102 ingress Authorize mantis-api security group tunnel port 7001 ingress \u2714 Bootstrap Mantis API service Mantis API will be provisioned in a few minutes with public DNS available at <ec2 address>:7101 Input this URL into your local Mantis UI to connect to the Mantis cluster. Mesos Master will be provisioned in a few minutes with public DNS available at <ec2 address>:5050 Input this URL into your local Mantis UI so it can connect to Mesos logs. This will launch and configure 5 AWS EC2 instances. Notice at the end of the bootstrap are 2 EC2 address. You will need these to input into the Mantis UI.","title":"Bootstrap your cluster"},{"location":"gettingstarted/cloud/#using-the-mantis-ui","text":"On your browser, navigate to the Mantis UI at: https://netflix.github.io/mantis-ui And fill out the Registration form as follows: Name: Example Email: example@example.com Master Name: Example Mantis API URL: <your ec2 Mantis API URL outputted from the Mantis CLI> Mesos URL: <your ec2 Mesos URL outputted from the Mantis CLI>","title":"Using the Mantis UI"},{"location":"gettingstarted/cloud/#launching-a-mantis-job","text":"When you go into the UI, you'll notice that the Mantis CLI has automatically preloaded a Job Cluster for you to try out. Simply click on the SineTest Job Cluster to go into the cluster details page. Once in the cluster details page, click on the green Submit latest version button on the top right to bring you to the Job Submit page. On the Job Submit page, everything has already been configured for you. All you have to do is hit the green Submit to Mantis button at the bottom of the page to launch your first Mantis Job. Now you can view the output of this job. If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585}","title":"Launching a Mantis Job"},{"location":"gettingstarted/cloud/#tearing-down-your-cluster","text":"Once you're done, you can clean up all of your AWS resources by tearing down instances and deleting security groups. $ aws:teardown ? select a region us-east-2 ? Proceed with Mantis cluster creation? ( Y/n ) Y \u2714 Terminate instances","title":"Tearing down your cluster"},{"location":"gettingstarted/cloud/#debugging-your-cluster","text":"You can debug your cluster by looking at the logs. To look at the logs, you'll need to go into your AWS EC2 Console and find instances with the Application: Mantis tag. From there, you can look at the instances with the following security groups: zookeeper mesos-slave mesos-master mantis-control-plane mantis-api You can connect to your EC2 instances by following instructions from the Connect button at the top. Note The Mantis CLI puts your EC2 .pem keys in the same folder as your AWS credentials, typically located in $HOME/.aws . Application logs, e.g. Mantis-related or Zookeeper, for all instances will be located in /logs . Mesos-related logs for mesos-master and mesos-slave will be located in /var/run/mesos .","title":"Debugging your cluster"},{"location":"gettingstarted/docker/","text":"Spinning up your first Mantis cluster using Docker \u00b6 Prerequisites \u00b6 Install Docker on your local machine (if you don't already have it) Mac Windows Linux Bootstraping your first Mantis Cluster in Docker \u00b6 Download the docker-compose file \u00b6 Download the docker-compose.yml to a local folder mantis $ cd <mantis> $ docker-compose -f docker-compose.yml up This starts up the following Docker containers: Zookeeper Mesos Master Mantis Master Mantis API Mesos Slave and Mantis Worker run on a single container (mantisagent) A simple hello world web application that sends events to Mantis A simple Java application that sends events to Manits Mantis Admin UI \u00b6 The Mantis Admin UI allows you to manage your Mantis Jobs. Open the Mantis UI in a new browser window. Fill out the Registration form as follows Name: Example Email: example@example.com Master Name: Example Mantis API URL: http://localhost:7101 Mesos URL: http://localhost:5050 Click on Create The Mantis Admin page should be pre-populated with all the Mantis examples. Try out Mantis Jobs \u00b6 Now that you have setup a Mantis cluster locally try running some of the preconfigured Mantis samples Sine Function Sample - A simple job that generates x and y coordinates of a sine wave. Twitter Sample - Connects to a twitter stream using consumer and token keys specified and performs a streaming word count. On Demand sourcing data from external apps sample - Demonstrates how Mantis Jobs can pull events on demand from external applications. Next steps. Setup Mantis in AWS and run the samples Learn to write your own Mantis Jobs To teardown the Mantis cluster, issue the following command $ cd <mantis> $ docker-compose -f docker-compose.yml down","title":"Mantis Cluster using Docker"},{"location":"gettingstarted/docker/#spinning-up-your-first-mantis-cluster-using-docker","text":"","title":"Spinning up your first Mantis cluster using Docker"},{"location":"gettingstarted/docker/#prerequisites","text":"Install Docker on your local machine (if you don't already have it) Mac Windows Linux","title":"Prerequisites"},{"location":"gettingstarted/docker/#bootstraping-your-first-mantis-cluster-in-docker","text":"","title":"Bootstraping your first Mantis Cluster in Docker"},{"location":"gettingstarted/docker/#download-the-docker-compose-file","text":"Download the docker-compose.yml to a local folder mantis $ cd <mantis> $ docker-compose -f docker-compose.yml up This starts up the following Docker containers: Zookeeper Mesos Master Mantis Master Mantis API Mesos Slave and Mantis Worker run on a single container (mantisagent) A simple hello world web application that sends events to Mantis A simple Java application that sends events to Manits","title":"Download the docker-compose file"},{"location":"gettingstarted/docker/#mantis-admin-ui","text":"The Mantis Admin UI allows you to manage your Mantis Jobs. Open the Mantis UI in a new browser window. Fill out the Registration form as follows Name: Example Email: example@example.com Master Name: Example Mantis API URL: http://localhost:7101 Mesos URL: http://localhost:5050 Click on Create The Mantis Admin page should be pre-populated with all the Mantis examples.","title":"Mantis Admin UI"},{"location":"gettingstarted/docker/#try-out-mantis-jobs","text":"Now that you have setup a Mantis cluster locally try running some of the preconfigured Mantis samples Sine Function Sample - A simple job that generates x and y coordinates of a sine wave. Twitter Sample - Connects to a twitter stream using consumer and token keys specified and performs a streaming word count. On Demand sourcing data from external apps sample - Demonstrates how Mantis Jobs can pull events on demand from external applications. Next steps. Setup Mantis in AWS and run the samples Learn to write your own Mantis Jobs To teardown the Mantis cluster, issue the following command $ cd <mantis> $ docker-compose -f docker-compose.yml down","title":"Try out Mantis Jobs"},{"location":"gettingstarted/local/","text":"Explore a Mantis Job locally \u00b6 Prerequisites \u00b6 JDK 8 or higher Build and run the synthetic-sourcejob sample \u00b6 Clone the mantis-examples repo $ git clone https://github.com/Netflix/mantis-examples.git Run the synthetic-sourcejob sample via gradle. This job outputs request events sourced from an imaginary service. The RequestEvent data has information such as uri, status, userId, country etc. Data Source Jobs are mantis jobs that allow consumers to filter the raw stream down to just the events they are interested in. This filtering is done by specifying an MQL query while connecting to the sink. To run the sample execute the following command. $ cd mantis-examples/synthetic-sourcejob $ ../gradlew execute This will launch the job and you would see output like 2019 -10-06 14 :14:07 INFO StageExecutors:254 main - initializing io.mantisrx.sourcejob.synthetic.stage.TaggingStage 2019 -10-06 14 :14:07 INFO SinkPublisher:82 main - Got sink subscription, onSubscribe = null 2019 -10-06 14 :14:07 INFO ServerSentEventsSink:141 main - Serving modern HTTP SSE server sink on port: 8436 The default Mantis sink is a ServerSentEvent sink that opens a port allowing anyone to connect to it and stream the results of the job. Look for a line like Serving modern HTTP SSE server sink on port: 8436 The source job is now up and ready to serve data. Let us query for requests from countries where the status code is 500. Such an MQL query would like like this. select country from stream where status == 500 In another terminal window curl this port $ curl \"localhost:8436?subscriptionId=nj&criterion=select%20country%20from%20stream%20where%20status%3D%3D500&clientId=nj2\" Here the subscriptionId and clientId are any valid strings. They are used to tag events that match the query. The criterion parameter is the URLEncoded MQL query. You should see events matching your query appear in your terminal data: { \"country\" : \"Ecuador\" , \"mantis.meta.sourceName\" : \"SyntheticRequestSource\" , \"mantis.meta.timestamp\" :1570396602599, \"status\" :500 } data: { \"country\" : \"Solomon Islands\" , \"mantis.meta.sourceName\" : \"SyntheticRequestSource\" , \"mantis.meta.timestamp\" :1570396603342, \"status\" :500 } data: { \"country\" : \"Liberia\" , \"mantis.meta.sourceName\" : \"SyntheticRequestSource\" , \"mantis.meta.timestamp\" :1570396603844, \"status\" :500 } Next Steps Import the project into your IDE to explore the code. Try out other samples from the Mantis examples repository Setup Mantis locally using Docker and run the samples Setup Mantis in AWS and run the samples Learn to write your own Mantis Jobs","title":"Explore a Mantis Job locally"},{"location":"gettingstarted/local/#explore-a-mantis-job-locally","text":"","title":"Explore a Mantis Job locally"},{"location":"gettingstarted/local/#prerequisites","text":"JDK 8 or higher","title":"Prerequisites"},{"location":"gettingstarted/local/#build-and-run-the-synthetic-sourcejob-sample","text":"Clone the mantis-examples repo $ git clone https://github.com/Netflix/mantis-examples.git Run the synthetic-sourcejob sample via gradle. This job outputs request events sourced from an imaginary service. The RequestEvent data has information such as uri, status, userId, country etc. Data Source Jobs are mantis jobs that allow consumers to filter the raw stream down to just the events they are interested in. This filtering is done by specifying an MQL query while connecting to the sink. To run the sample execute the following command. $ cd mantis-examples/synthetic-sourcejob $ ../gradlew execute This will launch the job and you would see output like 2019 -10-06 14 :14:07 INFO StageExecutors:254 main - initializing io.mantisrx.sourcejob.synthetic.stage.TaggingStage 2019 -10-06 14 :14:07 INFO SinkPublisher:82 main - Got sink subscription, onSubscribe = null 2019 -10-06 14 :14:07 INFO ServerSentEventsSink:141 main - Serving modern HTTP SSE server sink on port: 8436 The default Mantis sink is a ServerSentEvent sink that opens a port allowing anyone to connect to it and stream the results of the job. Look for a line like Serving modern HTTP SSE server sink on port: 8436 The source job is now up and ready to serve data. Let us query for requests from countries where the status code is 500. Such an MQL query would like like this. select country from stream where status == 500 In another terminal window curl this port $ curl \"localhost:8436?subscriptionId=nj&criterion=select%20country%20from%20stream%20where%20status%3D%3D500&clientId=nj2\" Here the subscriptionId and clientId are any valid strings. They are used to tag events that match the query. The criterion parameter is the URLEncoded MQL query. You should see events matching your query appear in your terminal data: { \"country\" : \"Ecuador\" , \"mantis.meta.sourceName\" : \"SyntheticRequestSource\" , \"mantis.meta.timestamp\" :1570396602599, \"status\" :500 } data: { \"country\" : \"Solomon Islands\" , \"mantis.meta.sourceName\" : \"SyntheticRequestSource\" , \"mantis.meta.timestamp\" :1570396603342, \"status\" :500 } data: { \"country\" : \"Liberia\" , \"mantis.meta.sourceName\" : \"SyntheticRequestSource\" , \"mantis.meta.timestamp\" :1570396603844, \"status\" :500 } Next Steps Import the project into your IDE to explore the code. Try out other samples from the Mantis examples repository Setup Mantis locally using Docker and run the samples Setup Mantis in AWS and run the samples Learn to write your own Mantis Jobs","title":"Build and run the synthetic-sourcejob sample"},{"location":"gettingstarted/samples/publishsample/","text":"Publishing Events to Mantis \u00b6 One of the key features of Mantis is the ability to stream filtered events on-demand from external applications. In this example we walk through publishing data to Mantis from a simple Java application using the mantis-publish library. Followed by setting up a Data Source Job that will act as a broker and a simple Mantis Job that when launched will trigger on-demand streaming of data matching a certain criterion. This end-to-end example highlights two powerful Mantis concepts On demand streaming of filtered data from external applications directly into Mantis Job Chaining where one Job connects to the output of another job. Prerequisites \u00b6 SharedMrePublishEventSource Job Cluster exists. JobConnectorSample Job cluster exists Java Sample is setup and running. Note If you are following the Mantis Cluster using Docker instructions all of this should be already setup. Publishing data to Mantis \u00b6 Note : The local docker setup has already preconfigured a simple Java Sample application to publish events to Mantis. For instructions on instrumenting your own application check out the Publishing events to Mantis Setting up a Publish Data Source Job \u00b6 A Publish Source Job is a special kind of a Mantis Job that interacts with the mantis-publish library on behalf of a downstream job to push subscriptions up to the mantis-publish library and receive events matching the subscription from the mantis-publish library. Submit the SharedMrePublishEventSource \u00b6 Part of the docker setup we have preconfigured the SharedMrePublishEventSource cluster. So all we have to do is submit an instance of it. Go to the clusters page and click on SharedMrePublishEventSource Click submit on the top right corner of the screen This will open up the Job Detail page. Now wait for the Job to go into Launched state You are all set. Now the Java application referenced in the previous section should be able to communicate with this source job to exchange subscriptions and data. At this point however there are no active subscriptions so no data is actually being sent out from our Java application. If you look at the shell window where the docker is running you should see output like mantispublish_1 | 2019 -10-16 17 :55:46 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = > SKIPPED_NO_SUBSCRIPTIONS ( PRECONDITION_FAILED ) Next step. Launch a new job to query for Data generated by the Java application. Query data generated by the Java application. \u00b6 Our Java application generates a stream of Events representing requests made to it by an external client. Say we want to look at all events that are failing i.e have status=500 Submit a query job \u00b6 Now we launch a simple Mantis Job to query data generated by our Java Application. Go to clusters page and click on JobConnectorSample Click the green submit button on the top right corner of the screen. On the Job submit page scroll down and click on Override Defaults to configure our query. Enter the following as the value for parameter target json {\"targets\":[{\"sourceJobName\":\"SharedMrePublishEventSource\",\"criterion\":\"select * from defaultStream where status==500\"}]} and hit submit. Two key things to note: We set the sourceJobName to SharedMrePublishEventSource which is the source job we configured in the previous step. We set the criterion key in the payload to our MQL query select * from defaultStream where status==500 Click on Submit . On the Job Detail page scroll down to the Job Output section and click on Start. In a few seconds you should see events matching our query flow through. If you go back to the shell that is running the docker images you should now see output like mantispublish_1 | 2019 -10-16 17 :58:32 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = > ENQUEUED ( SENDING ) If we now terminate our JobConnector Job, then our Java application will again revert to not sending any data. Take aways \u00b6 By integrating the mantis-publish library with their applications, users can get access to rich data generated by their applications in realtime and in a cost-effective manner for analysis into their jobs.","title":"Publish to Mantis Sample"},{"location":"gettingstarted/samples/publishsample/#publishing-events-to-mantis","text":"One of the key features of Mantis is the ability to stream filtered events on-demand from external applications. In this example we walk through publishing data to Mantis from a simple Java application using the mantis-publish library. Followed by setting up a Data Source Job that will act as a broker and a simple Mantis Job that when launched will trigger on-demand streaming of data matching a certain criterion. This end-to-end example highlights two powerful Mantis concepts On demand streaming of filtered data from external applications directly into Mantis Job Chaining where one Job connects to the output of another job.","title":"Publishing Events to Mantis"},{"location":"gettingstarted/samples/publishsample/#prerequisites","text":"SharedMrePublishEventSource Job Cluster exists. JobConnectorSample Job cluster exists Java Sample is setup and running. Note If you are following the Mantis Cluster using Docker instructions all of this should be already setup.","title":"Prerequisites"},{"location":"gettingstarted/samples/publishsample/#publishing-data-to-mantis","text":"Note : The local docker setup has already preconfigured a simple Java Sample application to publish events to Mantis. For instructions on instrumenting your own application check out the Publishing events to Mantis","title":"Publishing data to Mantis"},{"location":"gettingstarted/samples/publishsample/#setting-up-a-publish-data-source-job","text":"A Publish Source Job is a special kind of a Mantis Job that interacts with the mantis-publish library on behalf of a downstream job to push subscriptions up to the mantis-publish library and receive events matching the subscription from the mantis-publish library.","title":"Setting up a Publish Data Source Job"},{"location":"gettingstarted/samples/publishsample/#submit-the-sharedmrepublisheventsource","text":"Part of the docker setup we have preconfigured the SharedMrePublishEventSource cluster. So all we have to do is submit an instance of it. Go to the clusters page and click on SharedMrePublishEventSource Click submit on the top right corner of the screen This will open up the Job Detail page. Now wait for the Job to go into Launched state You are all set. Now the Java application referenced in the previous section should be able to communicate with this source job to exchange subscriptions and data. At this point however there are no active subscriptions so no data is actually being sent out from our Java application. If you look at the shell window where the docker is running you should see output like mantispublish_1 | 2019 -10-16 17 :55:46 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = > SKIPPED_NO_SUBSCRIPTIONS ( PRECONDITION_FAILED ) Next step. Launch a new job to query for Data generated by the Java application.","title":"Submit the SharedMrePublishEventSource"},{"location":"gettingstarted/samples/publishsample/#query-data-generated-by-the-java-application","text":"Our Java application generates a stream of Events representing requests made to it by an external client. Say we want to look at all events that are failing i.e have status=500","title":"Query data generated by the Java application."},{"location":"gettingstarted/samples/publishsample/#submit-a-query-job","text":"Now we launch a simple Mantis Job to query data generated by our Java Application. Go to clusters page and click on JobConnectorSample Click the green submit button on the top right corner of the screen. On the Job submit page scroll down and click on Override Defaults to configure our query. Enter the following as the value for parameter target json {\"targets\":[{\"sourceJobName\":\"SharedMrePublishEventSource\",\"criterion\":\"select * from defaultStream where status==500\"}]} and hit submit. Two key things to note: We set the sourceJobName to SharedMrePublishEventSource which is the source job we configured in the previous step. We set the criterion key in the payload to our MQL query select * from defaultStream where status==500 Click on Submit . On the Job Detail page scroll down to the Job Output section and click on Start. In a few seconds you should see events matching our query flow through. If you go back to the shell that is running the docker images you should now see output like mantispublish_1 | 2019 -10-16 17 :58:32 INFO SampleDataPublisher:56 - Mantis publish JavaApp send event status = > ENQUEUED ( SENDING ) If we now terminate our JobConnector Job, then our Java application will again revert to not sending any data.","title":"Submit a query job"},{"location":"gettingstarted/samples/publishsample/#take-aways","text":"By integrating the mantis-publish library with their applications, users can get access to rich data generated by their applications in realtime and in a cost-effective manner for analysis into their jobs.","title":"Take aways"},{"location":"gettingstarted/samples/sinesample/","text":"Sine Function sample \u00b6 The Sine function sample is a very simple job that generates a set of x and y coordinates of a sine wave. Prerequisites \u00b6 A SineFunction Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this should be already setup. Running the sample \u00b6 Go to the clusters page page and Click on SineFunction On the Job Cluster detail page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us skip all that and scroll directly to the bottom and hit the Submit button on the bottom left. View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585} Terminate the job \u00b6 To stop the job click on the red Kill Job button on the top right corner. Next Steps \u00b6 Explore the code Checkout out the other samples","title":"Sine Function"},{"location":"gettingstarted/samples/sinesample/#sine-function-sample","text":"The Sine function sample is a very simple job that generates a set of x and y coordinates of a sine wave.","title":"Sine Function sample"},{"location":"gettingstarted/samples/sinesample/#prerequisites","text":"A SineFunction Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this should be already setup.","title":"Prerequisites"},{"location":"gettingstarted/samples/sinesample/#running-the-sample","text":"Go to the clusters page page and Click on SineFunction On the Job Cluster detail page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us skip all that and scroll directly to the bottom and hit the Submit button on the bottom left. View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585}","title":"Running the sample"},{"location":"gettingstarted/samples/sinesample/#terminate-the-job","text":"To stop the job click on the red Kill Job button on the top right corner.","title":"Terminate the job"},{"location":"gettingstarted/samples/sinesample/#next-steps","text":"Explore the code Checkout out the other samples","title":"Next Steps"},{"location":"gettingstarted/samples/twittersample/","text":"Twitter sample \u00b6 The Twitter sample demonstrates sourcing data from an external source (twitter in this case) and calculates the number of occurrences of a word within a hopping window. Prerequisites \u00b6 A TwitterSample Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this should be already setup. Twitter credentials to be used to connect to Twitter. If you don't already have a twitter application You can create one here here The Keys and Tokens section should list the credentials needed for this application. Running the sample \u00b6 Let us try submitting this job. Click on TwitterSample from the clusters page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us scroll down to the parameters section. Here we fill in the required parameters for this job. These include Twitter consumer key Twitter consumer secret Twitter token Twitter token secret View output of the job \u00b6 If all goes well your job would go into Launched state. Scroll to the bottom and click on Start You should see output of the Twitter job being streamed below Terminate the job \u00b6 To stop the job click on the red Kill Job button on the top right corner. Explore the code Checkout out the other samples","title":"Twitter Sample"},{"location":"gettingstarted/samples/twittersample/#twitter-sample","text":"The Twitter sample demonstrates sourcing data from an external source (twitter in this case) and calculates the number of occurrences of a word within a hopping window.","title":"Twitter sample"},{"location":"gettingstarted/samples/twittersample/#prerequisites","text":"A TwitterSample Job Cluster exists. Note If you are following the Mantis Cluster using Docker instructions this should be already setup. Twitter credentials to be used to connect to Twitter. If you don't already have a twitter application You can create one here here The Keys and Tokens section should list the credentials needed for this application.","title":"Prerequisites"},{"location":"gettingstarted/samples/twittersample/#running-the-sample","text":"Let us try submitting this job. Click on TwitterSample from the clusters page. Click the Submit green button on the top right. This will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us scroll down to the parameters section. Here we fill in the required parameters for this job. These include Twitter consumer key Twitter consumer secret Twitter token Twitter token secret","title":"Running the sample"},{"location":"gettingstarted/samples/twittersample/#view-output-of-the-job","text":"If all goes well your job would go into Launched state. Scroll to the bottom and click on Start You should see output of the Twitter job being streamed below","title":"View output of the job"},{"location":"gettingstarted/samples/twittersample/#terminate-the-job","text":"To stop the job click on the red Kill Job button on the top right corner. Explore the code Checkout out the other samples","title":"Terminate the job"},{"location":"internals/mre/","text":"The Mantis Publish library (known internally to Netflix as Mantis Realtime Events, or MRE) allows your application to stream events into Mantis. The library supports JVM and Node.js applications. Mantis Publish takes care of filtering the events you send into Mantis, and it will only transmit them over the network if a Mantis client that is interested in such events is currently subscribed. Mantis Publish contains a subscription registry where each client subscription is represented by its corresponding Mantis Query Language (MQL) query. Mantis Publish evaluates all MQL queries from its subscription registry against each event as the event is generated by your application. It then tags events with all of the matching MQL queries and enriches the event with a superset of fields from the matched queries. That is, rather than emitting n events for n matching MQL queries, Mantis Publish will instead emit a single event containing all of the fields requested by the matched queries. This happens directly within your application before any events are sent over the network into Mantis. Further, Mantis Publish only dispatches events if a client is subscribed with a matching query. This means that you can freely produce events without incurring the cost until an active subscription exists. How to Add the Mantis Publish Library \u00b6 To add this library to your application, include mantis-publish-netty in your application\u2019s dependencies. compile 'io.mantisrx:mantis-publish-netty:1.2.+' If your application is guice enabled you can do the following compile 'io.mantisrx:mantis-publish-netty-guice:1.2.+' Note A spring based module is coming soon. How to Stream Events Into Mantis \u00b6 Setting up the client. \u00b6 To stream events into Mantis Without dependency injection : Directly instantiate a MantisEventPublisher in your application code. In order to create a MantisEventPublisher , you will have to inject a few parameters. An example of which parameters are required and how to inject them can be found in here . Next, you must use the MrePublishClientInitializer class and call MrePublishClientInitializer#start to start all the underlying components With Guice : Inject the MantisRealtimeEventsPublishModule into your application. In addition to injecting MantisRealtimeEventsPublishModule you will also need to add the ArchaiusModule and the SpectatorModule if not already injected. Injector injector = Guice . createInjector ( new BasicModule (), new ArchaiusModule (), new MantisRealtimeEventsPublishModule (), new SpectatorModule ()); Once injected, either manually via constructor or an injection framework such as Guice or Spring, the mantis-publish library is ready for use. For each event your application wishes to send to Mantis, create a Event object with your desired event fields, and pass that Event to the MantisEventPublisher#publish method. For example: // Create an `Event` for Mantis Publish using your application event. final Event event = new Event (); event . set ( \"testKey\" , \"testValue\" ); // Send your `Event` into Mantis. // Note: This event will only be dispatched over the network // if a subscription with a matching MQL query exists. eventPublisher . publish ( event ); Configuring where to send data \u00b6 We need to configure the location of the Mantis API server for the mantis-publish library to bootstrap Add the following properties to your application.properties mantis.publish.discovery.api.hostname = <IP of Mantis API> # mantis api port mantis.publish.discovery.api.port = <port for Mantis API> # This application's name mantis.publish.app.name = JavaApp The Runtime Flow of Mantis Publish \u00b6 Mantis Publish runtime flow consists of three phases: connecting, event processing, and event delivery. Phase 1: Connecting \u00b6 Mantis Publish will only stream an event from your application into Mantis if there is a subscriber to that specific application instance with an MQL query that matches the event. Any Mantis Job can connect to these applications. However, it is a best practice to have Source Jobs connect to Mantis Publish applications. This is because Source Jobs provide several conveniences over regular Mantis Jobs, such as multiplexing events and connection management. By leveraging Source Jobs as an intermediary, Mantis Jobs are able to consume events from an external source without having to worry about lower-level details of that external source. This is possible through job chaining, which Mantis provides by default. When connecting to a Mantis Publish application, downstream Mantis Jobs will send subscription requests with an MQL query via HTTP to a Source Job. The Source Job will store these subscriptions in memory. These subscriptions are then fetched by upstream applications at the edge running the Mantis Publish library. Once the upstream edge Mantis Publish application is aware of the subscription, it will start pushing events downstream into the Source Job. Note The Mantis Publish library not only handles subscriptions, but also takes care of discovering Source Job workers so you do not have to worry about Source Job rebalancing/autoscaling. For more information about Source Jobs see Mantis Internals: Mantis Source Jobs . Creating Stream Subscriptions \u00b6 Clients such as Mantis Jobs connect to a Mantis Publish application by submitting a subscription represented by an HTTP request. Mantis Publish\u2019s StreamManager maintains these subscriptions in-memory. The StreamManager manages internal resources such as subscriptions, streams, and internal processing queues. Clients can create subscriptions to different event streams. There are two types: default streams contain events emitted by applications that use the Mantis Publish library. log streams contain events which may not be core to the application, such as log or general infrastructure events. Phase 2: Event Processing \u00b6 Event processing within Mantis Publish takes place in two steps: event ingestion and event dispatch. Event Ingestion \u00b6 Event ingestion begins at the edge, in your application, by invoking EventPublisher#publish which places the event onto an internal queue for dispatching. Event Dispatch \u00b6 Events are dispatched by a drainer thread created by the Event Publisher. The drainer will drain events from the internal queue previously populated by EventPublisher#publish , perform some transformations, and finally dispatch events over the network and into Mantis. Events are transformed by an EventProcessor which processes events one at a time. Transformation includes the following steps: Masks sensitive fields in the event. Sensitive fields are referenced by a blacklist defined by a configuration ( mantis.publish.blacklist ). This blacklist is a comma-delimited string of keys you wish to blacklist in your event. The param.password key is included by default. Evaluates the MQL query of each subscription and builds a list of matching subscriptions. For each matching subscription, enriches the event with a superset of fields from the MQL query from all the other matching subscriptions (see the following diagram). Sends this enriched event to all of the subscribers (see Event Delivery below for the details). Note More Mantis Publish configuration options can be found here . Phase 3: Event Delivery \u00b6 Mantis Publish delivers events on-demand. When a client subscribes to a Mantis Job that issues an MQL query, the Event Publisher delivers the event using non-blocking I/O.","title":"Publishing Events to Mantis"},{"location":"internals/mre/#how-to-add-the-mantis-publish-library","text":"To add this library to your application, include mantis-publish-netty in your application\u2019s dependencies. compile 'io.mantisrx:mantis-publish-netty:1.2.+' If your application is guice enabled you can do the following compile 'io.mantisrx:mantis-publish-netty-guice:1.2.+' Note A spring based module is coming soon.","title":"How to Add the Mantis Publish Library"},{"location":"internals/mre/#how-to-stream-events-into-mantis","text":"","title":"How to Stream Events Into Mantis"},{"location":"internals/mre/#setting-up-the-client","text":"To stream events into Mantis Without dependency injection : Directly instantiate a MantisEventPublisher in your application code. In order to create a MantisEventPublisher , you will have to inject a few parameters. An example of which parameters are required and how to inject them can be found in here . Next, you must use the MrePublishClientInitializer class and call MrePublishClientInitializer#start to start all the underlying components With Guice : Inject the MantisRealtimeEventsPublishModule into your application. In addition to injecting MantisRealtimeEventsPublishModule you will also need to add the ArchaiusModule and the SpectatorModule if not already injected. Injector injector = Guice . createInjector ( new BasicModule (), new ArchaiusModule (), new MantisRealtimeEventsPublishModule (), new SpectatorModule ()); Once injected, either manually via constructor or an injection framework such as Guice or Spring, the mantis-publish library is ready for use. For each event your application wishes to send to Mantis, create a Event object with your desired event fields, and pass that Event to the MantisEventPublisher#publish method. For example: // Create an `Event` for Mantis Publish using your application event. final Event event = new Event (); event . set ( \"testKey\" , \"testValue\" ); // Send your `Event` into Mantis. // Note: This event will only be dispatched over the network // if a subscription with a matching MQL query exists. eventPublisher . publish ( event );","title":"Setting up the client."},{"location":"internals/mre/#configuring-where-to-send-data","text":"We need to configure the location of the Mantis API server for the mantis-publish library to bootstrap Add the following properties to your application.properties mantis.publish.discovery.api.hostname = <IP of Mantis API> # mantis api port mantis.publish.discovery.api.port = <port for Mantis API> # This application's name mantis.publish.app.name = JavaApp","title":"Configuring where to send data"},{"location":"internals/mre/#the-runtime-flow-of-mantis-publish","text":"Mantis Publish runtime flow consists of three phases: connecting, event processing, and event delivery.","title":"The Runtime Flow of Mantis Publish"},{"location":"internals/mre/#phase-1-connecting","text":"Mantis Publish will only stream an event from your application into Mantis if there is a subscriber to that specific application instance with an MQL query that matches the event. Any Mantis Job can connect to these applications. However, it is a best practice to have Source Jobs connect to Mantis Publish applications. This is because Source Jobs provide several conveniences over regular Mantis Jobs, such as multiplexing events and connection management. By leveraging Source Jobs as an intermediary, Mantis Jobs are able to consume events from an external source without having to worry about lower-level details of that external source. This is possible through job chaining, which Mantis provides by default. When connecting to a Mantis Publish application, downstream Mantis Jobs will send subscription requests with an MQL query via HTTP to a Source Job. The Source Job will store these subscriptions in memory. These subscriptions are then fetched by upstream applications at the edge running the Mantis Publish library. Once the upstream edge Mantis Publish application is aware of the subscription, it will start pushing events downstream into the Source Job. Note The Mantis Publish library not only handles subscriptions, but also takes care of discovering Source Job workers so you do not have to worry about Source Job rebalancing/autoscaling. For more information about Source Jobs see Mantis Internals: Mantis Source Jobs .","title":"Phase 1: Connecting"},{"location":"internals/mre/#creating-stream-subscriptions","text":"Clients such as Mantis Jobs connect to a Mantis Publish application by submitting a subscription represented by an HTTP request. Mantis Publish\u2019s StreamManager maintains these subscriptions in-memory. The StreamManager manages internal resources such as subscriptions, streams, and internal processing queues. Clients can create subscriptions to different event streams. There are two types: default streams contain events emitted by applications that use the Mantis Publish library. log streams contain events which may not be core to the application, such as log or general infrastructure events.","title":"Creating Stream Subscriptions"},{"location":"internals/mre/#phase-2-event-processing","text":"Event processing within Mantis Publish takes place in two steps: event ingestion and event dispatch.","title":"Phase 2: Event Processing"},{"location":"internals/mre/#event-ingestion","text":"Event ingestion begins at the edge, in your application, by invoking EventPublisher#publish which places the event onto an internal queue for dispatching.","title":"Event Ingestion"},{"location":"internals/mre/#event-dispatch","text":"Events are dispatched by a drainer thread created by the Event Publisher. The drainer will drain events from the internal queue previously populated by EventPublisher#publish , perform some transformations, and finally dispatch events over the network and into Mantis. Events are transformed by an EventProcessor which processes events one at a time. Transformation includes the following steps: Masks sensitive fields in the event. Sensitive fields are referenced by a blacklist defined by a configuration ( mantis.publish.blacklist ). This blacklist is a comma-delimited string of keys you wish to blacklist in your event. The param.password key is included by default. Evaluates the MQL query of each subscription and builds a list of matching subscriptions. For each matching subscription, enriches the event with a superset of fields from the MQL query from all the other matching subscriptions (see the following diagram). Sends this enriched event to all of the subscribers (see Event Delivery below for the details). Note More Mantis Publish configuration options can be found here .","title":"Event Dispatch"},{"location":"internals/mre/#phase-3-event-delivery","text":"Mantis Publish delivers events on-demand. When a client subscribes to a Mantis Job that issues an MQL query, the Event Publisher delivers the event using non-blocking I/O.","title":"Phase 3: Event Delivery"},{"location":"internals/runtime/","text":"The Mantis Runtime is consists of two components: A single Mantis Master which coordinates the execution of Mantis Jobs . Independent Mantis Jobs which receive streams of events as input, transform events one at a time, and produce streams of events as output. This page assumes familiarity with Mantis Job high-level concepts. An introduction can be found in Writing Mantis Jobs . This page presents internal details for Mantis Jobs. Mantis Job Components \u00b6 A Mantis Job consists of three components . Each one is based on a cold Observable that emits events to the next Observer in the Observable chain: Source The Source component is an RxFunction that consumes data in a streaming, non-blocking, backpressure -aware manner from an external service. Processing Stage A Processing Stage component is based on an RxFunction . This is where event transformations take place. There can be many Processing Stages in a Mantis Job. Sink The Sink component is based on an RxAction . It asynchronously emits results of the final Processing Stage to an external service. Note Mantis Jobs can consume events from typical external services such as APIs, databases, and Kafka topics. Mantis Jobs can also consume events emitted by other Mantis Jobs. This is referred to in Mantis as job chaining . Runtime Lifecycle \u00b6 The entry point for a Mantis Job is the Mantis Worker . The Mantis Master starts three primary services on a Mantis Worker when the Master boots the Worker up: The virtual machine worker service interacts with the underlying substrate, currently Mesos . This service subscribes to task updates and registers the Mantis Worker with Mesos executor callbacks to launch Mantis Jobs. The heartbeat service sends HTTP heartbeat requests to notify the Mantis Master that the worker is alive and available to process events. The stage executor dynamically loads bytecode for a Mantis Job, creates an in-memory representation of all the metadata required to execute events for that Mantis Job, and processes events for the current Processing Stage. Job Master Stage \u00b6 The Job Master autoscales Processing Stages. It can autoscale such stages independently of each other. If the configuration of a Job indicates that any Processing Stage is autoscalable, Mantis will automatically add a Job Master as the initial processing stage of the Job. This is a hidden stage that Job owners do not explicitly manage; instead, Mantis will create and configure a JobMasterService . This service creates a subscription to worker metrics via the WorkerMetricHandler and a MetricsClient which receives metrics over HTTP via SSE and sends them over to the JobAutoScaler . Job Autoscaler \u00b6 The Job autoscaler is based on a PID controller . Within this autoscaler are three controllers for CPU, memory, and network resources which continuously calculate an error value and apply corrections. Once the autoscaler makes a prediction, it delegates an API call to the Mantis Master to perform the scaling action on resources for a Processing stage. Single-Stage and Multi-Stage Jobs \u00b6 A Job with only one Processing Stage is a single-stage Job. In such a case, the entire Job (Source, Processing Stage, and Sink) will execute on the current worker node. A Job with more than one Processing Stage is a multi-stage Job. In such a Job, the stage executor will first inspect the current component. If the current component is a Source, then the executor will execute it as a Source. Otherwise, it will inspect the context again to determine if current component is a Sink. If so, it will acquire a port and create a SinkPublisher to publish events to the next Job. Finally, if the component is a normal Processing Stage, then the executor will execute its transformations.","title":"Mantis Runtime"},{"location":"internals/runtime/#mantis-job-components","text":"A Mantis Job consists of three components . Each one is based on a cold Observable that emits events to the next Observer in the Observable chain: Source The Source component is an RxFunction that consumes data in a streaming, non-blocking, backpressure -aware manner from an external service. Processing Stage A Processing Stage component is based on an RxFunction . This is where event transformations take place. There can be many Processing Stages in a Mantis Job. Sink The Sink component is based on an RxAction . It asynchronously emits results of the final Processing Stage to an external service. Note Mantis Jobs can consume events from typical external services such as APIs, databases, and Kafka topics. Mantis Jobs can also consume events emitted by other Mantis Jobs. This is referred to in Mantis as job chaining .","title":"Mantis Job Components"},{"location":"internals/runtime/#runtime-lifecycle","text":"The entry point for a Mantis Job is the Mantis Worker . The Mantis Master starts three primary services on a Mantis Worker when the Master boots the Worker up: The virtual machine worker service interacts with the underlying substrate, currently Mesos . This service subscribes to task updates and registers the Mantis Worker with Mesos executor callbacks to launch Mantis Jobs. The heartbeat service sends HTTP heartbeat requests to notify the Mantis Master that the worker is alive and available to process events. The stage executor dynamically loads bytecode for a Mantis Job, creates an in-memory representation of all the metadata required to execute events for that Mantis Job, and processes events for the current Processing Stage.","title":"Runtime Lifecycle"},{"location":"internals/runtime/#job-master-stage","text":"The Job Master autoscales Processing Stages. It can autoscale such stages independently of each other. If the configuration of a Job indicates that any Processing Stage is autoscalable, Mantis will automatically add a Job Master as the initial processing stage of the Job. This is a hidden stage that Job owners do not explicitly manage; instead, Mantis will create and configure a JobMasterService . This service creates a subscription to worker metrics via the WorkerMetricHandler and a MetricsClient which receives metrics over HTTP via SSE and sends them over to the JobAutoScaler .","title":"Job Master Stage"},{"location":"internals/runtime/#job-autoscaler","text":"The Job autoscaler is based on a PID controller . Within this autoscaler are three controllers for CPU, memory, and network resources which continuously calculate an error value and apply corrections. Once the autoscaler makes a prediction, it delegates an API call to the Mantis Master to perform the scaling action on resources for a Processing stage.","title":"Job Autoscaler"},{"location":"internals/runtime/#single-stage-and-multi-stage-jobs","text":"A Job with only one Processing Stage is a single-stage Job. In such a case, the entire Job (Source, Processing Stage, and Sink) will execute on the current worker node. A Job with more than one Processing Stage is a multi-stage Job. In such a Job, the stage executor will first inspect the current component. If the current component is a Source, then the executor will execute it as a Source. Otherwise, it will inspect the context again to determine if current component is a Sink. If so, it will acquire a port and create a SinkPublisher to publish events to the next Job. Finally, if the component is a normal Processing Stage, then the executor will execute its transformations.","title":"Single-Stage and Multi-Stage Jobs"},{"location":"internals/sourcejobs/","text":"Mantis Source Jobs are Mantis Jobs that fetch data from external sources. There are four types of Source Jobs: Mantis Publish Source Jobs read from their sources by using Mantis Publish . As such, they do not apply MQL on events themselves. Instead, they propagate the MQL queries upstream to Mantis Publish running on the external source, which then applies the MQL queries to the events it produces and only then pushes those events downstream to the Request Source Job. Kafka Source Jobs consume events from Kafka and apply MQL to each incoming event. Composition of a Source Job \u00b6 A Source Job is composed of three components: the default Source custom Processing Stages including a tagging operator the default Sink consisting of an SSE operator For example, here is how you might declare a Kafka Source Job: MantisJob . source ( KafkaSource ) . stage ( getAckableTaggingStage (), CustomizedAutoAckTaggingStage . config ()) . sink ( new TaggedDataSourceSink ( new QueryRequestPreProcessor (), new QueryRequestPostProcessor ())) . lifecycle (...) . create (); Source (RxFunction) \u00b6 The Source in this example contains code that creates and manages connections to Kafka using the 0.10 high level consumer. It creates an Observable with backpressure semantics by leveraging the SyncOnSubscribe class. Processing Stage (RxFunction) \u00b6 The next stage in this Job is the Processing Stage which enriches events with metadata . This stage transforms events in the following way: Applies a user-defined pre-mapping function. This is a Groovy function that takes a Map<String, Object> and returns a Map<String, Object> referenced by a variable named e . Filters out empty events. Inspects its internal subscription registry and enriches each event with all matching subscriptions. Subscriptions are represented by an MQL query and are registered when a consumer (e.g. Mantis Job) subscribes to the Source Job. Each event is enriched with fields specified by the projections of a subscription\u2019s MQL query, as in the following illustration: Sink (RxAction) \u00b6 In order for a consumer to consume events from a Source Job, the consumer connects to the Job\u2019s Sink. Consumers subscribe to a Source Job by sending a subscription request over HTTP to the Source Job\u2019s Sink. When a consumer connects to a Sink, the consumer must provide three query parameters: criterion \u2014 An MQL query string clientId \u2014 This is automatically generated if you use the Mantis client library; it defaults to the Mantis Job ID subscriptionId \u2014 This is used as a load-balancing mechanism for clientId A consumer (represented as a client through clientId ) may have many consumer instances (represented as susbcriptions through subscriptionId ). Source Jobs use clientId and subscriptionId to broadcast and/or load balance events to consumers. Source Jobs will broadcast an event to all clientId s. This means that consumer instances with different clientId s will each receive the same event. However, Source Jobs will load balance an event within a clientId . This means that consumer instances with the same clientId but different subscriptionId s are effectively grouped together. Events with the same clientId are load balanced among its subscriptionId s. Example of subscribing to a Source Job\u2019s Sink which outputs the results of a sine wave function: curl \"http:// instance-address : port ?clientId= myId &subscriptionId= mySubscription &criterion=select%20*%20where%20true\" data: {\"x\": 60.000000, \"y\": -3.048106} data: {\"x\": 100.000000, \"y\": -5.063656} data: {\"x\": 26.000000, \"y\": 7.625585} \u22ee Sinks have a pre-processor ( QueryRequestPreProcessor ), a post-processor ( QueryRequestPostProcessor ), and a router: The pre-processor is an RxFunction that registers the consumer\u2019s query, with their clientId and subscriptionId , into an in-memory cache called a QueryRefCountMap when a consumer instance connects to the Sink. This registers queries so that the Source Job can apply them to events as those events are ingested by the Source Job. The post-processor is an RxFunction that de-registers subscriptions from the QueryRefCountMap when a consumer instance disconnects from the Sink. The Source Job removes removes the clientId entirely from the QueryRefCountMap only when all of its subscriptionId s have been removed. The router routes incoming events for a clientId s to its subscriptions. It does this by using a drainer called a ChunkProcessor to drain events from an internal queue on an interval and randomly distribute the events to subscriptions. Note Typically, subscriptions to a Source Job come from other Mantis Jobs. However, because subscriptions are SSE endpoints, you can subscribe to Source Jobs over that same SSE endpoint to view the Job\u2019s output for debugging purposes. Caveats \u00b6 Source Jobs are single-stage Mantis Jobs that perform projection and filtering operations. MQL queries containing groupBy , orderBy , and window are ignored. These clauses are interpreted into RxJava operations and run by downstream Mantis Jobs. Mantis Publish-based Source Jobs do not autoscale . Autoscaling Mantis Publish-based Source Jobs requires future work to reshuffle connections among all Source Job instances and their upstream Mantis Publish connections.","title":"Mantis Source Jobs"},{"location":"internals/sourcejobs/#composition-of-a-source-job","text":"A Source Job is composed of three components: the default Source custom Processing Stages including a tagging operator the default Sink consisting of an SSE operator For example, here is how you might declare a Kafka Source Job: MantisJob . source ( KafkaSource ) . stage ( getAckableTaggingStage (), CustomizedAutoAckTaggingStage . config ()) . sink ( new TaggedDataSourceSink ( new QueryRequestPreProcessor (), new QueryRequestPostProcessor ())) . lifecycle (...) . create ();","title":"Composition of a Source Job"},{"location":"internals/sourcejobs/#source-rxfunction","text":"The Source in this example contains code that creates and manages connections to Kafka using the 0.10 high level consumer. It creates an Observable with backpressure semantics by leveraging the SyncOnSubscribe class.","title":"Source (RxFunction)"},{"location":"internals/sourcejobs/#processing-stage-rxfunction","text":"The next stage in this Job is the Processing Stage which enriches events with metadata . This stage transforms events in the following way: Applies a user-defined pre-mapping function. This is a Groovy function that takes a Map<String, Object> and returns a Map<String, Object> referenced by a variable named e . Filters out empty events. Inspects its internal subscription registry and enriches each event with all matching subscriptions. Subscriptions are represented by an MQL query and are registered when a consumer (e.g. Mantis Job) subscribes to the Source Job. Each event is enriched with fields specified by the projections of a subscription\u2019s MQL query, as in the following illustration:","title":"Processing Stage (RxFunction)"},{"location":"internals/sourcejobs/#sink-rxaction","text":"In order for a consumer to consume events from a Source Job, the consumer connects to the Job\u2019s Sink. Consumers subscribe to a Source Job by sending a subscription request over HTTP to the Source Job\u2019s Sink. When a consumer connects to a Sink, the consumer must provide three query parameters: criterion \u2014 An MQL query string clientId \u2014 This is automatically generated if you use the Mantis client library; it defaults to the Mantis Job ID subscriptionId \u2014 This is used as a load-balancing mechanism for clientId A consumer (represented as a client through clientId ) may have many consumer instances (represented as susbcriptions through subscriptionId ). Source Jobs use clientId and subscriptionId to broadcast and/or load balance events to consumers. Source Jobs will broadcast an event to all clientId s. This means that consumer instances with different clientId s will each receive the same event. However, Source Jobs will load balance an event within a clientId . This means that consumer instances with the same clientId but different subscriptionId s are effectively grouped together. Events with the same clientId are load balanced among its subscriptionId s. Example of subscribing to a Source Job\u2019s Sink which outputs the results of a sine wave function: curl \"http:// instance-address : port ?clientId= myId &subscriptionId= mySubscription &criterion=select%20*%20where%20true\" data: {\"x\": 60.000000, \"y\": -3.048106} data: {\"x\": 100.000000, \"y\": -5.063656} data: {\"x\": 26.000000, \"y\": 7.625585} \u22ee Sinks have a pre-processor ( QueryRequestPreProcessor ), a post-processor ( QueryRequestPostProcessor ), and a router: The pre-processor is an RxFunction that registers the consumer\u2019s query, with their clientId and subscriptionId , into an in-memory cache called a QueryRefCountMap when a consumer instance connects to the Sink. This registers queries so that the Source Job can apply them to events as those events are ingested by the Source Job. The post-processor is an RxFunction that de-registers subscriptions from the QueryRefCountMap when a consumer instance disconnects from the Sink. The Source Job removes removes the clientId entirely from the QueryRefCountMap only when all of its subscriptionId s have been removed. The router routes incoming events for a clientId s to its subscriptions. It does this by using a drainer called a ChunkProcessor to drain events from an internal queue on an interval and randomly distribute the events to subscriptions. Note Typically, subscriptions to a Source Job come from other Mantis Jobs. However, because subscriptions are SSE endpoints, you can subscribe to Source Jobs over that same SSE endpoint to view the Job\u2019s output for debugging purposes.","title":"Sink (RxAction)"},{"location":"internals/sourcejobs/#caveats","text":"Source Jobs are single-stage Mantis Jobs that perform projection and filtering operations. MQL queries containing groupBy , orderBy , and window are ignored. These clauses are interpreted into RxJava operations and run by downstream Mantis Jobs. Mantis Publish-based Source Jobs do not autoscale . Autoscaling Mantis Publish-based Source Jobs requires future work to reshuffle connections among all Source Job instances and their upstream Mantis Publish connections.","title":"Caveats"},{"location":"localdevelopment/localdockersetup/","text":"Local Mantis platform development using docker. \u00b6 Prerequisites \u00b6 Install Docker on your local machine (if you don't already have it) + Mac + Windows + Linux Bootstraping the Mantis Cluster in Docker \u00b6 Download the docker-compose file \u00b6 Download the docker-compose-local.yml to a local folder mantis Build a Docker image for Mantis Control Plane (Master) \u00b6 Clone the Mantis Control Plane: $ git clone https://github.com/Netflix/mantis-control-plane.git $ cd mantis-control-plane/ $ ./buildDockerImage.sh Build Docker image for Mantis Worker \u00b6 Clone the main Mantis repository: $ git clone https://github.com/Netflix/mantis.git # Set an environment variable called MANTIS_INSTALL_DIR to the root of this project $ export MANTIS_INSTALL_DIR = /Users/user/mantis Clone the Mantis examples: $ git clone https://github.com/Netflix/mantis-examples.git # Create a sine-function artifact $ cd mantis-examples/ $ ./gradlew clean mantis-examples-sine-function:mantisZipArtifact # Copy to conf/ that is mounted on the mantis worker Docker image $ cp sine-function/build/distributions/mantis-examples-sine-function-0.1.0-SNAPSHOT.zip <path to mantis repo>/localdev/conf/ # Build the Worker docker image $ cd <path to mantis repo>/mantis-server/mantis-server-worker $ ./buildDockerImage.sh Build Docker image for Mantis API \u00b6 Clone the Mantis API project: $ git clone https://github.com/Netflix/mantis-api.git $ cd mantis-api/ $ ./buildDockerImage.sh Start the Mantis cluster \u00b6 $ cd <path to mantis repo> $ docker-compose -f docker-compose-new-master.yml up This starts up the following Docker containers: Zookeeper Mesos Master Mantis Master Mantis API Mesos Slave and Mantis Worker run on a single container (mantisagent) Create and submit the sine-function Job Cluster via CLI \u00b6 $ curl -X POST http://127.0.0.1:8100/api/namedjob/create --silent --data @ $MANTIS_INSTALL_DIR /localdev/conf/namedJob-template -vvv $ curl -X POST http://127.0.0.1:8100/api/submit --silent --data @ $MANTIS_INSTALL_DIR /localdev/conf/submitJob-template -vvv To get a shell on a running container: $ docker exec -it mantis_mantisagent_1 bash # Job logs can be found here. $ cd /tmp/mesos_workdir/slaves/<frameworkid>/frameworks/MantisFramework/executors Using the Mantis UI \u00b6 Clone the Mantis UI project: $ git clone https://github.com/Netflix/mantis-ui.git Run the following commands (in the root directory of this project) to get all dependencies installed and to start the server: $ yarn $ yarn serve Once the node server is up it should print something like App running at: Local: http://localhost:8080/ Point your browser to the above URL and fill out the Registration form as follows Name: Example Email: example@example.com Master Name: Example Mantis API URL: http://localhost:7101 Mesos URL: http://localhost:5050 Click on Create The Mantis Admin page should have no Job clusters currently. Create a new Job Cluster Click on the Create New Job Cluster button on the top right. Specify the cluster name as MySine Click on Upload file and drag and drop the these two files mantis-examples/sine-function/build/distributions/mantis-examples-sine-function-0.1.0-SNAPSHOT.zip mantis-examples/sine-function/build/distributions/mantis-examples-sine-function-0.1.0-SNAPSHOT.json Under the section Stage 1 - Scheduling Information click on Edit and reduce the Disk and Network requirements for this worker to 10 and 12 respectively. Under the section Parameters click on Override Defaults Here you can override values for Job parameters. Let us override the parameter useRandom to true Click Create Job Cluster on the bottom left This will create the job cluster Now let us submit a new Job for our Job Cluster Click the Submit green button this will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us skip all that and scroll directly to the bottom and hit the Submit button on the bottom left. View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585} To teardown the Mantis cluster, issue the following command $ cd <path to mantis repo> $ docker-compose -f docker-compose-new-master.yml down","title":"Local Development"},{"location":"localdevelopment/localdockersetup/#local-mantis-platform-development-using-docker","text":"","title":"Local Mantis platform development using docker."},{"location":"localdevelopment/localdockersetup/#prerequisites","text":"Install Docker on your local machine (if you don't already have it) + Mac + Windows + Linux","title":"Prerequisites"},{"location":"localdevelopment/localdockersetup/#bootstraping-the-mantis-cluster-in-docker","text":"","title":"Bootstraping the Mantis Cluster in Docker"},{"location":"localdevelopment/localdockersetup/#download-the-docker-compose-file","text":"Download the docker-compose-local.yml to a local folder mantis","title":"Download the docker-compose file"},{"location":"localdevelopment/localdockersetup/#build-a-docker-image-for-mantis-control-plane-master","text":"Clone the Mantis Control Plane: $ git clone https://github.com/Netflix/mantis-control-plane.git $ cd mantis-control-plane/ $ ./buildDockerImage.sh","title":"Build a Docker image for Mantis Control Plane (Master)"},{"location":"localdevelopment/localdockersetup/#build-docker-image-for-mantis-worker","text":"Clone the main Mantis repository: $ git clone https://github.com/Netflix/mantis.git # Set an environment variable called MANTIS_INSTALL_DIR to the root of this project $ export MANTIS_INSTALL_DIR = /Users/user/mantis Clone the Mantis examples: $ git clone https://github.com/Netflix/mantis-examples.git # Create a sine-function artifact $ cd mantis-examples/ $ ./gradlew clean mantis-examples-sine-function:mantisZipArtifact # Copy to conf/ that is mounted on the mantis worker Docker image $ cp sine-function/build/distributions/mantis-examples-sine-function-0.1.0-SNAPSHOT.zip <path to mantis repo>/localdev/conf/ # Build the Worker docker image $ cd <path to mantis repo>/mantis-server/mantis-server-worker $ ./buildDockerImage.sh","title":"Build Docker image for Mantis Worker"},{"location":"localdevelopment/localdockersetup/#build-docker-image-for-mantis-api","text":"Clone the Mantis API project: $ git clone https://github.com/Netflix/mantis-api.git $ cd mantis-api/ $ ./buildDockerImage.sh","title":"Build Docker image for Mantis API"},{"location":"localdevelopment/localdockersetup/#start-the-mantis-cluster","text":"$ cd <path to mantis repo> $ docker-compose -f docker-compose-new-master.yml up This starts up the following Docker containers: Zookeeper Mesos Master Mantis Master Mantis API Mesos Slave and Mantis Worker run on a single container (mantisagent)","title":"Start the Mantis cluster"},{"location":"localdevelopment/localdockersetup/#create-and-submit-the-sine-function-job-cluster-via-cli","text":"$ curl -X POST http://127.0.0.1:8100/api/namedjob/create --silent --data @ $MANTIS_INSTALL_DIR /localdev/conf/namedJob-template -vvv $ curl -X POST http://127.0.0.1:8100/api/submit --silent --data @ $MANTIS_INSTALL_DIR /localdev/conf/submitJob-template -vvv To get a shell on a running container: $ docker exec -it mantis_mantisagent_1 bash # Job logs can be found here. $ cd /tmp/mesos_workdir/slaves/<frameworkid>/frameworks/MantisFramework/executors","title":"Create and submit the sine-function Job Cluster via CLI"},{"location":"localdevelopment/localdockersetup/#using-the-mantis-ui","text":"Clone the Mantis UI project: $ git clone https://github.com/Netflix/mantis-ui.git Run the following commands (in the root directory of this project) to get all dependencies installed and to start the server: $ yarn $ yarn serve Once the node server is up it should print something like App running at: Local: http://localhost:8080/ Point your browser to the above URL and fill out the Registration form as follows Name: Example Email: example@example.com Master Name: Example Mantis API URL: http://localhost:7101 Mesos URL: http://localhost:5050 Click on Create The Mantis Admin page should have no Job clusters currently. Create a new Job Cluster Click on the Create New Job Cluster button on the top right. Specify the cluster name as MySine Click on Upload file and drag and drop the these two files mantis-examples/sine-function/build/distributions/mantis-examples-sine-function-0.1.0-SNAPSHOT.zip mantis-examples/sine-function/build/distributions/mantis-examples-sine-function-0.1.0-SNAPSHOT.json Under the section Stage 1 - Scheduling Information click on Edit and reduce the Disk and Network requirements for this worker to 10 and 12 respectively. Under the section Parameters click on Override Defaults Here you can override values for Job parameters. Let us override the parameter useRandom to true Click Create Job Cluster on the bottom left This will create the job cluster Now let us submit a new Job for our Job Cluster Click the Submit green button this will open up a submit screen that will allow you to override Resource configurations as well as parameter values. Let us skip all that and scroll directly to the bottom and hit the Submit button on the bottom left. View output of the job If all goes well your job would go into Launched state. Scroll to the bottom and in the Job Output section click on Start You should see output of the Sine function job being streamed below Oct 4 2019, 03:55:39.338 PM - {\"x\": 26.000000, \"y\": 7.625585} To teardown the Mantis cluster, issue the following command $ cd <path to mantis repo> $ docker-compose -f docker-compose-new-master.yml down","title":"Using the Mantis UI"},{"location":"mantisapi/","text":"In addition to the Mantis UI, there is a REST API with which you can maintain Mantis Jobs and Job Clusters . This page describes the open source version of the Mantis REST API. You can use the Mantis REST API to submit Jobs based on existing Job Cluster or to connect to the output of running Jobs. It is easier to set up or update new Job Clusters by using the Mantis UI, but you can also do this with the Mantis REST API. Response Content Type Mantis API endpoints always return JSON, and do not respect content-type headers in API requests. Summary of REST API \u00b6 Cluster APIs \u00b6 endpoint verb purpose /api/v1/jobClusters GET Return a list of Mantis clusters . /api/v1/jobClusters POST Create a new cluster. /api/v1/jobClusters/ clusterName GET Return information about a single cluster by name. /api/v1/jobClusters/ clusterName PUT Update the information about a particular cluster. /api/v1/jobClusters/ clusterName DELETE Permanently delete a paticular cluster. /api/v1/jobClusters/ clusterName /actions/updateArtifact POST Update the job cluster artifact and optionally resubmit the job . /api/v1/jobClusters/ clusterName /actions/updateSla POST Update cluster SLA information. /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy POST Update the cluster migration strategy . /api/v1/jobClusters/ clusterName /actions/updateLabel POST Update cluster labels . /api/v1/jobClusters/ clusterName /actions/enableCluster POST Enable a disabled cluster. /api/v1/jobClusters/ clusterName /actions/disableCluster POST Disable a cluster. /api/v1/mantis/publish/streamJobClusterMap GET Return a mapping of Mantis Publish push-based streams to clusters. Job APIs \u00b6 endpoint verb purpose /api/v1/jobs GET Return a list of jobs. /api/v1/jobClusters/ clusterName /jobs GET Return a list of jobs for a particular cluster. /api/v1/jobs/ jobID GET Return information about a particular job. /api/v1/jobClusters/ clusterName /jobs/ jobID GET Return information about a particular job. /api/v1/jobs/ jobID DELETE Permanently kill a particular job. /api/v1/jobClusters/ clusterName /jobs POST Submit a new job. /api/v1/jobs/actions/quickSubmit POST Update a job cluster and submit a new job at the same time. /api/v1/jobs/ jobID /actions/postJobStatus POST Post job heartbeat status. /api/v1/jobs/ jobID /actions/scaleStage POST Horizontally scale a stage. /api/v1/jobs/ jobID /actions/resubmitWorker POST Resubmit a worker . Admin APIs \u00b6 endpoint verb purpose /api/v1/masterInfo GET Return Job Master information. /api/v1/masterConfigs GET Return Job Master configs. /api/v1/agentClusters/ GET Get information about active agent clusters. /api/v1/agentClusters/ POST Activate or deactivate an agent cluster. /api/v1/agentClusters/jobs GET Get jobs and host information for an agent cluster. /api/v1/agentClusters/autoScalePolicy GET Retrieve the Agent Cluster Scaling Policy. Streaming WebSocket/SSE APIs \u00b6 endpoint verb purpose ws:// masterHost :7101/api/v1/jobStatusStream/ jobID n/a Stream Job Status Changes. /api/v1/jobDiscoveryStream/ jobID ( SSE ) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobs/schedulingInfo/ jobID (SSE) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobClusters/discoveryInfoStream/ clusterName (SSE) GET Return streaming (SSE) discovery info for the given job cluster. /api/v1/lastSubmittedJobIdStream/ clusterName (SSE) GET Return streaming (SSE) job information for a particular cluster. /api/v1/jobConnectbyid/ jobID (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobConnectbyname/ jobName (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobsubmitandconnect (SSE) POST Submit a job, collect messages from the job sinks, and merge them into a single websocket or SSE stream. Cluster Tasks \u00b6 TBD Get a List of Clusters \u00b6 /api/v1/jobClusters ( GET ) To retrieve a list of JSON objects that include details about the available Job Clusters, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. For example ?fields=name . offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. matching (optional) You can set this to a regular expression, and Mantis will filter the list of Job Clusters on the server side, and will return only those that match this expression. Example Response Format GET /api/v1/jobClusters?pageSize=5&fields=name&sortBy=name&ascending=false { \"list\": [{\"name\":\"jschmoeSLATest\"}, {\"name\":\"sinefn\"}, {\"name\":\"Validation_ZV93BAWR2\"}, {\"name\":\"Validation_Z8NB06L1A\"}, {\"name\":\"Validation_Y828QAI01\"}], \"prev\":null, \"next\":\"/api/v1/jobClusters?pageSize=5&fields=name&sortBy=name&ascending=false&offset=5\" } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Create a New Cluster \u00b6 /api/v1/jobClusters ( POST ) Before you submit a Mantis Job , you must first have set up a Job Cluster . A Job Cluster contains a unique name for the Job, a URL of the Job\u2019s .jar or .zip artifact file, resource requirements to run your Job, and other optional information such as SLA values for minimum and maximum Jobs to keep active for this Cluster, or a cron-based schedule to launch a Job for this Cluster. Each new Job can be considered as an instance of the Job Cluster, and is given a unique ID by appending a number suffix to the Cluster name. The Job inherits the resource requirements from the Cluster unless whoever submits the Job overrides these at submit time. A Job Cluster name must match this regular expression: ^[A-Za-z]+[A-Za-z0-9+-_=:;]* To create a new Job Cluster, issue a POST command to the Mantis REST API endpoint /api/v1/jobClusters with a request body that matches format of the following example: Example Request Body { \"jobDefinition\": { \"name\": \"jschmoe_validation\", \"user\": \"jschmoe\", \"jobJarFileLocation\": \"https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"version\": \"0.2.9 2019-03-19 17:01:36\", \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": \"1\", \"machineDefinition\": { \"cpuCores\": \"1\", \"memoryMB\": \"1024\", \"diskMB\": \"1024\", \"networkMbps\": \"128\", \"numPorts\": \"1\" }, \"scalable\": false, \"softConstraints\": [], \"hardConstraints\": [] } } }, \"parameters\": [], \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"mantis-examples-sine-function\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"0.2.9\" } ], \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\" }, \"slaMin\": \"0\", \"slaMax\": \"0\", \"cronSpec\": null, \"cronPolicy\": \"KEEP_EXISTING\" }, \"owner\": { \"contactEmail\": \"jschmoe@netflix.com\", \"description\": \"\", \"name\": \"Joe Schmoe\", \"repo\": \"\", \"teamName\": \"\" } } Example Response Format { \"name\": \"jschmoe_validation1\", \"jars\": [ { \"url\": \"https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"uploadedAt\": 1553040262171, \"version\": \"0.2.9 2019-03-19 17:01:36\", \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 1, \"memoryMB\": 1024, \"networkMbps\": 128, \"diskMB\": 1024, \"numPorts\": 1 }, \"hardConstraints\": [], \"softConstraints\": [], \"scalingPolicy\": null, \"scalable\": false } } } } ], \"sla\": { \"min\": 0, \"max\": 0, \"cronSpec\": null, \"cronPolicy\": null }, \"parameters\": [], \"owner\": { \"name\": \"Joe Schmoe\", \"teamName\": \"\", \"description\": \"\", \"contactEmail\": \"jschmoe@netflix.com\", \"repo\": \"\" }, \"lastJobCount\": 0, \"disabled\": false, \"isReadyForJobMaster\": false, \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\" }, \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"mantis-examples-sine-function\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"0.2.9\" } ], \"cronActive\": false, \"latestVersion\": \"0.2.9 2019-03-19 17:01:36\" } Possible Response Codes Response Code Reason 201 normal response 405 incorrect HTTP verb (use POST instead) 409 cluster name already exists 500 unknown server error Setting Jobs to Launch at Particular Times \u00b6 You can use the cronSpec field in the body of this request to specify when to launch the Jobs in the Cluster. By default this is blank ( \"\" ). If you set cronSpec to a non-blank value, this also sets the min and max values for the Job Cluster to 0 and 1 respectively. That is to say, you can have no more than one Job running at any one time for that Cluster. Optionally, you can provide a policy ( cronpolicy ) to use when a cron trigger fires while a previosuly submitted Job for the Job Cluster is still running. The possible policy values are KEEP_EXISTING (do not replace the current Job) and KEEP_NEW (replace the current Job with a new one). The default policy is KEEP_EXISTING . Note If the Mantis Master is down during a time window when cron would normally have fired, that cron trigger time window is lost. Mantis does not check for this upon restart. The next cron trigger will resume normally. Get Information about a Cluster \u00b6 /api/v1/jobClusters/ clusterName ( GET ) To retrieve a JSON object that includes details about a Job Cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. Example Response Format { \"name\": \"jschmoe_validation1\", \"jars\": [ { \"url\": \"https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"uploadedAt\": 1553040262171, \"version\": \"0.2.9 2019-03-19 17:01:36\", \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 1, \"memoryMB\": 1024, \"networkMbps\": 128, \"diskMB\": 1024, \"numPorts\": 1 }, \"hardConstraints\": [], \"softConstraints\": [], \"scalingPolicy\": null, \"scalable\": false } } } } ], \"sla\": { \"min\": 0, \"max\": 0, \"cronSpec\": null, \"cronPolicy\": null }, \"parameters\": [], \"owner\": { \"name\": \"Joe Schmoe\", \"teamName\": \"\", \"description\": \"\", \"contactEmail\": \"jschmoe@netflix.com\", \"repo\": \"\" }, \"lastJobCount\": 0, \"disabled\": false, \"isReadyForJobMaster\": false, \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\" }, \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"mantis-examples-sine-function\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"0.2.9\" } ], \"cronActive\": false, \"latestVersion\": \"0.2.9 2019-03-19 17:01:36\" } Possible Response Codes Response Code Reason 200 normal response 404 no cluster with that cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Change Information about a Cluster \u00b6 /api/v1/jobClusters/ clusterName ( PUT ) To update an existing Job Cluster , send a PUT command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName with the same sort of body described above in the case of a Job Cluster create operation. Increment the version number so as to differentiate your new Cluster from the previous Job artifacts . If you try to update an existing Job Cluster by reusing the version number of an existing one, the operation will fail. Example Request Body { \"jobDefinition\": { \"name\": \"Validation_jschmoe\", \"user\": \"validator\", \"jobJarFileLocation\": \"https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"parameters\": [ { \"name\": \"useRandom\", \"value\": false } ], \"schedulingInfo\": { \"stages\": { \"0\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 2, \"memoryMB\": 4096, \"diskMB\": 10, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false }, \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 2, \"memoryMB\": 4096, \"diskMB\": 10, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false } } }, \"slaMin\": 0, \"slaMax\": 0, \"cronSpec\": null, \"cronPolicy\": \"KEEP_EXISTING\", \"migrationConfig\": { \"configString\": \"{\\\"percentToMove\\\":60, \\\"intervalMs\\\":30000}\", \"strategy\": \"PERCENTAGE\" } }, \"owner\": { \"name\": \"validator\", \"teamName\": \"Mantis\", \"description\": \"integration validator\", \"contactEmail\": \"mantisteam@netflix.com\" } } Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 400 client failure 404 no existing cluster with that cluster name was found 405 incorrect HTTP verb (use PUT instead) 500 unknown server error Delete a Cluster \u00b6 /api/v1/jobClusters/ clusterName ( DELETE ) To permanently delete an existing Job Cluster , send a DELETE command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose user (required) Must match the original user in the cluster payload. Example Response Format TBD Possible Response Codes Response Code Reason 202 normal response: asynchronous delete has been scheduled 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error Update a Cluster\u2019s Artifacts \u00b6 /api/v1/jobClusters/ clusterName /actions/updateArtifact ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. Mantis copies the scheduling information, Job parameters , and so forth, from the last Job submitted. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateArtifact with a body that matches the format of the following example: Example Request Body { \"name\": \"ValidatorDemo\", \"version\": \"0.0.1 2019-02-06 11:30:49\", \"url\": \"mantis-artifacts/demo-0.0.1-dev201901231434.zip\", \"skipsubmit\": false, \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error You will receive in the response the Job ID of the newly submitted Job (unless you set skipsubmit to true in the request body, in which case no such Job will be created). Update a Cluster\u2019s SLA \u00b6 /api/v1/jobClusters/ clusterName /actions/updateSla ( POST ) To update the SLA of a Job Cluster without having to submit a new version, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateSla with a body that matches the format of the following example: Example Request Body { \"user\": \"YourUserName\", \"name\": \"Foo\", \"min\": 0, \"max\": 2, \"cronspec\": \"5 * * * * ?\", \"cronpolicy\": \"KEEP_EXISTING\", \"forceenable\": true } The fields of this body are as follows: SLA Field Purpose user (required) name of the user calling this endpoint name (required) name of the Job Cluster min minimum number of Jobs to keep running max maximum number of Jobs to allow running simultaneously cronspec cron specification, see below for format and examples cronpolicy either KEEP_EXISTING or KEEP_NEW (see above for details) forceenable either true or false ; reenable the Job Cluster if it is in the disabled state Note While min , max , cronspec , cronpolicy , and forceenable are all optional fields, you should provide at minimum either cronspec or the combination of min & max . The cron specification string is defined by Quartz CronTrigger . Here are some examples: Examples of cronspec Values example cronspec value resulting job trigger time \"0 0 12 * * ?\" Fire at 12 p.m. (noon) every day. \"0 15 10 ? * *\" Fire at 10:15 a.m. every day. \"0 15 10 * * ?\" Fire at 10:15 a.m. every day. \"0 0-5 14 * * ?\" Fire every minute starting at 2 p.m. and ending at 2:05 p.m., every day. Scheduling information for Jobs launched by means of cron triggers is inherited from the scheduling information for the Job Cluster. Warning If a Job takes required parameters , the Job will not launch successfully if the Job Cluster does not establish defaults for those parameters. A Job launched by means of a cron trigger always uses these default parameters to launch the Job. If you provide an invalid cron specification, this will disable the Job Cluster. To fix this, when you reformulate your cron specification, also set forceenable to \"true\" in the body that you send via POST to /api/v2/jobClusters/ clusterName /actions/updateSla . Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Example: Creating a Job Cluster with a cron Specification The following POST body to /api/v2/jobClusters/ clusterName /actions/updateSla would specify Jobs that are launched based on a timed schedule: { \"jobDefinition\": { \"name\": \"Foo\", \"user\": \"YourUserName\", \"version\": \"1.0\", \"parameters\": [ { \"name\": \"param1\", \"value\": \"value1\" }, { \"name\": \"param2\", \"value\": \"value2\" } ], \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 2, \"memoryMB\": 4096, \"diskMB\": 10 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false } } }, \"slaMin\": 0, \"slaMax\": 0, \"cronSpec\": \"2 * * * * ?\", \"cronPolicy\":\"KEEP_EXISTING\", \"jobJarFileLocation\": \"http://www.jobjars.com/foo\" }, \"owner\": { \"name\": \"MyName\", \"teamName\": \"myTeam\", \"description\": \"description\", \"contactEmail\": \"email@company.com\", \"repo\": \"http://repos.com/myproject.git\" } } Update a Cluster\u2019s Migration Strategy \u00b6 /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy ( POST ) You can quickly update the migration strategy of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy with a body that matches the format of the following example: Example Request Body { \"name\": \"NameOfJobCluster\", \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":10, \\\"intervalMs\\\":1000}\" }, \"user\": \"YourUserName\" } You will receive in the response the migration strategy config that you have updated the Job Cluster to. Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Update a Cluster\u2019s Labels \u00b6 /api/v1/jobClusters/ clusterName /actions/updateLabel ( POST ) You can quickly update the labels of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateLabel with a body that matches the format of the following example: Example Request Body { \"name\": \"SPaaSBackpressureDemp\", \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"backpressure-demo-aggregator-0.0.1\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"dev201901231434\" }, { \"name\": \"_mantis.jobType\", \"value\": \"aggregator\" }, { \"name\": \"_mantis.criticality\", \"value\": \"medium\" }, { \"name\": \"myTestLabel\", \"value\": \"bingo\" } ], \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Enable a Cluster \u00b6 /api/v1/jobClusters/ clusterName /actions/enableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=true without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/enableCluster with a body that matches the format of the following example: Example Request Body { \"name\": \"SPaaSBackpressureDemp\", \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Disable a Cluster \u00b6 /api/v1/jobClusters/ clusterName /actions/disableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=false without having to update the entirety of the Cluster definition. When you disable a Job Cluster Mantis will not allow new Job submissions under that Cluster and it will terminate any Jobs from that Cluster that are currently running. Mantis will also stop enforcing the SLA requirements for the Cluster, including any cron setup that would otherwise launch new Jobs. This is useful when a Job Cluster must be temporarily made inactive, for instance if you have determined that there is a problem with it. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/disableCluster with a body that matches the format of the following example: Example Request Body { \"name\": \"SPaaSBackpressureDemp\", \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Get a Map of Mantis Publish Push-Based Streams to Clusters \u00b6 /api/v1/mantis/publish/streamJobClusterMap ( GET ) describe Example Response Format { \"version\": \"1\", \"timestamp\": 2, \"mappings\": { \"__default__\": { \"requestEventStream\": \"SharedMantisPublishEventSource\", \"__default__\": \"SharedMantisPublishEventSource\" } } } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Job Tasks \u00b6 TBD Get a List of Jobs \u00b6 /api/v1/jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs , issue a GET command to the Mantis REST API endpoint /api/v1/jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . matching (optional) The cluster name. Set this to a regex string. You can also use the /api/v1/jobClusters/ clusterName /jobs endpoint if you mean to match a specific cluster name and do not need the flexibility of a regex filter. stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Information About a Particular Job \u00b6 /api/v1/jobs/ jobID ( GET ) /api/v1/jobClusters/ clusterName /jobs/ jobID ( GET ) To retrieve a JSON record of a particular Job , issue a GET command to the Mantis REST API endpoint /api/v1/jobs/ jobID or /api/v1/jobClusters/ clusterName /jobs/ jobID . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. archived (optional) By default only information about an active job will be returned. Set this to true if you want information about the job returned even if it is an archived inactive job. Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Kill a Job \u00b6 /api/v1/jobs/ jobID ( DELETE ) To permanently kill a particular Job , issue a DELETE command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID . Query Parameters Query Parameter Purpose reason (required) Specify why you are killing this job. user (required) Specify which user is initiating this request. Example Response Format TBD Possible Response Codes Response Code Reason 202 the kill request has been accepted and is being processed asynchronously 404 no job with that ID was found 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error List the Archived Workers for a Job \u00b6 /api/v1/jobs/ jobID /archivedWorkers ( GET ) To list all of the archived workers for a particular Job , issue a GET command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID /archivedWorkers . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get a List of Jobs for a Particular Cluster \u00b6 /api/v1/jobClusters/ clusterName /jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs in a particular cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Submit a New Job for a Particular Cluster \u00b6 /api/v1/jobClusters/ clusterName /jobs ( POST ) To submit a new Job based on a Job Cluster , issue a POST command to the /api/v1/jobClusters/ clusterName /jobs endpoint with a request body like the following: Example Request Body { \"name\": \"myClusterName\", \"user\": \"jschmoe\", \"jobJarFileLocation\": null, \"version\": \"0.0.1 2019-02-06 13:30:07\", \"subscriptionTimeoutSecs\": 0, \"jobSla\": { \"runtimeLimitSecs\": \"0\", \"slaType\": \"Lossy\", \"durationType\": \"Perpetual\", \"userProvidedType\": \"\" }, \"schedulingInfo\": { \"stages\": { \"0\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 0.35, \"memoryMB\": 600, \"networkMbps\": 30, \"diskMB\": 100, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false }, \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 0.35, \"memoryMB\": 600, \"networkMbps\": 30, \"diskMB\": 100, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": true } } }, \"parameters\": [ { \"name\": \"criterion\", \"value\": \"mock\" }, { \"name\": \"sourceJobName\", \"value\": \"RequestSource\" }, { \"name\": \"spaasJobId\", \"value\": \"spaasjschmoe-clsessionizer_backpressuretest\" } ], \"isReadyForJobMaster\": false } Example Response Format (Same as \"Get Information about a Job\" below) Possible Response Codes Response Code Reason 201 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Update a Job Cluster and Submit a New Job at the Same Time \u00b6 /api/v1/jobs/actions/quickSubmit ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobs/actions/quickSubmit with a body that matches the format of the following example: Example Request Body { \"name\": \"NameOfJobCluster\", \"user\": \"myusername\", \"jobSla\": { \"durationType\": \"Perpetual\", \"runtimeLimitSecs\": \"0\", \"minRuntimeSecs\": \"0\", \"userProvidedType\": \"\" } } Example Response Format You will receive in the response the Job ID of the newly submitted Job. sample response body? Possible Response Codes Response Code Reason 201 normal response 404 no cluster was found with a cluster name matching the value of name from the request body 405 incorrect HTTP verb (use POST instead) 500 unknown server error Post Job Heartbeat Status \u00b6 /api/v1/jobs/actions/postJobStatus ( POST ) Query Parameters TBD Example Request Body { \"jobId\": \"sine-function-1\", \"status\": { \"jobId\": \"sine-function-1\", \"stageNum\": 1, \"workerIndex\": 0, \"workerNumber\": 2, \"type\": \"HEARTBEAT\", \"message\": \"heartbeat\", \"state\": \"Noop\", \"hostname\": null, \"timestamp\": 1525813363585, \"reason\": \"Normal\", \"payloads\": [ { \"type\": \"SubscriptionState\", \"data\": \"false\" }, { \"type\": \"IncomingDataDrop\", \"data\": \"{\\\"onNextCount\\\":0,\\\"droppedCount\\\":0}\" } ] } } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 405 incorrect HTTP verb (use POST instead) 500 unknown server error Horizontally Scale a Stage \u00b6 /api/v1/jobs/ jobID /actions/scaleStage ( POST ) To manually scale a Job Processing Stage , that is, to alter the number of workers assigned to that stage, send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/scaleStage with a request body in the following format: Note You can only manually scale a Processing Stage if the Job was submitted with the scalable flag turned on. Example Request Body { \"JobId\": \"ValidatorDemo-33\", \"StageNumber\": 1, \"NumWorkers\": 3 } NumWorkers here is the number of workers you want to be assigned to the stage after the scaling action takes place (that is, it is not the delta by which you want to change the number of workers in the stage). Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Resubmit a Worker \u00b6 /api/v1/jobs/ jobID /actions/resubmitWorker ( POST ) To resubmit a particular worker for a particular Job , send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/resubmitWorker with a request body resembling the following: Example Request Body { \"user\": \"jschmoe\", \"workerNumber\": 5, \"reason\": \"test worker resubmit\" } Note workerNumber is the worker number not the worker index . Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Administrative Tasks \u00b6 TBD Return Job Master Information \u00b6 /api/v1/masterInfo ( GET ) TBD Example Response Body { \"hostname\": \"100.86.121.198\", \"hostIP\": \"100.86.121.198\", \"apiPort\": 7101, \"schedInfoPort\": 7101, \"apiPortV2\": 7075, \"apiStatusUri\": \"api/v1/jobs/actions/postJobStatus\", \"consolePort\": 7101, \"createTime\": 1548803881867, \"fullApiStatusUri\": \"http://100.86.121.198:7101/api/v1/jobs/actions/postJobStatus\" } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Return Job Master Configs \u00b6 /api/v1/masterConfigs ( GET ) TBD Example Response Body [ { \"name\": \"JobConstraints\", \"value\": \"[\\\"UniqueHost\\\",\\\"ExclusiveHost\\\",\\\"ZoneBalance\\\",\\\"M4Cluster\\\",\\\"M3Cluster\\\",\\\"M5Cluster\\\"]\" }, { \"name\": \"ScalingReason\", \"value\": \"[\\\"CPU\\\",\\\"Memory\\\",\\\"Network\\\",\\\"DataDrop\\\",\\\"KafkaLag\\\",\\\"UserDefined\\\",\\\"KafkaProcessed\\\"]\" }, { \"name\": \"MigrationStrategyEnum\", \"value\": \"[\\\"ONE_WORKER\\\",\\\"PERCENTAGE\\\"]\" }, { \"name\": \"WorkerResourceLimits\", \"value\": \"{\\\"maxCpuCores\\\":8,\\\"maxMemoryMB\\\":28000,\\\"maxNetworkMbps\\\":1024}\" } ] Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Information about Agent Clusters \u00b6 /api/v1/agentClusters/ ( GET ) Returns information about active agent clusters (agent clusters are physical AWS resources that Mantis connects to). Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Activate or Deactivate an Agent Cluster \u00b6 /api/v1/agentClusters/ ( POST ) TBD Example Request Body [\"mantisagent-staging-cl1-m5.2xlarge-1-v00\"] (an array of active clusters) Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 400 client failure 405 incorrect HTTP verb (use POST instead) 500 unknown server error Retrieve the Agent Cluster Scaling Policy \u00b6 /api/v1/agentClusters/autoScalePolicy ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Jobs and Host Information for an Agent Cluster \u00b6 /api/v1/agentClusters/jobs ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Streaming WebSocket/SSE Tasks \u00b6 TBD: introduction Stream Job Status Changes \u00b6 ws:// masterHost :7101/api/v1/jobStatusStream/ jobID TBD: description See also: mantisapi/websocket/ Example Response Stream Excerpt { \"status\": { \"jobId\": \"SPaaSBackpressureDemp-26\", \"stageNum\": 1, \"workerIndex\": 0, \"workerNumber\": 7, \"type\": \"INFO\", \"message\": \"SPaaSBackpressureDemp-26-worker-0-7 worker status update\", \"state\": \"StartInitiated\", \"hostname\": null, \"timestamp\": 1549404217065, \"reason\": \"Normal\", \"payloads\": [] } } Stream Scheduling Information for a Job \u00b6 /api/v1/jobDiscoveryStream/ jobID ( GET ) /api/v1/jobs/schedulingInfo/ jobID ( GET ) To retrieve an SSE stream of scheduling information for a particular job, send an HTTP GET command to either the Mantis REST API endpoint /api/v1/jobDiscoveryStream/ jobID or /api/v1/jobs/schedulingInfo/ jobID . Query Parameters jobDiscoveryStream Query Parameter Purpose sendHB (optional) Indicate whether or not to send heartbeats (default= false ). jobs/schedulingInfo Query Parameter Purpose jobId (required) The job ID of the job for which scheduling information is to be streamed. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Get Streaming (SSE) Discovery Info for a Cluster \u00b6 /api/v1/jobClusters/discoveryInfoStream/ clusterName ( GET ) describe Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error Stream Job Information for a Cluster \u00b6 /api/v1/lastSubmittedJobIdStream/ clusterName ( GET ) To retrieve an SSE stream of job information for a particular cluster, send an HTTP GET command to the Mantis REST API endpoint /api/v1/lastSubmittedJobIdStream/ clusterName . Query Parameters Query Parameter Purpose sendHB Indicate whether or not to send heartbeats. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Cluster with that Cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error Stream Job Sink Messages \u00b6 /api/v1/jobConnectbyid/ jobID (SSE) /api/v1/jobConnectbyname/ jobName (SSE) Collect messages from the job sinks and merge them into a single websocket or SSE stream. Example Response Stream Excerpt data: {\"x\": 928400.000000, \"y\": 3.139934} data: {\"x\": 928402.000000, \"y\": -9.939772} data: {\"x\": 928404.000000, \"y\": 5.132876} data: {\"x\": 928406.000000, \"y\": 5.667712} Submit Job and Stream Job Sink Messages \u00b6 /api/v1/jobsubmitandconnect ( POST ) To submit a job and then collect messages from the job sinks and merge them into a single websocket or SSE stream, send a POST request to /api/v1/jobsubmitandconnect with a request body that describes the job. Example Request Body { \"name\": \"ValidatorDemo\", \"user\": \"jschmoe\", \"jobSla\": { \"durationType\": \"Perpetual\", \"runtimeLimitSecs\": \"0\", \"minRuntimeSecs\": \"0\", \"userProvidedType\": \"\" } } Example Response Stream Excerpt data: {\"x\": 928400.000000, \"y\": 3.139934} data: {\"x\": 928402.000000, \"y\": -9.939772} data: {\"x\": 928404.000000, \"y\": 5.132876} data: {\"x\": 928406.000000, \"y\": 5.667712} Pagination \u00b6 You can configure some endpoints to return their responses in pages . This is to say that rather than returning all of the data responsive to a request all at once, the endpoint can return a specific subset of the data at a time. An endpoint that supports pagination will typically do so by means of two query parameters: Query Parameter Purpose pageSize the maximum number of records to return in this request (default = 0, which means all records) offset the record number to begin with in the set of records to return in this request So, for example, you might make consecutive requests for\u2026 endpoint ?pageSize=10&offset=0 endpoint ?pageSize=10&offset=10 endpoint ?pageSize=10&offset=20 \u2026and so forth, until you reach the final page of records. When you request records in a paginated fashion like this, the Mantis API will enclose them in a JSON structure like the following: { \"list\": [ { \"Id\": originalOffset , \u22ee }, { \"Id\": originalOffset+1 , \u22ee }, \u22ee ], \"prev\": \"/api/v1/ endpoint ?pageSize= pageSize &offset= originalOffset+pageSize \", \"next\": \"/api/v1/ endpoint ?pageSize= pageSize &offset= originalOffset\u2212pageSize \" } For example: { \"list\" : [ { \"Id\" : 101 , \u22ee }, { \"Id\" : 102 , \u22ee } ], \"next\" : \"/api/v1/someResource?pageSize=20&offset=120\" , \"prev\" : \"/api/v1/someResource?pageSize=20&offset=80\" } prev and/or next will be null if there is no previous or next page, that is, if you are at the first and/or last page in the pagination.","title":"Mantis API Reference"},{"location":"mantisapi/#summary-of-rest-api","text":"","title":"Summary of REST API"},{"location":"mantisapi/#cluster-apis","text":"endpoint verb purpose /api/v1/jobClusters GET Return a list of Mantis clusters . /api/v1/jobClusters POST Create a new cluster. /api/v1/jobClusters/ clusterName GET Return information about a single cluster by name. /api/v1/jobClusters/ clusterName PUT Update the information about a particular cluster. /api/v1/jobClusters/ clusterName DELETE Permanently delete a paticular cluster. /api/v1/jobClusters/ clusterName /actions/updateArtifact POST Update the job cluster artifact and optionally resubmit the job . /api/v1/jobClusters/ clusterName /actions/updateSla POST Update cluster SLA information. /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy POST Update the cluster migration strategy . /api/v1/jobClusters/ clusterName /actions/updateLabel POST Update cluster labels . /api/v1/jobClusters/ clusterName /actions/enableCluster POST Enable a disabled cluster. /api/v1/jobClusters/ clusterName /actions/disableCluster POST Disable a cluster. /api/v1/mantis/publish/streamJobClusterMap GET Return a mapping of Mantis Publish push-based streams to clusters.","title":"Cluster APIs"},{"location":"mantisapi/#job-apis","text":"endpoint verb purpose /api/v1/jobs GET Return a list of jobs. /api/v1/jobClusters/ clusterName /jobs GET Return a list of jobs for a particular cluster. /api/v1/jobs/ jobID GET Return information about a particular job. /api/v1/jobClusters/ clusterName /jobs/ jobID GET Return information about a particular job. /api/v1/jobs/ jobID DELETE Permanently kill a particular job. /api/v1/jobClusters/ clusterName /jobs POST Submit a new job. /api/v1/jobs/actions/quickSubmit POST Update a job cluster and submit a new job at the same time. /api/v1/jobs/ jobID /actions/postJobStatus POST Post job heartbeat status. /api/v1/jobs/ jobID /actions/scaleStage POST Horizontally scale a stage. /api/v1/jobs/ jobID /actions/resubmitWorker POST Resubmit a worker .","title":"Job APIs"},{"location":"mantisapi/#admin-apis","text":"endpoint verb purpose /api/v1/masterInfo GET Return Job Master information. /api/v1/masterConfigs GET Return Job Master configs. /api/v1/agentClusters/ GET Get information about active agent clusters. /api/v1/agentClusters/ POST Activate or deactivate an agent cluster. /api/v1/agentClusters/jobs GET Get jobs and host information for an agent cluster. /api/v1/agentClusters/autoScalePolicy GET Retrieve the Agent Cluster Scaling Policy.","title":"Admin APIs"},{"location":"mantisapi/#streaming-websocketsse-apis","text":"endpoint verb purpose ws:// masterHost :7101/api/v1/jobStatusStream/ jobID n/a Stream Job Status Changes. /api/v1/jobDiscoveryStream/ jobID ( SSE ) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobs/schedulingInfo/ jobID (SSE) GET Return streaming (SSE) scheduling information for a particular job. /api/v1/jobClusters/discoveryInfoStream/ clusterName (SSE) GET Return streaming (SSE) discovery info for the given job cluster. /api/v1/lastSubmittedJobIdStream/ clusterName (SSE) GET Return streaming (SSE) job information for a particular cluster. /api/v1/jobConnectbyid/ jobID (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobConnectbyname/ jobName (SSE) n/a Collect messages from the job sinks and merge them into a single websocket or SSE stream. /api/v1/jobsubmitandconnect (SSE) POST Submit a job, collect messages from the job sinks, and merge them into a single websocket or SSE stream.","title":"Streaming WebSocket/SSE APIs"},{"location":"mantisapi/#cluster-tasks","text":"TBD","title":"Cluster Tasks"},{"location":"mantisapi/#get-a-list-of-clusters","text":"/api/v1/jobClusters ( GET ) To retrieve a list of JSON objects that include details about the available Job Clusters, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. For example ?fields=name . offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. matching (optional) You can set this to a regular expression, and Mantis will filter the list of Job Clusters on the server side, and will return only those that match this expression. Example Response Format GET /api/v1/jobClusters?pageSize=5&fields=name&sortBy=name&ascending=false { \"list\": [{\"name\":\"jschmoeSLATest\"}, {\"name\":\"sinefn\"}, {\"name\":\"Validation_ZV93BAWR2\"}, {\"name\":\"Validation_Z8NB06L1A\"}, {\"name\":\"Validation_Y828QAI01\"}], \"prev\":null, \"next\":\"/api/v1/jobClusters?pageSize=5&fields=name&sortBy=name&ascending=false&offset=5\" } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a List of Clusters"},{"location":"mantisapi/#create-a-new-cluster","text":"/api/v1/jobClusters ( POST ) Before you submit a Mantis Job , you must first have set up a Job Cluster . A Job Cluster contains a unique name for the Job, a URL of the Job\u2019s .jar or .zip artifact file, resource requirements to run your Job, and other optional information such as SLA values for minimum and maximum Jobs to keep active for this Cluster, or a cron-based schedule to launch a Job for this Cluster. Each new Job can be considered as an instance of the Job Cluster, and is given a unique ID by appending a number suffix to the Cluster name. The Job inherits the resource requirements from the Cluster unless whoever submits the Job overrides these at submit time. A Job Cluster name must match this regular expression: ^[A-Za-z]+[A-Za-z0-9+-_=:;]* To create a new Job Cluster, issue a POST command to the Mantis REST API endpoint /api/v1/jobClusters with a request body that matches format of the following example: Example Request Body { \"jobDefinition\": { \"name\": \"jschmoe_validation\", \"user\": \"jschmoe\", \"jobJarFileLocation\": \"https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"version\": \"0.2.9 2019-03-19 17:01:36\", \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": \"1\", \"machineDefinition\": { \"cpuCores\": \"1\", \"memoryMB\": \"1024\", \"diskMB\": \"1024\", \"networkMbps\": \"128\", \"numPorts\": \"1\" }, \"scalable\": false, \"softConstraints\": [], \"hardConstraints\": [] } } }, \"parameters\": [], \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"mantis-examples-sine-function\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"0.2.9\" } ], \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\" }, \"slaMin\": \"0\", \"slaMax\": \"0\", \"cronSpec\": null, \"cronPolicy\": \"KEEP_EXISTING\" }, \"owner\": { \"contactEmail\": \"jschmoe@netflix.com\", \"description\": \"\", \"name\": \"Joe Schmoe\", \"repo\": \"\", \"teamName\": \"\" } } Example Response Format { \"name\": \"jschmoe_validation1\", \"jars\": [ { \"url\": \"https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"uploadedAt\": 1553040262171, \"version\": \"0.2.9 2019-03-19 17:01:36\", \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 1, \"memoryMB\": 1024, \"networkMbps\": 128, \"diskMB\": 1024, \"numPorts\": 1 }, \"hardConstraints\": [], \"softConstraints\": [], \"scalingPolicy\": null, \"scalable\": false } } } } ], \"sla\": { \"min\": 0, \"max\": 0, \"cronSpec\": null, \"cronPolicy\": null }, \"parameters\": [], \"owner\": { \"name\": \"Joe Schmoe\", \"teamName\": \"\", \"description\": \"\", \"contactEmail\": \"jschmoe@netflix.com\", \"repo\": \"\" }, \"lastJobCount\": 0, \"disabled\": false, \"isReadyForJobMaster\": false, \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\" }, \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"mantis-examples-sine-function\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"0.2.9\" } ], \"cronActive\": false, \"latestVersion\": \"0.2.9 2019-03-19 17:01:36\" } Possible Response Codes Response Code Reason 201 normal response 405 incorrect HTTP verb (use POST instead) 409 cluster name already exists 500 unknown server error","title":"Create a New Cluster"},{"location":"mantisapi/#setting-jobs-to-launch-at-particular-times","text":"You can use the cronSpec field in the body of this request to specify when to launch the Jobs in the Cluster. By default this is blank ( \"\" ). If you set cronSpec to a non-blank value, this also sets the min and max values for the Job Cluster to 0 and 1 respectively. That is to say, you can have no more than one Job running at any one time for that Cluster. Optionally, you can provide a policy ( cronpolicy ) to use when a cron trigger fires while a previosuly submitted Job for the Job Cluster is still running. The possible policy values are KEEP_EXISTING (do not replace the current Job) and KEEP_NEW (replace the current Job with a new one). The default policy is KEEP_EXISTING . Note If the Mantis Master is down during a time window when cron would normally have fired, that cron trigger time window is lost. Mantis does not check for this upon restart. The next cron trigger will resume normally.","title":"Setting Jobs to Launch at Particular Times"},{"location":"mantisapi/#get-information-about-a-cluster","text":"/api/v1/jobClusters/ clusterName ( GET ) To retrieve a JSON object that includes details about a Job Cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields of the payload. Example Response Format { \"name\": \"jschmoe_validation1\", \"jars\": [ { \"url\": \"https://mantis.us-east-1.prod.netflix.net/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"uploadedAt\": 1553040262171, \"version\": \"0.2.9 2019-03-19 17:01:36\", \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 1, \"memoryMB\": 1024, \"networkMbps\": 128, \"diskMB\": 1024, \"numPorts\": 1 }, \"hardConstraints\": [], \"softConstraints\": [], \"scalingPolicy\": null, \"scalable\": false } } } } ], \"sla\": { \"min\": 0, \"max\": 0, \"cronSpec\": null, \"cronPolicy\": null }, \"parameters\": [], \"owner\": { \"name\": \"Joe Schmoe\", \"teamName\": \"\", \"description\": \"\", \"contactEmail\": \"jschmoe@netflix.com\", \"repo\": \"\" }, \"lastJobCount\": 0, \"disabled\": false, \"isReadyForJobMaster\": false, \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":25,\\\"intervalMs\\\":60000}\" }, \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"mantis-examples-sine-function\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"0.2.9\" } ], \"cronActive\": false, \"latestVersion\": \"0.2.9 2019-03-19 17:01:36\" } Possible Response Codes Response Code Reason 200 normal response 404 no cluster with that cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Information about a Cluster"},{"location":"mantisapi/#change-information-about-a-cluster","text":"/api/v1/jobClusters/ clusterName ( PUT ) To update an existing Job Cluster , send a PUT command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName with the same sort of body described above in the case of a Job Cluster create operation. Increment the version number so as to differentiate your new Cluster from the previous Job artifacts . If you try to update an existing Job Cluster by reusing the version number of an existing one, the operation will fail. Example Request Body { \"jobDefinition\": { \"name\": \"Validation_jschmoe\", \"user\": \"validator\", \"jobJarFileLocation\": \"https://some.host/mantis-artifacts/mantis-examples-sine-function-0.2.9.zip\", \"parameters\": [ { \"name\": \"useRandom\", \"value\": false } ], \"schedulingInfo\": { \"stages\": { \"0\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 2, \"memoryMB\": 4096, \"diskMB\": 10, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false }, \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 2, \"memoryMB\": 4096, \"diskMB\": 10, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false } } }, \"slaMin\": 0, \"slaMax\": 0, \"cronSpec\": null, \"cronPolicy\": \"KEEP_EXISTING\", \"migrationConfig\": { \"configString\": \"{\\\"percentToMove\\\":60, \\\"intervalMs\\\":30000}\", \"strategy\": \"PERCENTAGE\" } }, \"owner\": { \"name\": \"validator\", \"teamName\": \"Mantis\", \"description\": \"integration validator\", \"contactEmail\": \"mantisteam@netflix.com\" } } Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 400 client failure 404 no existing cluster with that cluster name was found 405 incorrect HTTP verb (use PUT instead) 500 unknown server error","title":"Change Information about a Cluster"},{"location":"mantisapi/#delete-a-cluster","text":"/api/v1/jobClusters/ clusterName ( DELETE ) To permanently delete an existing Job Cluster , send a DELETE command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName . Query Parameters Query Parameter Purpose user (required) Must match the original user in the cluster payload. Example Response Format TBD Possible Response Codes Response Code Reason 202 normal response: asynchronous delete has been scheduled 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error","title":"Delete a Cluster"},{"location":"mantisapi/#update-a-clusters-artifacts","text":"/api/v1/jobClusters/ clusterName /actions/updateArtifact ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. Mantis copies the scheduling information, Job parameters , and so forth, from the last Job submitted. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateArtifact with a body that matches the format of the following example: Example Request Body { \"name\": \"ValidatorDemo\", \"version\": \"0.0.1 2019-02-06 11:30:49\", \"url\": \"mantis-artifacts/demo-0.0.1-dev201901231434.zip\", \"skipsubmit\": false, \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error You will receive in the response the Job ID of the newly submitted Job (unless you set skipsubmit to true in the request body, in which case no such Job will be created).","title":"Update a Cluster\u2019s Artifacts"},{"location":"mantisapi/#update-a-clusters-sla","text":"/api/v1/jobClusters/ clusterName /actions/updateSla ( POST ) To update the SLA of a Job Cluster without having to submit a new version, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateSla with a body that matches the format of the following example: Example Request Body { \"user\": \"YourUserName\", \"name\": \"Foo\", \"min\": 0, \"max\": 2, \"cronspec\": \"5 * * * * ?\", \"cronpolicy\": \"KEEP_EXISTING\", \"forceenable\": true } The fields of this body are as follows: SLA Field Purpose user (required) name of the user calling this endpoint name (required) name of the Job Cluster min minimum number of Jobs to keep running max maximum number of Jobs to allow running simultaneously cronspec cron specification, see below for format and examples cronpolicy either KEEP_EXISTING or KEEP_NEW (see above for details) forceenable either true or false ; reenable the Job Cluster if it is in the disabled state Note While min , max , cronspec , cronpolicy , and forceenable are all optional fields, you should provide at minimum either cronspec or the combination of min & max . The cron specification string is defined by Quartz CronTrigger . Here are some examples: Examples of cronspec Values example cronspec value resulting job trigger time \"0 0 12 * * ?\" Fire at 12 p.m. (noon) every day. \"0 15 10 ? * *\" Fire at 10:15 a.m. every day. \"0 15 10 * * ?\" Fire at 10:15 a.m. every day. \"0 0-5 14 * * ?\" Fire every minute starting at 2 p.m. and ending at 2:05 p.m., every day. Scheduling information for Jobs launched by means of cron triggers is inherited from the scheduling information for the Job Cluster. Warning If a Job takes required parameters , the Job will not launch successfully if the Job Cluster does not establish defaults for those parameters. A Job launched by means of a cron trigger always uses these default parameters to launch the Job. If you provide an invalid cron specification, this will disable the Job Cluster. To fix this, when you reformulate your cron specification, also set forceenable to \"true\" in the body that you send via POST to /api/v2/jobClusters/ clusterName /actions/updateSla . Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error Example: Creating a Job Cluster with a cron Specification The following POST body to /api/v2/jobClusters/ clusterName /actions/updateSla would specify Jobs that are launched based on a timed schedule: { \"jobDefinition\": { \"name\": \"Foo\", \"user\": \"YourUserName\", \"version\": \"1.0\", \"parameters\": [ { \"name\": \"param1\", \"value\": \"value1\" }, { \"name\": \"param2\", \"value\": \"value2\" } ], \"schedulingInfo\": { \"stages\": { \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 2, \"memoryMB\": 4096, \"diskMB\": 10 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false } } }, \"slaMin\": 0, \"slaMax\": 0, \"cronSpec\": \"2 * * * * ?\", \"cronPolicy\":\"KEEP_EXISTING\", \"jobJarFileLocation\": \"http://www.jobjars.com/foo\" }, \"owner\": { \"name\": \"MyName\", \"teamName\": \"myTeam\", \"description\": \"description\", \"contactEmail\": \"email@company.com\", \"repo\": \"http://repos.com/myproject.git\" } }","title":"Update a Cluster\u2019s SLA"},{"location":"mantisapi/#update-a-clusters-migration-strategy","text":"/api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy ( POST ) You can quickly update the migration strategy of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateMigrationStrategy with a body that matches the format of the following example: Example Request Body { \"name\": \"NameOfJobCluster\", \"migrationConfig\": { \"strategy\": \"PERCENTAGE\", \"configString\": \"{\\\"percentToMove\\\":10, \\\"intervalMs\\\":1000}\" }, \"user\": \"YourUserName\" } You will receive in the response the migration strategy config that you have updated the Job Cluster to. Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Update a Cluster\u2019s Migration Strategy"},{"location":"mantisapi/#update-a-clusters-labels","text":"/api/v1/jobClusters/ clusterName /actions/updateLabel ( POST ) You can quickly update the labels of an existing Job Cluster without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/updateLabel with a body that matches the format of the following example: Example Request Body { \"name\": \"SPaaSBackpressureDemp\", \"labels\": [ { \"name\": \"_mantis.user\", \"value\": \"jschmoe\" }, { \"name\": \"_mantis.ownerEmail\", \"value\": \"jschmoe@netflix.com\" }, { \"name\": \"_mantis.artifact\", \"value\": \"backpressure-demo-aggregator-0.0.1\" }, { \"name\": \"_mantis.artifact.version\", \"value\": \"dev201901231434\" }, { \"name\": \"_mantis.jobType\", \"value\": \"aggregator\" }, { \"name\": \"_mantis.criticality\", \"value\": \"medium\" }, { \"name\": \"myTestLabel\", \"value\": \"bingo\" } ], \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Update a Cluster\u2019s Labels"},{"location":"mantisapi/#enable-a-cluster","text":"/api/v1/jobClusters/ clusterName /actions/enableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=true without having to update the entirety of the Cluster definition. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/enableCluster with a body that matches the format of the following example: Example Request Body { \"name\": \"SPaaSBackpressureDemp\", \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Enable a Cluster"},{"location":"mantisapi/#disable-a-cluster","text":"/api/v1/jobClusters/ clusterName /actions/disableCluster ( POST ) You can quickly change the state of an existing Job Cluster to enabled=false without having to update the entirety of the Cluster definition. When you disable a Job Cluster Mantis will not allow new Job submissions under that Cluster and it will terminate any Jobs from that Cluster that are currently running. Mantis will also stop enforcing the SLA requirements for the Cluster, including any cron setup that would otherwise launch new Jobs. This is useful when a Job Cluster must be temporarily made inactive, for instance if you have determined that there is a problem with it. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /actions/disableCluster with a body that matches the format of the following example: Example Request Body { \"name\": \"SPaaSBackpressureDemp\", \"user\": \"jschmoe\" } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Disable a Cluster"},{"location":"mantisapi/#get-a-map-of-mantis-publish-push-based-streams-to-clusters","text":"/api/v1/mantis/publish/streamJobClusterMap ( GET ) describe Example Response Format { \"version\": \"1\", \"timestamp\": 2, \"mappings\": { \"__default__\": { \"requestEventStream\": \"SharedMantisPublishEventSource\", \"__default__\": \"SharedMantisPublishEventSource\" } } } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a Map of Mantis Publish Push-Based Streams to Clusters"},{"location":"mantisapi/#job-tasks","text":"TBD","title":"Job Tasks"},{"location":"mantisapi/#get-a-list-of-jobs","text":"/api/v1/jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs , issue a GET command to the Mantis REST API endpoint /api/v1/jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . matching (optional) The cluster name. Set this to a regex string. You can also use the /api/v1/jobClusters/ clusterName /jobs endpoint if you mean to match a specific cluster name and do not need the flexibility of a regex filter. stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a List of Jobs"},{"location":"mantisapi/#get-information-about-a-particular-job","text":"/api/v1/jobs/ jobID ( GET ) /api/v1/jobClusters/ clusterName /jobs/ jobID ( GET ) To retrieve a JSON record of a particular Job , issue a GET command to the Mantis REST API endpoint /api/v1/jobs/ jobID or /api/v1/jobClusters/ clusterName /jobs/ jobID . Query Parameters Query Parameter Purpose fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. archived (optional) By default only information about an active job will be returned. Set this to true if you want information about the job returned even if it is an archived inactive job. Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Information About a Particular Job"},{"location":"mantisapi/#kill-a-job","text":"/api/v1/jobs/ jobID ( DELETE ) To permanently kill a particular Job , issue a DELETE command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID . Query Parameters Query Parameter Purpose reason (required) Specify why you are killing this job. user (required) Specify which user is initiating this request. Example Response Format TBD Possible Response Codes Response Code Reason 202 the kill request has been accepted and is being processed asynchronously 404 no job with that ID was found 405 incorrect HTTP verb (use DELETE instead) 500 unknown server error","title":"Kill a Job"},{"location":"mantisapi/#list-the-archived-workers-for-a-job","text":"/api/v1/jobs/ jobID /archivedWorkers ( GET ) To list all of the archived workers for a particular Job , issue a GET command to the Mantis REST API endpoint endpoint /api/v1/jobs/ jobID /archivedWorkers . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no job with that ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"List the Archived Workers for a Job"},{"location":"mantisapi/#get-a-list-of-jobs-for-a-particular-cluster","text":"/api/v1/jobClusters/ clusterName /jobs ( GET ) To retrieve a JSON array of IDs of the active Jobs in a particular cluster, issue a GET command to the Mantis REST API endpoint /api/v1/jobClusters/ clusterName /jobs . Query Parameters Query Parameter Purpose ascending (optional) You can use this to indicate whether or not to sort the records in ascending order ( true compact (optional) Ask the server to return compact responses ( true fields (optional) By default this endpoint will return all of the fields in the payload. You can set fields to a comma-delimited series of payload fields, in which case this endpoint will return only those fields in the response. limit (optional) The maximum record size to return (default = no limit). offset (optional) The record number to begin with in the set of records to return in this request (use this with pageSize to get records by the page). See Pagination for more details. pageSize (optional) The maximum number of records to return in this request (default = 0, which means all records). See Pagination for more details. sortBy (optional) You can set this to the name of any payload field whose values are Comparable and this endpoint will return its results sorted by that field. There is also a series of query parameters that you can use to set server-side filters that will restrict the jobs represented in the resulting list of jobs to only those jobs that match the filters: Query Parameter What It Filters activeOnly (optional) The activeOnly field (boolean). By default, this is true . jobState (optional) Job state, Active or Terminal . By default, this endpoint filters on jobState=Active . This query parameter has precedence over the activeOnly parameter. labels (optional) Labels in the labels array. You can express this by setting this parameter to a comma-delimited list of label strings. labels.op (optional) Use this parameter to tell the server whether to treat the list of labels you have provided as an or (default: a job that contains any of the labels will be returned in the list), or an and (only jobs that contain all of the labels will be returned). Set this to or or and . stageNumber (optional) Stage number (integer). This filters the list to contain only those jobs corresponding to workers that are relevant to the specified stage. workerIndex (optional) The workerIndex field (integer). workerNumber (optional) The workerNumber field (integer). workerState (optional) The workerState field ( Noop , Active (default), or Terminal ) Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get a List of Jobs for a Particular Cluster"},{"location":"mantisapi/#submit-a-new-job-for-a-particular-cluster","text":"/api/v1/jobClusters/ clusterName /jobs ( POST ) To submit a new Job based on a Job Cluster , issue a POST command to the /api/v1/jobClusters/ clusterName /jobs endpoint with a request body like the following: Example Request Body { \"name\": \"myClusterName\", \"user\": \"jschmoe\", \"jobJarFileLocation\": null, \"version\": \"0.0.1 2019-02-06 13:30:07\", \"subscriptionTimeoutSecs\": 0, \"jobSla\": { \"runtimeLimitSecs\": \"0\", \"slaType\": \"Lossy\", \"durationType\": \"Perpetual\", \"userProvidedType\": \"\" }, \"schedulingInfo\": { \"stages\": { \"0\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 0.35, \"memoryMB\": 600, \"networkMbps\": 30, \"diskMB\": 100, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": false }, \"1\": { \"numberOfInstances\": 1, \"machineDefinition\": { \"cpuCores\": 0.35, \"memoryMB\": 600, \"networkMbps\": 30, \"diskMB\": 100, \"numPorts\": 1 }, \"hardConstraints\": null, \"softConstraints\": null, \"scalable\": true } } }, \"parameters\": [ { \"name\": \"criterion\", \"value\": \"mock\" }, { \"name\": \"sourceJobName\", \"value\": \"RequestSource\" }, { \"name\": \"spaasJobId\", \"value\": \"spaasjschmoe-clsessionizer_backpressuretest\" } ], \"isReadyForJobMaster\": false } Example Response Format (Same as \"Get Information about a Job\" below) Possible Response Codes Response Code Reason 201 normal response 404 no cluster with the given cluster name was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Submit a New Job for a Particular Cluster"},{"location":"mantisapi/#update-a-job-cluster-and-submit-a-new-job-at-the-same-time","text":"/api/v1/jobs/actions/quickSubmit ( POST ) You can make a \u201cquick update\u201d of an existing Job Cluster and also submit a new Job with the updated cluster artifacts . This lets you update the Job Cluster with minimal information, without having to specify the scheduling info, as long as at least one Job was previously submitted for this Job Cluster. To do this, send a POST command to the Mantis REST API endpoint /api/v1/jobs/actions/quickSubmit with a body that matches the format of the following example: Example Request Body { \"name\": \"NameOfJobCluster\", \"user\": \"myusername\", \"jobSla\": { \"durationType\": \"Perpetual\", \"runtimeLimitSecs\": \"0\", \"minRuntimeSecs\": \"0\", \"userProvidedType\": \"\" } } Example Response Format You will receive in the response the Job ID of the newly submitted Job. sample response body? Possible Response Codes Response Code Reason 201 normal response 404 no cluster was found with a cluster name matching the value of name from the request body 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Update a Job Cluster and Submit a New Job at the Same Time"},{"location":"mantisapi/#post-job-heartbeat-status","text":"/api/v1/jobs/actions/postJobStatus ( POST ) Query Parameters TBD Example Request Body { \"jobId\": \"sine-function-1\", \"status\": { \"jobId\": \"sine-function-1\", \"stageNum\": 1, \"workerIndex\": 0, \"workerNumber\": 2, \"type\": \"HEARTBEAT\", \"message\": \"heartbeat\", \"state\": \"Noop\", \"hostname\": null, \"timestamp\": 1525813363585, \"reason\": \"Normal\", \"payloads\": [ { \"type\": \"SubscriptionState\", \"data\": \"false\" }, { \"type\": \"IncomingDataDrop\", \"data\": \"{\\\"onNextCount\\\":0,\\\"droppedCount\\\":0}\" } ] } } Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Post Job Heartbeat Status"},{"location":"mantisapi/#horizontally-scale-a-stage","text":"/api/v1/jobs/ jobID /actions/scaleStage ( POST ) To manually scale a Job Processing Stage , that is, to alter the number of workers assigned to that stage, send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/scaleStage with a request body in the following format: Note You can only manually scale a Processing Stage if the Job was submitted with the scalable flag turned on. Example Request Body { \"JobId\": \"ValidatorDemo-33\", \"StageNumber\": 1, \"NumWorkers\": 3 } NumWorkers here is the number of workers you want to be assigned to the stage after the scaling action takes place (that is, it is not the delta by which you want to change the number of workers in the stage). Example Response Format TBD Possible Response Codes Response Code Reason 204 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Horizontally Scale a Stage"},{"location":"mantisapi/#resubmit-a-worker","text":"/api/v1/jobs/ jobID /actions/resubmitWorker ( POST ) To resubmit a particular worker for a particular Job , send a POST command to the Mantis REST API endpoint /api/v1/jobs/ jobID /actions/resubmitWorker with a request body resembling the following: Example Request Body { \"user\": \"jschmoe\", \"workerNumber\": 5, \"reason\": \"test worker resubmit\" } Note workerNumber is the worker number not the worker index . Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Resubmit a Worker"},{"location":"mantisapi/#administrative-tasks","text":"TBD","title":"Administrative Tasks"},{"location":"mantisapi/#return-job-master-information","text":"/api/v1/masterInfo ( GET ) TBD Example Response Body { \"hostname\": \"100.86.121.198\", \"hostIP\": \"100.86.121.198\", \"apiPort\": 7101, \"schedInfoPort\": 7101, \"apiPortV2\": 7075, \"apiStatusUri\": \"api/v1/jobs/actions/postJobStatus\", \"consolePort\": 7101, \"createTime\": 1548803881867, \"fullApiStatusUri\": \"http://100.86.121.198:7101/api/v1/jobs/actions/postJobStatus\" } Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Return Job Master Information"},{"location":"mantisapi/#return-job-master-configs","text":"/api/v1/masterConfigs ( GET ) TBD Example Response Body [ { \"name\": \"JobConstraints\", \"value\": \"[\\\"UniqueHost\\\",\\\"ExclusiveHost\\\",\\\"ZoneBalance\\\",\\\"M4Cluster\\\",\\\"M3Cluster\\\",\\\"M5Cluster\\\"]\" }, { \"name\": \"ScalingReason\", \"value\": \"[\\\"CPU\\\",\\\"Memory\\\",\\\"Network\\\",\\\"DataDrop\\\",\\\"KafkaLag\\\",\\\"UserDefined\\\",\\\"KafkaProcessed\\\"]\" }, { \"name\": \"MigrationStrategyEnum\", \"value\": \"[\\\"ONE_WORKER\\\",\\\"PERCENTAGE\\\"]\" }, { \"name\": \"WorkerResourceLimits\", \"value\": \"{\\\"maxCpuCores\\\":8,\\\"maxMemoryMB\\\":28000,\\\"maxNetworkMbps\\\":1024}\" } ] Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Return Job Master Configs"},{"location":"mantisapi/#get-information-about-agent-clusters","text":"/api/v1/agentClusters/ ( GET ) Returns information about active agent clusters (agent clusters are physical AWS resources that Mantis connects to). Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Information about Agent Clusters"},{"location":"mantisapi/#activate-or-deactivate-an-agent-cluster","text":"/api/v1/agentClusters/ ( POST ) TBD Example Request Body [\"mantisagent-staging-cl1-m5.2xlarge-1-v00\"] (an array of active clusters) Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 400 client failure 405 incorrect HTTP verb (use POST instead) 500 unknown server error","title":"Activate or Deactivate an Agent Cluster"},{"location":"mantisapi/#retrieve-the-agent-cluster-scaling-policy","text":"/api/v1/agentClusters/autoScalePolicy ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Retrieve the Agent Cluster Scaling Policy"},{"location":"mantisapi/#get-jobs-and-host-information-for-an-agent-cluster","text":"/api/v1/agentClusters/jobs ( GET ) TBD Example Response Body response body? Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Jobs and Host Information for an Agent Cluster"},{"location":"mantisapi/#streaming-websocketsse-tasks","text":"TBD: introduction","title":"Streaming WebSocket/SSE Tasks"},{"location":"mantisapi/#stream-job-status-changes","text":"ws:// masterHost :7101/api/v1/jobStatusStream/ jobID TBD: description See also: mantisapi/websocket/ Example Response Stream Excerpt { \"status\": { \"jobId\": \"SPaaSBackpressureDemp-26\", \"stageNum\": 1, \"workerIndex\": 0, \"workerNumber\": 7, \"type\": \"INFO\", \"message\": \"SPaaSBackpressureDemp-26-worker-0-7 worker status update\", \"state\": \"StartInitiated\", \"hostname\": null, \"timestamp\": 1549404217065, \"reason\": \"Normal\", \"payloads\": [] } }","title":"Stream Job Status Changes"},{"location":"mantisapi/#stream-scheduling-information-for-a-job","text":"/api/v1/jobDiscoveryStream/ jobID ( GET ) /api/v1/jobs/schedulingInfo/ jobID ( GET ) To retrieve an SSE stream of scheduling information for a particular job, send an HTTP GET command to either the Mantis REST API endpoint /api/v1/jobDiscoveryStream/ jobID or /api/v1/jobs/schedulingInfo/ jobID . Query Parameters jobDiscoveryStream Query Parameter Purpose sendHB (optional) Indicate whether or not to send heartbeats (default= false ). jobs/schedulingInfo Query Parameter Purpose jobId (required) The job ID of the job for which scheduling information is to be streamed. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Job with that Job ID was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Stream Scheduling Information for a Job"},{"location":"mantisapi/#get-streaming-sse-discovery-info-for-a-cluster","text":"/api/v1/jobClusters/discoveryInfoStream/ clusterName ( GET ) describe Example Response Format TBD Possible Response Codes Response Code Reason 200 normal response 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Get Streaming (SSE) Discovery Info for a Cluster"},{"location":"mantisapi/#stream-job-information-for-a-cluster","text":"/api/v1/lastSubmittedJobIdStream/ clusterName ( GET ) To retrieve an SSE stream of job information for a particular cluster, send an HTTP GET command to the Mantis REST API endpoint /api/v1/lastSubmittedJobIdStream/ clusterName . Query Parameters Query Parameter Purpose sendHB Indicate whether or not to send heartbeats. Example Response Stream Excerpt response body? Possible Response Codes Response Code Reason 200 normal response 404 no Cluster with that Cluster name was found 405 incorrect HTTP verb (use GET instead) 500 unknown server error","title":"Stream Job Information for a Cluster"},{"location":"mantisapi/#stream-job-sink-messages","text":"/api/v1/jobConnectbyid/ jobID (SSE) /api/v1/jobConnectbyname/ jobName (SSE) Collect messages from the job sinks and merge them into a single websocket or SSE stream. Example Response Stream Excerpt data: {\"x\": 928400.000000, \"y\": 3.139934} data: {\"x\": 928402.000000, \"y\": -9.939772} data: {\"x\": 928404.000000, \"y\": 5.132876} data: {\"x\": 928406.000000, \"y\": 5.667712}","title":"Stream Job Sink Messages"},{"location":"mantisapi/#submit-job-and-stream-job-sink-messages","text":"/api/v1/jobsubmitandconnect ( POST ) To submit a job and then collect messages from the job sinks and merge them into a single websocket or SSE stream, send a POST request to /api/v1/jobsubmitandconnect with a request body that describes the job. Example Request Body { \"name\": \"ValidatorDemo\", \"user\": \"jschmoe\", \"jobSla\": { \"durationType\": \"Perpetual\", \"runtimeLimitSecs\": \"0\", \"minRuntimeSecs\": \"0\", \"userProvidedType\": \"\" } } Example Response Stream Excerpt data: {\"x\": 928400.000000, \"y\": 3.139934} data: {\"x\": 928402.000000, \"y\": -9.939772} data: {\"x\": 928404.000000, \"y\": 5.132876} data: {\"x\": 928406.000000, \"y\": 5.667712}","title":"Submit Job and Stream Job Sink Messages"},{"location":"mantisapi/#pagination","text":"You can configure some endpoints to return their responses in pages . This is to say that rather than returning all of the data responsive to a request all at once, the endpoint can return a specific subset of the data at a time. An endpoint that supports pagination will typically do so by means of two query parameters: Query Parameter Purpose pageSize the maximum number of records to return in this request (default = 0, which means all records) offset the record number to begin with in the set of records to return in this request So, for example, you might make consecutive requests for\u2026 endpoint ?pageSize=10&offset=0 endpoint ?pageSize=10&offset=10 endpoint ?pageSize=10&offset=20 \u2026and so forth, until you reach the final page of records. When you request records in a paginated fashion like this, the Mantis API will enclose them in a JSON structure like the following: { \"list\": [ { \"Id\": originalOffset , \u22ee }, { \"Id\": originalOffset+1 , \u22ee }, \u22ee ], \"prev\": \"/api/v1/ endpoint ?pageSize= pageSize &offset= originalOffset+pageSize \", \"next\": \"/api/v1/ endpoint ?pageSize= pageSize &offset= originalOffset\u2212pageSize \" } For example: { \"list\" : [ { \"Id\" : 101 , \u22ee }, { \"Id\" : 102 , \u22ee } ], \"next\" : \"/api/v1/someResource?pageSize=20&offset=120\" , \"prev\" : \"/api/v1/someResource?pageSize=20&offset=80\" } prev and/or next will be null if there is no previous or next page, that is, if you are at the first and/or last page in the pagination.","title":"Pagination"},{"location":"mantisapi/websocket/","text":"Some desktops have trouble dealing with SSE . In such a case you can use WebSocket to connect to a Job (other API features are only available via the Mantis REST API). The WebSocket API runs from the same servers as the Mantis REST API, and you can reach it by using the ws:// or wss:// protocol and port 7102. Connecting to Job Output (Sink) \u00b6 Append this to the WebSocket URI to connect to a Job by name: /jobconnectbyname/ JobName Append this to the WebSocket URI to connect to a specific running Job ID: /jobconnectbyid/ JobID Upon connecting, the server starts writing messages that are coming in from the corresponding Jobs. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d Sink parameter which limits the rate at which the Sink output is sampled by the server. Submitting and Connecting to Job Output \u00b6 Append this to the WebSocket URI to submit and connect to the submitted Job: /jobsubmitandconnect Note that you will have to send one message on the WebSocket, which is the same JSON payload that you would send as a POST body for the equivalent REST API . The server then submits the Job and then starts writing messages into the WebSocket as it gets output from the Job. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d sink parameter which limits the rate at which the Sink output is sampled by the server. Example: Javascript to Connect a Job \u00b6 <!DOCTYPE html> < meta charset = \"utf-8\" /> < title > WebSocket Test </ title > < script language = \"javascript\" type = \"text/javascript\" > var wsUri = \"ws://your-domain.com/jobconnectbyname/YourJobName?sampleMSec=2000\" ; var output ; function init () { output = document . getElementById ( \"output\" ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( \"CONNECTED\" ); } function onClose ( evt ) { writeToScreen ( \"DISCONNECTED\" ); } function onMessage ( evt ) { writeToScreen ( '<span style=\"color: blue;\">RESPONSE: ' + evt . data + '</span>' ); } function onError ( evt ) { writeToScreen ( '<span style=\"color: red;\">ERROR:</span> ' + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( \"p\" ); pre . style . wordWrap = \"break-word\" ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( \"load\" , init , true ); </ script > < h2 > WebSocket Test </ h2 > < div id = \"output\" ></ div > Example: Javascript to Submit a Job and Connect to It \u00b6 <!DOCTYPE html> < meta charset = \"utf-8\" /> < title > WebSocket Test </ title > < script language = \"javascript\" type = \"text/javascript\" > var wsUri = \"ws://your-domain.com/jobsubmitandconnect/\" ; var output ; function init () { output = document . getElementById ( \"output\" ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( \"CONNECTED\" ); // Change this to your job's submit json content. // See job submit REST API above for another example. doSend ( \"{\\n\" + \" \\\"name\\\":\\\"Outliers-mock3\\\",\\n\" + \" \\\"version\\\":\\\"\\\",\\n\" + \" \\\"parameters\\\":[],\\n\" + \" \\\"jobSla\\\":{\\\"runtimeLimitSecs\\\":0,\\\"durationType\\\":\\\"Transient\\\",\\\"userProvidedType\\\":\\\"{\\\\\\\"unique\\\\\\\":\\\\\\\"foobar\\\\\\\"}\\\"},\\n\" + \" \\\"subscriptionTimeoutSecs\\\":\\\"90\\\",\\n\" + \" \\\"jobJarFileLocation\\\":null,\\n\" + \" \\\"schedulingInfo\\\":{\\\"stages\\\":{\\\"1\\\":{\\\"numberOfInstances\\\":1,\\\"machineDefinition\\\":{\\\"cpuCores\\\":1.0,\\\"memoryMB\\\":2048.0,\\\"diskMB\\\":1.0,\\\"scalable\\\":\\\"true\\\"}}}\\n\" + \"}\" ); } function onClose ( evt ) { writeToScreen ( \"DISCONNECTED\" ); } function onMessage ( evt ) { writeToScreen ( '<span style=\"color: blue;\">RESPONSE: ' + evt . data + '</span>' ); } function onError ( evt ) { writeToScreen ( '<span style=\"color: red;\">ERROR:</span> ' + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( \"p\" ); pre . style . wordWrap = \"break-word\" ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( \"load\" , init , true ); </ script > < h2 > WebSocket Test </ h2 > < div id = \"output\" ></ div >","title":"Mantis Websocket API"},{"location":"mantisapi/websocket/#connecting-to-job-output-sink","text":"Append this to the WebSocket URI to connect to a Job by name: /jobconnectbyname/ JobName Append this to the WebSocket URI to connect to a specific running Job ID: /jobconnectbyid/ JobID Upon connecting, the server starts writing messages that are coming in from the corresponding Jobs. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d Sink parameter which limits the rate at which the Sink output is sampled by the server.","title":"Connecting to Job Output (Sink)"},{"location":"mantisapi/websocket/#submitting-and-connecting-to-job-output","text":"Append this to the WebSocket URI to submit and connect to the submitted Job: /jobsubmitandconnect Note that you will have to send one message on the WebSocket, which is the same JSON payload that you would send as a POST body for the equivalent REST API . The server then submits the Job and then starts writing messages into the WebSocket as it gets output from the Job. You can append query parameters to the WebSocket URI (preceded by \u201c ? \u201d) as is the case in a REST API. Use this to pass any Sink parameters your Job accepts. All Jobs accept the \u201c sampleMSec= mSecs \u201d sink parameter which limits the rate at which the Sink output is sampled by the server.","title":"Submitting and Connecting to Job Output"},{"location":"mantisapi/websocket/#example-javascript-to-connect-a-job","text":"<!DOCTYPE html> < meta charset = \"utf-8\" /> < title > WebSocket Test </ title > < script language = \"javascript\" type = \"text/javascript\" > var wsUri = \"ws://your-domain.com/jobconnectbyname/YourJobName?sampleMSec=2000\" ; var output ; function init () { output = document . getElementById ( \"output\" ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( \"CONNECTED\" ); } function onClose ( evt ) { writeToScreen ( \"DISCONNECTED\" ); } function onMessage ( evt ) { writeToScreen ( '<span style=\"color: blue;\">RESPONSE: ' + evt . data + '</span>' ); } function onError ( evt ) { writeToScreen ( '<span style=\"color: red;\">ERROR:</span> ' + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( \"p\" ); pre . style . wordWrap = \"break-word\" ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( \"load\" , init , true ); </ script > < h2 > WebSocket Test </ h2 > < div id = \"output\" ></ div >","title":"Example: Javascript to Connect a Job"},{"location":"mantisapi/websocket/#example-javascript-to-submit-a-job-and-connect-to-it","text":"<!DOCTYPE html> < meta charset = \"utf-8\" /> < title > WebSocket Test </ title > < script language = \"javascript\" type = \"text/javascript\" > var wsUri = \"ws://your-domain.com/jobsubmitandconnect/\" ; var output ; function init () { output = document . getElementById ( \"output\" ); testWebSocket (); } function testWebSocket () { websocket = new WebSocket ( wsUri ); websocket . onopen = function ( evt ) { onOpen ( evt ) }; websocket . onclose = function ( evt ) { onClose ( evt ) }; websocket . onmessage = function ( evt ) { onMessage ( evt ) }; websocket . onerror = function ( evt ) { onError ( evt ) }; } function onOpen ( evt ) { writeToScreen ( \"CONNECTED\" ); // Change this to your job's submit json content. // See job submit REST API above for another example. doSend ( \"{\\n\" + \" \\\"name\\\":\\\"Outliers-mock3\\\",\\n\" + \" \\\"version\\\":\\\"\\\",\\n\" + \" \\\"parameters\\\":[],\\n\" + \" \\\"jobSla\\\":{\\\"runtimeLimitSecs\\\":0,\\\"durationType\\\":\\\"Transient\\\",\\\"userProvidedType\\\":\\\"{\\\\\\\"unique\\\\\\\":\\\\\\\"foobar\\\\\\\"}\\\"},\\n\" + \" \\\"subscriptionTimeoutSecs\\\":\\\"90\\\",\\n\" + \" \\\"jobJarFileLocation\\\":null,\\n\" + \" \\\"schedulingInfo\\\":{\\\"stages\\\":{\\\"1\\\":{\\\"numberOfInstances\\\":1,\\\"machineDefinition\\\":{\\\"cpuCores\\\":1.0,\\\"memoryMB\\\":2048.0,\\\"diskMB\\\":1.0,\\\"scalable\\\":\\\"true\\\"}}}\\n\" + \"}\" ); } function onClose ( evt ) { writeToScreen ( \"DISCONNECTED\" ); } function onMessage ( evt ) { writeToScreen ( '<span style=\"color: blue;\">RESPONSE: ' + evt . data + '</span>' ); } function onError ( evt ) { writeToScreen ( '<span style=\"color: red;\">ERROR:</span> ' + evt . data ); } function doSend ( message ) { websocket . send ( message ); } function writeToScreen ( message ) { var pre = document . createElement ( \"p\" ); pre . style . wordWrap = \"break-word\" ; pre . innerHTML = message ; output . appendChild ( pre ); } window . addEventListener ( \"load\" , init , true ); </ script > < h2 > WebSocket Test </ h2 > < div id = \"output\" ></ div >","title":"Example: Javascript to Submit a Job and Connect to It"},{"location":"writingjobs/","text":"A Mantis Job is a JVM-based stream processing application that takes in an Observable stream of data items, transforms this stream by using RxJava operators, and then outputs the results as another Observable stream. RxJava Observables can be visualized by using \u201cmarble diagrams\u201d: You can combine multiple RxJava operators to transform an Observable stream of items in many ways: The above diagram shows a Mantis Job composed of two operators that process an input stream to compose an output stream. The first operator, map , emits a new Observable for each item emitted by the source Observable. The second operator, merge , emits each item emitted by those Observables as a fresh Observable stream. There is an enormous wealth of ways in which you can transform streams of data by using RxJava Observable operators. If the volume of data to be processed is too large for a single worker to handle, you can \u201cdivide and conquer\u201d by grouping and dividing the operators across various processing stages , as in the following diagram: Mantis Job Clusters \u00b6 You define a Job Cluster before you submit Jobs . A Job Cluster is a containing entity for Jobs. It defines metadata and certain service-level agreements ( SLA s). Job Clusters ease Job lifecycle management and Job revisioning. For example, by setting the SLA of a Job Cluster to Min=1 and Max=1, you ensure that exactly one Job instance is always running for that Cluster. The Job Cluster also has default Job parameters that any new Jobs submitted to the Cluster inherit. You can update new Job artifacts into the Job Cluster so that the next Job submission picks up the latest version. Components of a Mantis Job \u00b6 A Mantis Job has three components , each of which has a corresponding chapter in this documentation: Source \u2014 Fetches data from an external source and makes it available in the form of an Observable. Processing Stage \u2014 Transforms the Observable stream by means of a variety of operators. Sink \u2014 Pushes the resulting Observable out in the form of a fresh stream. Directory Structure of a Mantis Job \u00b6 In addition to the source files, Mantis requires some meta-files to be present in the Job artifact. Here is a sample directory structure: src/ - main/ - java/ - com/ - myorg/ - MyJob.java - resources/ - META-INF/ - services/ - io.mantisrx.runtime.MantisJobProvider - job.properties - job-test.properties io.mantisrx.runtime.MantisJobProvider (required) \u2014 lists the fully-qualified name of the Java class that implements the MantisJobProvider interface job.properties and job-test.properties (optional) \u2014 required only if you are initializing the platform via the .lifecycle() method Creating a Mantis Job \u00b6 To create a Mantis Job, call MantisJob\u2026create() . When you do so, interpolate the following builder methods. The first three \u2014 .source() , .stage() , and .sink() \u2014 are required, they must be the first three of the methods that you call, and you must call them in that order: .source( AbstractJobSource ) \u2014 required, see The Source Component .stage( Stage , stageConfig ) \u2014 required, (call this one or more times) see The Processing Stage Component .sink( Sink ) \u2014 required, see The Sink Component .lifecycle( Lifecycle ) \u2014 optional, allows for start-up and shut-down hooks .parameterDefinition( ParameterDefinition ) \u2014 optional, (call this zero to many times) define any parameters your job requires here .metadata( Metadata ) \u2014 optional, (call this zero to many times) define your job name and description here of this class this method returns an object of this class MantisJob \u27f6 source() \u27f6 SourceHolder \u2936 SourceHolder \u27f6 stage() \u27f6 Stages \u2936 [ Stages \u27f6 stage() \u27f6 Stages ] \u2936 Stages \u27f6 sink() \u27f6 Config \u2936 [ Config \u27f6 lifecycle() \u27f6 Config ] \u2936 [ Config \u27f6 parameterDefinition() \u27f6 Config ] \u2936 [ Config \u27f6 metadata() \u27f6 Config ] \u2936 Config \u27f6 create() \u27f6 Job Lifecycle Management \u00b6 You can establish start-up and shut-down procedures for your Mantis Job by means of the .lifecycle(). builder method. Pass this method a Lifecycle object, that is to say, an object that implements the following methods: startup() \u2014 initialize arbitrary application configs, perform dependency injection, and any long running or shared service libraries. shutdown() \u2014 gracefully close connections, shut down long running or shared service libraries, and general process cleanup. getServiceLocator() \u2014 returns a ServiceLocator that implements the service(key) method. Implement this method to return your dependency injection object such as Guice. Defining Parameters \u00b6 To create a Parameter in order to pass it to the .parameterDefinition() builder method of the MantisJob builder, use the following new ParameterVariety ()\u2026build() builder methods: .name( string ) \u2014 a user-friendly name for your Parameter .description( string ) \u2014 a user-friendly description of your Parameter .defaultValue( value ) \u2014 the value of this Parameter if the Job does not override it .validator( Validator ) \u2014 a way of checking the proposed Parameter values for validity so bad values can be rejected before you submit the Job .required() \u2014 call this builder method if the Job must provide a value for this Parameter There are some built-in Parameter varieties you can choose from that correspond to common data types: BooleanParameter DoubleParameter IntParameter StringParameter Validators \u00b6 There are some standard Validators you can choose from that cover some common varieties of parameters: Validators.range( start , end ) \u2014 will reject as invalid Parameter values that do not lie between the indicated start and end numerical values (where start and end themselves are valid Parameter values) Validators.notNullOrEmpty() \u2014 will reject empty strings or null values Validators.alwaysPass() \u2014 will not reject any Parameter values as invalid Example \u00b6 For example: myStringParameter = new StringParameter (). name ( \"MyParameter\" ) . description ( \"This is a human-friendly description of my parameter\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"SomeValue\" ) . required () . build (); Defining Metadata \u00b6 To create metadata in order to pass it to the .metadata() builder method of the MantisJob builder, use the following new Metadata.Builder()\u2026build() builder methods: .name( string ) .description( string ) For example: myMetadata = new Metadata . Builder (). name ( \"MyMetadata\" ) . description ( \"Description of my metadata\" ) . build ();","title":"Introduction to Mantis Jobs"},{"location":"writingjobs/#mantis-job-clusters","text":"You define a Job Cluster before you submit Jobs . A Job Cluster is a containing entity for Jobs. It defines metadata and certain service-level agreements ( SLA s). Job Clusters ease Job lifecycle management and Job revisioning. For example, by setting the SLA of a Job Cluster to Min=1 and Max=1, you ensure that exactly one Job instance is always running for that Cluster. The Job Cluster also has default Job parameters that any new Jobs submitted to the Cluster inherit. You can update new Job artifacts into the Job Cluster so that the next Job submission picks up the latest version.","title":"Mantis Job Clusters"},{"location":"writingjobs/#components-of-a-mantis-job","text":"A Mantis Job has three components , each of which has a corresponding chapter in this documentation: Source \u2014 Fetches data from an external source and makes it available in the form of an Observable. Processing Stage \u2014 Transforms the Observable stream by means of a variety of operators. Sink \u2014 Pushes the resulting Observable out in the form of a fresh stream.","title":"Components of a Mantis Job"},{"location":"writingjobs/#directory-structure-of-a-mantis-job","text":"In addition to the source files, Mantis requires some meta-files to be present in the Job artifact. Here is a sample directory structure: src/ - main/ - java/ - com/ - myorg/ - MyJob.java - resources/ - META-INF/ - services/ - io.mantisrx.runtime.MantisJobProvider - job.properties - job-test.properties io.mantisrx.runtime.MantisJobProvider (required) \u2014 lists the fully-qualified name of the Java class that implements the MantisJobProvider interface job.properties and job-test.properties (optional) \u2014 required only if you are initializing the platform via the .lifecycle() method","title":"Directory Structure of a Mantis Job"},{"location":"writingjobs/#creating-a-mantis-job","text":"To create a Mantis Job, call MantisJob\u2026create() . When you do so, interpolate the following builder methods. The first three \u2014 .source() , .stage() , and .sink() \u2014 are required, they must be the first three of the methods that you call, and you must call them in that order: .source( AbstractJobSource ) \u2014 required, see The Source Component .stage( Stage , stageConfig ) \u2014 required, (call this one or more times) see The Processing Stage Component .sink( Sink ) \u2014 required, see The Sink Component .lifecycle( Lifecycle ) \u2014 optional, allows for start-up and shut-down hooks .parameterDefinition( ParameterDefinition ) \u2014 optional, (call this zero to many times) define any parameters your job requires here .metadata( Metadata ) \u2014 optional, (call this zero to many times) define your job name and description here of this class this method returns an object of this class MantisJob \u27f6 source() \u27f6 SourceHolder \u2936 SourceHolder \u27f6 stage() \u27f6 Stages \u2936 [ Stages \u27f6 stage() \u27f6 Stages ] \u2936 Stages \u27f6 sink() \u27f6 Config \u2936 [ Config \u27f6 lifecycle() \u27f6 Config ] \u2936 [ Config \u27f6 parameterDefinition() \u27f6 Config ] \u2936 [ Config \u27f6 metadata() \u27f6 Config ] \u2936 Config \u27f6 create() \u27f6 Job","title":"Creating a Mantis Job"},{"location":"writingjobs/#lifecycle-management","text":"You can establish start-up and shut-down procedures for your Mantis Job by means of the .lifecycle(). builder method. Pass this method a Lifecycle object, that is to say, an object that implements the following methods: startup() \u2014 initialize arbitrary application configs, perform dependency injection, and any long running or shared service libraries. shutdown() \u2014 gracefully close connections, shut down long running or shared service libraries, and general process cleanup. getServiceLocator() \u2014 returns a ServiceLocator that implements the service(key) method. Implement this method to return your dependency injection object such as Guice.","title":"Lifecycle Management"},{"location":"writingjobs/#defining-parameters","text":"To create a Parameter in order to pass it to the .parameterDefinition() builder method of the MantisJob builder, use the following new ParameterVariety ()\u2026build() builder methods: .name( string ) \u2014 a user-friendly name for your Parameter .description( string ) \u2014 a user-friendly description of your Parameter .defaultValue( value ) \u2014 the value of this Parameter if the Job does not override it .validator( Validator ) \u2014 a way of checking the proposed Parameter values for validity so bad values can be rejected before you submit the Job .required() \u2014 call this builder method if the Job must provide a value for this Parameter There are some built-in Parameter varieties you can choose from that correspond to common data types: BooleanParameter DoubleParameter IntParameter StringParameter","title":"Defining Parameters"},{"location":"writingjobs/#validators","text":"There are some standard Validators you can choose from that cover some common varieties of parameters: Validators.range( start , end ) \u2014 will reject as invalid Parameter values that do not lie between the indicated start and end numerical values (where start and end themselves are valid Parameter values) Validators.notNullOrEmpty() \u2014 will reject empty strings or null values Validators.alwaysPass() \u2014 will not reject any Parameter values as invalid","title":"Validators"},{"location":"writingjobs/#example","text":"For example: myStringParameter = new StringParameter (). name ( \"MyParameter\" ) . description ( \"This is a human-friendly description of my parameter\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"SomeValue\" ) . required () . build ();","title":"Example"},{"location":"writingjobs/#defining-metadata","text":"To create metadata in order to pass it to the .metadata() builder method of the MantisJob builder, use the following new Metadata.Builder()\u2026build() builder methods: .name( string ) .description( string ) For example: myMetadata = new Metadata . Builder (). name ( \"MyMetadata\" ) . description ( \"Description of my metadata\" ) . build ();","title":"Defining Metadata"},{"location":"writingjobs/sink/","text":"The Sink component of a Mantis Job serves two purposes: Trigger job execution when a client subscribes to a Job RxJava cold Observables have a lazy execution model. Execution begins only when someone subscribes to the Observable. A Mantis Job is a complex Observable chain, and to trigger the execution of such a Job, somebody needs to subscribe to it. This happens in the Sink component. Output the results of job execution in a streaming fashion Once you are done processing the input stream in the Processing Stage component, you need to figure out what to do with the results. Most jobs that write to some other system do so within the Processing Stage component itself (in such a case, the Sink component is usually used for debugging purposes). To create a Sink component, you implement the Sink interface: import io.mantisrx.runtime.Context ; import io.mantisrx.runtime.PortRequest ; import rx.Observable ; import rx.functions.Action3 ; public interface Sink < T > extends Action3 < Context , PortRequest , Observable < T >> { } Built-in Sinks \u00b6 Some Sinks are provided by Mantis. To get access to these Sinks, add this line to your source: import io.mantisrx.runtime.sink.Sinks ; SSE Sink \u00b6 The SSE Sink is commonly used. It makes the results of your Stage transformation available in the form of an SSE stream. To get an SSE Sink, pass an encoder function to the sse() method that accepts the data to be streamed as input and outputs data encoded as needed. The following example attaches a sink to a Mantis Job that passes the data from the last Processing Stage of the job, unchanged, to the SSE stream: return MantisJob . \u2026 . sink ( Sinks . sse (( String data ) -> data )). \u2026 . create (); sysout Sink \u00b6 The sysout Sink simply outputs the results from the previous Processing Stage directly to sysout. For example: return MantisJob . \u2026 . sink ( Sinks . sysout ()). \u2026 . create (); eagerSubscribe Sink \u00b6 A typical Sink subscribes to the output of the previous Processing Stage at some point during the call to its call() method. You may want some of your Mantis Jobs to start executing as soon as you launch them. These include Jobs that run perpetually and power things like alerts, dashboards, and so forth. You can modify a Sink with eagerSubscribe() to create a Sink that that instead subscribes to the output of the previous Processing Stage immediately when its call() method is called, even if it has more processing to do within that method before it can respond to the output. This will start your Mantis Job more quickly, but may mean some of its initial data is dropped. For example: return MantisJob . \u2026 . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) -> data ))). \u2026 . create (); Here is a more complete example: return MantisJob // Reuse existing class that does all the plumbing of connecting to source jobs . source ( new JobSource ()) // Groups requests by ESN . stage ( new GroupByStage (), GroupByStage . config ()) // Computes sum over a window . stage ( new FastAggregateStage (), FastAggregateStage . config ()) // Collects the data and makes it availabe over SSE . stage ( new CollectStage (), CollectStage . config ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) -> data ) )) // Here we define the job parameter overrides // The query sent to the source job. // Here we fetch the esn for all requests hitting the source . parameterDefinition ( new StringParameter () . name ( MantisSourceJobConnector . MANTIS_SOURCEJOB_CRITERION ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"SELECT customer_id, client_ip WHERE true\" ) . build ()) . metadata ( new Metadata . Builder () . name ( \"GroupByIp\" ) . description ( \"Connects to a source job and counts the number of requests sent from each ip within a window\" ) . build ()) . create (); toMany Sink \u00b6 You can hook up multiple Sinks to the same final Stage of a Job by using the toMany Sink. To do this, pass each Sink to the toMany() method. For example: return MantisJob . \u2026 . sink ( Sinks . toMany ( Sinks . sysout (), Sinks . sse (( String data ) -> data ))). \u2026 . create (); Custom Sinks \u00b6 If you do not want to use one of the provided Sinks, or if you need to customize one of those (for instance, if you need access to the query parameters supplied by a client who is connecting into your Job, or if you need pre and post hooks to perform operations as a new client connects or disconnects), you can create your own Sink. To do so, implement the Sink interface. The ServerSentEventsSink builder takes three parameters: Preprocess function this callback gives you access to the query parameters; it is invoked before job execution begins Postprocess function this callback allows you to perform any clean-up actions; it is invoked just after the client connection is terminated Predicate function allows you to filter your output stream based on the given predicate, which allows you to filter based on query parameters sent by the client Here is an example of a custom Sink that uses the ServerSentEventsSink builder: package com.netflix.mantis.sourcejob ; import io.mantisrx.runtime.Context ; import io.mantisrx.runtime.PortRequest ; import io.mantisrx.runtime.sink.ServerSentEventsSink ; import io.mantisrx.runtime.sink.Sink ; import io.mantisrx.runtime.sink.predicate.Predicate ; import java.util.List ; import java.util.Map ; import rx.Observable ; import rx.functions.Func1 ; import rx.functions.Func2 ; import com.google.common.base.Charsets ; public class SourceSink implements Sink < String > { Func2 < Map < String , List < String >>, Context , Void > preProcessor = new NoOpProcessor (); Func2 < Map < String , List < String >>, Context , Void > postProcessor = new NoOpProcessor (); private String clientId = \"DEFAULT_CLIENT_ID\" ; static class NoOpProcessor implements Func2 < Map < String , List < String >>, Context , Void > { @Override public Void call ( Map < String , List < String >> t1 , Context t2 ) { return null ; } } public SourceSink () { } public SourceSink ( Func2 < Map < String , List < String >>, Context , Void > preProcessor , Func2 < Map < String , List < String >>, Context , Void > postProcessor , String mantisClientId ) { this . postProcessor = postProcessor ; this . preProcessor = preProcessor ; this . clientId = mantisClientId ; } @Override public void call ( Context context , PortRequest portRequest , Observable < String > observable ) { observable = observable . filter ( new Func1 < String , Boolean >() { @Override public Boolean call ( String t1 ) { return ! t1 . isEmpty (); } }); ServerSentEventsSink < String > sink = new ServerSentEventsSink . Builder < String >() . withEncoder ( new Func1 < String , String >() { @Override public String call ( String data ) { return data ; } }) . withPredicate ( new Predicate < String >( \"description\" , new EventFilter ( clientId ))) . withRequestPreprocessor ( preProcessor ) . withRequestPostprocessor ( postProcessor ) . build (); observable . subscribe (); sink . call ( context , portRequest , observable ); } public static void main ( String [] args ) { String s = \"{\\\"amazon.availability-zone\\\":\\\"us-east-1e\\\",\\\"status\\\":200,\\\"type\\\":\\\"EVENT\\\",\\\"matched-clients\\\":\\\"client6\\\",\\\"currentTime\\\":1409595016697,\\\"duration-millis\\\":172}\" ; byte [] barr = s . getBytes ( Charsets . UTF_8 ); System . out . println ( \"size: \" + barr . length ); } }","title":"The Sink Component"},{"location":"writingjobs/sink/#built-in-sinks","text":"Some Sinks are provided by Mantis. To get access to these Sinks, add this line to your source: import io.mantisrx.runtime.sink.Sinks ;","title":"Built-in Sinks"},{"location":"writingjobs/sink/#sse-sink","text":"The SSE Sink is commonly used. It makes the results of your Stage transformation available in the form of an SSE stream. To get an SSE Sink, pass an encoder function to the sse() method that accepts the data to be streamed as input and outputs data encoded as needed. The following example attaches a sink to a Mantis Job that passes the data from the last Processing Stage of the job, unchanged, to the SSE stream: return MantisJob . \u2026 . sink ( Sinks . sse (( String data ) -> data )). \u2026 . create ();","title":"SSE Sink"},{"location":"writingjobs/sink/#sysout-sink","text":"The sysout Sink simply outputs the results from the previous Processing Stage directly to sysout. For example: return MantisJob . \u2026 . sink ( Sinks . sysout ()). \u2026 . create ();","title":"sysout Sink"},{"location":"writingjobs/sink/#eagersubscribe-sink","text":"A typical Sink subscribes to the output of the previous Processing Stage at some point during the call to its call() method. You may want some of your Mantis Jobs to start executing as soon as you launch them. These include Jobs that run perpetually and power things like alerts, dashboards, and so forth. You can modify a Sink with eagerSubscribe() to create a Sink that that instead subscribes to the output of the previous Processing Stage immediately when its call() method is called, even if it has more processing to do within that method before it can respond to the output. This will start your Mantis Job more quickly, but may mean some of its initial data is dropped. For example: return MantisJob . \u2026 . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) -> data ))). \u2026 . create (); Here is a more complete example: return MantisJob // Reuse existing class that does all the plumbing of connecting to source jobs . source ( new JobSource ()) // Groups requests by ESN . stage ( new GroupByStage (), GroupByStage . config ()) // Computes sum over a window . stage ( new FastAggregateStage (), FastAggregateStage . config ()) // Collects the data and makes it availabe over SSE . stage ( new CollectStage (), CollectStage . config ()) // Reuse built in sink that eagerly subscribes and delivers data over SSE . sink ( Sinks . eagerSubscribe ( Sinks . sse (( String data ) -> data ) )) // Here we define the job parameter overrides // The query sent to the source job. // Here we fetch the esn for all requests hitting the source . parameterDefinition ( new StringParameter () . name ( MantisSourceJobConnector . MANTIS_SOURCEJOB_CRITERION ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"SELECT customer_id, client_ip WHERE true\" ) . build ()) . metadata ( new Metadata . Builder () . name ( \"GroupByIp\" ) . description ( \"Connects to a source job and counts the number of requests sent from each ip within a window\" ) . build ()) . create ();","title":"eagerSubscribe Sink"},{"location":"writingjobs/sink/#tomany-sink","text":"You can hook up multiple Sinks to the same final Stage of a Job by using the toMany Sink. To do this, pass each Sink to the toMany() method. For example: return MantisJob . \u2026 . sink ( Sinks . toMany ( Sinks . sysout (), Sinks . sse (( String data ) -> data ))). \u2026 . create ();","title":"toMany Sink"},{"location":"writingjobs/sink/#custom-sinks","text":"If you do not want to use one of the provided Sinks, or if you need to customize one of those (for instance, if you need access to the query parameters supplied by a client who is connecting into your Job, or if you need pre and post hooks to perform operations as a new client connects or disconnects), you can create your own Sink. To do so, implement the Sink interface. The ServerSentEventsSink builder takes three parameters: Preprocess function this callback gives you access to the query parameters; it is invoked before job execution begins Postprocess function this callback allows you to perform any clean-up actions; it is invoked just after the client connection is terminated Predicate function allows you to filter your output stream based on the given predicate, which allows you to filter based on query parameters sent by the client Here is an example of a custom Sink that uses the ServerSentEventsSink builder: package com.netflix.mantis.sourcejob ; import io.mantisrx.runtime.Context ; import io.mantisrx.runtime.PortRequest ; import io.mantisrx.runtime.sink.ServerSentEventsSink ; import io.mantisrx.runtime.sink.Sink ; import io.mantisrx.runtime.sink.predicate.Predicate ; import java.util.List ; import java.util.Map ; import rx.Observable ; import rx.functions.Func1 ; import rx.functions.Func2 ; import com.google.common.base.Charsets ; public class SourceSink implements Sink < String > { Func2 < Map < String , List < String >>, Context , Void > preProcessor = new NoOpProcessor (); Func2 < Map < String , List < String >>, Context , Void > postProcessor = new NoOpProcessor (); private String clientId = \"DEFAULT_CLIENT_ID\" ; static class NoOpProcessor implements Func2 < Map < String , List < String >>, Context , Void > { @Override public Void call ( Map < String , List < String >> t1 , Context t2 ) { return null ; } } public SourceSink () { } public SourceSink ( Func2 < Map < String , List < String >>, Context , Void > preProcessor , Func2 < Map < String , List < String >>, Context , Void > postProcessor , String mantisClientId ) { this . postProcessor = postProcessor ; this . preProcessor = preProcessor ; this . clientId = mantisClientId ; } @Override public void call ( Context context , PortRequest portRequest , Observable < String > observable ) { observable = observable . filter ( new Func1 < String , Boolean >() { @Override public Boolean call ( String t1 ) { return ! t1 . isEmpty (); } }); ServerSentEventsSink < String > sink = new ServerSentEventsSink . Builder < String >() . withEncoder ( new Func1 < String , String >() { @Override public String call ( String data ) { return data ; } }) . withPredicate ( new Predicate < String >( \"description\" , new EventFilter ( clientId ))) . withRequestPreprocessor ( preProcessor ) . withRequestPostprocessor ( postProcessor ) . build (); observable . subscribe (); sink . call ( context , portRequest , observable ); } public static void main ( String [] args ) { String s = \"{\\\"amazon.availability-zone\\\":\\\"us-east-1e\\\",\\\"status\\\":200,\\\"type\\\":\\\"EVENT\\\",\\\"matched-clients\\\":\\\"client6\\\",\\\"currentTime\\\":1409595016697,\\\"duration-millis\\\":172}\" ; byte [] barr = s . getBytes ( Charsets . UTF_8 ); System . out . println ( \"size: \" + barr . length ); } }","title":"Custom Sinks"},{"location":"writingjobs/source/","text":"To implement a Mantis Job Source component , you must implement the io.mantisrx.runtime.Source interface. A Source returns Observable<Observable<T>> , that is, an Observable that emits Observables. Each of the emitted Observables represents a stream of data from a single target server. Varieties of Sources \u00b6 Sources can be roughly divided into two categories: Sources that read data from the output of other Mantis Jobs this may include Source Jobs (more on this below) or ordinary Mantis Jobs Custom sources that read data directly from Amazon S3, SQS, Apache Kafka , etc. Mantis Job Sources \u00b6 You can string Mantis Jobs together by using the output of one Mantis Job as the input to another. This is useful if you want to break up your processing into multiple, reusable components and to take advantage of code and data reuse. In such a case, you do not have access to the complete set of Mantis Query Language (MQL) capabilities that you do in the case of a Source Job, but you can use MQL in client mode. Connecting to a Mantis Job \u00b6 To connect to a Mantis Job, use a JobSource when you call MantisJob.create() \u2014 declare the following parameters when you use this class: sourceJobName (required) \u2014 the name of any valid Job Cluster (not necessarily a \u201cSource Job\u201d) sample (optional) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( \"sourceJobName\" ) . description ( \"The name of the job\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"MyDefaultJob\" ) . build ()) . parameterDefinition ( new IntParameter (). name ( \"sample\" ) . description ( \"The number of samples per second\" ) . validator ( Validators . range (- 1 , 10000 )) . defaultValue (- 1 ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create (); Source Job Sources \u00b6 Mantis has a concept of Source Jobs which are Mantis Jobs with added conveniences and efficiences that simplify accessing data from certain sources. Your job can simply connect to a source job as its data source rather than trying to retrieve the data from its native home. There are two advantages to this approach: Source Jobs handle all of the implementation details around interacting with the native data source. Source Jobs come with a simple query interface based on the Mantis Query Language (MQL) , which allows you to filter the data from the source before processing it. In the case of source jobs that fetch data from application servers directly, this filter gets pushed all the way to those target servers so that no data flows unless someone is asking for it. Source Jobs reuse data so that multiple matching MQL queries are forwarded downstream instead of paying the cost to fetch and serialize/deserialize the same data multiple times from the upstream source. Broadcast Mode \u00b6 By default, Mantis will distribute the data that is output from the Source Job among the various workers in the processing stage of your Mantis Job. Each of those workers will get a subset of the complete data from the Source Job. You can override this by instructing the Source Job to use \u201cbroadcast mode\u201d. If you do this, Mantis will send the complete set of data from the Source Job to every worker in your Job. Connecting to a Source Job \u00b6 Since Source Jobs are fundamentally Mantis Jobs, you should use a JobSource when you call MantisJob.create() to connect to a particular Source Job. The difference is that you should pass in additional parameters: sourceJobName (required) \u2014 the name of the source Job Cluster you want to connect to sample (required) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling criterion (required) \u2014 a query expression in MQL to filter the source clientId (optional) \u2014 by default, the jobId of the client Job; the Source Job uses this to distribute data between all the subscriptions of the client Job enableMetaMessages (optional) \u2014 the source job may occasionally inject meta messages (with the prefix mantis.meta. ) that indicate things like data drops on the Source Job side. For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( \"sourceJobName\" ) . description ( \"The name of the job\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"MyDefaultSourceJob\" ) . build ()) . parameterDefinition ( new IntParameter (). name ( \"sample\" ) . description ( \"The number of samples per second\" ) . validator ( Validators . range (- 1 , 10000 )) . defaultValue (- 1 ) . build ()) . parameterDefinition ( new StringParameter (). name ( \"criterion\" ) . description ( \"Filter the source with this MQL statement\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"true\" ) . build ()) . parameterDefinition ( new StringParameter (). name ( \"clientId\" ) . description ( \"the ID of the client job\" ) . validator ( Validators . alwaysPass ()) . build ()) . parameterDefinition ( new BooleanParameter (). name ( \"enableMetaMessages\" ) . description ( \"Is the source allowed to inject meta messages\" ) . validator ( Validators . alwaysPass ()) . defaultValue ( \"true\" ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create (); Custom Sources \u00b6 Custom sources may be implemented and used to access data sources for which Mantis does not have a Source Job. Implementers are free to implement the Source interface to fetch data from an external source. Here is an example in a source which implements the Source interface to consume data from Kafka. Learning When Source Data is Incomplete \u00b6 You may want to know whether or not the stream you are receiving from your source is complete. Streams may be incomplete for a number of reasons: A connection to one or more of the Source Job workers is lost. A connection exists but no data is flowing. Data is intentionally dropped from a source because of the backpressure strategy you are using. You can use the following boolean method within your JobSource#call method to determine whether or not all of your client connections are complete: DefaultSinkConnectionStatusObserver . getInstance ( true ). isConnectedToAllSinks ()","title":"The Source Component"},{"location":"writingjobs/source/#varieties-of-sources","text":"Sources can be roughly divided into two categories: Sources that read data from the output of other Mantis Jobs this may include Source Jobs (more on this below) or ordinary Mantis Jobs Custom sources that read data directly from Amazon S3, SQS, Apache Kafka , etc.","title":"Varieties of Sources"},{"location":"writingjobs/source/#mantis-job-sources","text":"You can string Mantis Jobs together by using the output of one Mantis Job as the input to another. This is useful if you want to break up your processing into multiple, reusable components and to take advantage of code and data reuse. In such a case, you do not have access to the complete set of Mantis Query Language (MQL) capabilities that you do in the case of a Source Job, but you can use MQL in client mode.","title":"Mantis Job Sources"},{"location":"writingjobs/source/#connecting-to-a-mantis-job","text":"To connect to a Mantis Job, use a JobSource when you call MantisJob.create() \u2014 declare the following parameters when you use this class: sourceJobName (required) \u2014 the name of any valid Job Cluster (not necessarily a \u201cSource Job\u201d) sample (optional) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( \"sourceJobName\" ) . description ( \"The name of the job\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"MyDefaultJob\" ) . build ()) . parameterDefinition ( new IntParameter (). name ( \"sample\" ) . description ( \"The number of samples per second\" ) . validator ( Validators . range (- 1 , 10000 )) . defaultValue (- 1 ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create ();","title":"Connecting to a Mantis Job"},{"location":"writingjobs/source/#source-job-sources","text":"Mantis has a concept of Source Jobs which are Mantis Jobs with added conveniences and efficiences that simplify accessing data from certain sources. Your job can simply connect to a source job as its data source rather than trying to retrieve the data from its native home. There are two advantages to this approach: Source Jobs handle all of the implementation details around interacting with the native data source. Source Jobs come with a simple query interface based on the Mantis Query Language (MQL) , which allows you to filter the data from the source before processing it. In the case of source jobs that fetch data from application servers directly, this filter gets pushed all the way to those target servers so that no data flows unless someone is asking for it. Source Jobs reuse data so that multiple matching MQL queries are forwarded downstream instead of paying the cost to fetch and serialize/deserialize the same data multiple times from the upstream source.","title":"Source Job Sources"},{"location":"writingjobs/source/#broadcast-mode","text":"By default, Mantis will distribute the data that is output from the Source Job among the various workers in the processing stage of your Mantis Job. Each of those workers will get a subset of the complete data from the Source Job. You can override this by instructing the Source Job to use \u201cbroadcast mode\u201d. If you do this, Mantis will send the complete set of data from the Source Job to every worker in your Job.","title":"Broadcast Mode"},{"location":"writingjobs/source/#connecting-to-a-source-job","text":"Since Source Jobs are fundamentally Mantis Jobs, you should use a JobSource when you call MantisJob.create() to connect to a particular Source Job. The difference is that you should pass in additional parameters: sourceJobName (required) \u2014 the name of the source Job Cluster you want to connect to sample (required) \u2014 use this if you want to sample the output sample times per second, or set this to -1 to disable sampling criterion (required) \u2014 a query expression in MQL to filter the source clientId (optional) \u2014 by default, the jobId of the client Job; the Source Job uses this to distribute data between all the subscriptions of the client Job enableMetaMessages (optional) \u2014 the source job may occasionally inject meta messages (with the prefix mantis.meta. ) that indicate things like data drops on the Source Job side. For example: MantisJob . source ( new JobSource ()) . stage ( \u2026 ) . sink ( \u2026 ) . parameterDefinition ( new StringParameter (). name ( \"sourceJobName\" ) . description ( \"The name of the job\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"MyDefaultSourceJob\" ) . build ()) . parameterDefinition ( new IntParameter (). name ( \"sample\" ) . description ( \"The number of samples per second\" ) . validator ( Validators . range (- 1 , 10000 )) . defaultValue (- 1 ) . build ()) . parameterDefinition ( new StringParameter (). name ( \"criterion\" ) . description ( \"Filter the source with this MQL statement\" ) . validator ( Validators . notNullOrEmpty ()) . defaultValue ( \"true\" ) . build ()) . parameterDefinition ( new StringParameter (). name ( \"clientId\" ) . description ( \"the ID of the client job\" ) . validator ( Validators . alwaysPass ()) . build ()) . parameterDefinition ( new BooleanParameter (). name ( \"enableMetaMessages\" ) . description ( \"Is the source allowed to inject meta messages\" ) . validator ( Validators . alwaysPass ()) . defaultValue ( \"true\" ) . build ()) . lifecycle ( \u2026 ) . metadata ( \u2026 ) . create ();","title":"Connecting to a Source Job"},{"location":"writingjobs/source/#custom-sources","text":"Custom sources may be implemented and used to access data sources for which Mantis does not have a Source Job. Implementers are free to implement the Source interface to fetch data from an external source. Here is an example in a source which implements the Source interface to consume data from Kafka.","title":"Custom Sources"},{"location":"writingjobs/source/#learning-when-source-data-is-incomplete","text":"You may want to know whether or not the stream you are receiving from your source is complete. Streams may be incomplete for a number of reasons: A connection to one or more of the Source Job workers is lost. A connection exists but no data is flowing. Data is intentionally dropped from a source because of the backpressure strategy you are using. You can use the following boolean method within your JobSource#call method to determine whether or not all of your client connections are complete: DefaultSinkConnectionStatusObserver . getInstance ( true ). isConnectedToAllSinks ()","title":"Learning When Source Data is Incomplete"},{"location":"writingjobs/stage/","text":"A Processing Stage component of a Mantis Job processes the stream of data by [transforming] it in some way. You can combine multiple Processing Stages into a single Job, or you can create a Job that consists of a single Processing Stage. In its simplest form, a Processing Stage is a chain of RxJava operators operating on the Observable provided by the Source component. Transformations can be broadly categorized into two types: Scalar or Grouped . There are four varieties of Processing Stage, based on what type of transformation they accomplish: Scalar-to-Scalar \u2014 Also known as \u201cnarrow transformation,\u201d this variety of Stage converts an Observable<T> into an Observable<R> . Such a Stage extends the ScalarToScalar class . Scalar-to-Key / Scalar-to-Group \u2014 Also known as \u201cwidening transformation,\u201d this variety of Stage groups each element emitted by the source Observable by key. Typically you use such a Stage when you build a map/reduce-style job in which you need to perform stateful computations but the volume of data is too large to fit on a single worker. In such a model, the incoming data is divided into multiple streams, one per group. A subsequent State will typically to the stateful computation (for instance, calculating percentiles for the group). The purpose of this Scalar-to-Key State is to tag each incoming element with the group that it belongs to. Mantis then takes care of routing all traffic for the same group to the same worker in the subsequent stage. Scalar-to-Key \u2014 This is the legacy way of grouping data (it is more elegant but comes with a performance penalty). You extend the ScalarToKey class and transform an Observable<T> into an Observable<GroupedObservable<K,R>> (where K is the key). See the RxJava groupBy operator for more information. Scalar-to-Group \u2014 This is a more efficient way to group data. You extend the ScalarToGroup class and transform an Observable<T> into an Observable<MantisGroup<K,R>> (where K is the key). This avoids the overhead associated with creating a GroupedObservable which can limit the number of groups it is possible to create. Key-to-Scalar / Group-to-Scalar \u2014 Once you have split a stream, you need a stage that can take grouped data and return it to a scalar form. Key-to-Scalar \u2014 This is the legacy method and is designed for streams that have been split via a ScalarToKey Stage. You extend the KeyToScalar class and transform a GroupedObservable<K,T> into an Observable<T> . Group-to-Scalar \u2014 This is the newer, faster method. It is less elegant, in that you must transform an Observable<MantisGroup> that contains payloads from all groups, and you must therefore manage the per-group state. Typically you would do this via a map that holds per-group state and evicts entries that haven\u2019t been touched recently. This method has much better performance than the Key-to-Scalar method because it omits the overhead around RxJava\u2019s GroupedObservable . Key-to-Key / Group-to-Group \u2014 You can further split a keyed stream by grouping it again if you have some use case that requires this.","title":"The Processing Stage Component"}]}